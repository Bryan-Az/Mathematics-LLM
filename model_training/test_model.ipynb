{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing the Fine-tuned Math Model\n",
        "\n",
        "This notebook loads and tests the fine-tuned model from HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "%%capture\n",
        "!pip install transformers torch accelerate bitsandbytes sympy peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel, PeftConfig\n",
        "import sympy\n",
        "from sympy.parsing.sympy_parser import parse_expr, standard_transformations, implicit_multiplication_application\n",
        "\n",
        "# Configure sympy parsing\n",
        "transformations = standard_transformations + (implicit_multiplication_application,)\n",
        "x = sympy.Symbol('x')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Model configuration\n",
        "BASE_MODEL = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"  # Base model\n",
        "ADAPTER_MODEL = \"Joash2024/Math-SmolLM2-1.7B\"       # LoRA adapter\n",
        "device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Configure quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        ")\n",
        "\n",
        "# Load base model and tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Loading base model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load LoRA adapter\n",
        "print(\"Loading LoRA adapter...\")\n",
        "model = PeftModel.from_pretrained(model, ADAPTER_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def format_prompt(function: str) -> str:\n",
        "    \"\"\"Format input prompt for the model\"\"\"\n",
        "    return f\"\"\"Given a mathematical function, find its derivative.\n",
        "\n",
        "Function: {function}\n",
        "The derivative of this function is:\"\"\"\n",
        "\n",
        "def generate_derivative(function: str, max_length: int = 200) -> str:\n",
        "    \"\"\"Generate derivative for a given function\"\"\"\n",
        "    # Format the prompt\n",
        "    prompt = format_prompt(function)\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.1,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode and extract derivative\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    derivative = generated[len(prompt):].strip()\n",
        "    \n",
        "    return derivative\n",
        "\n",
        "def test_model():\n",
        "    \"\"\"Test the model on various functions\"\"\"\n",
        "    test_cases = [\n",
        "        \"x^2\",\n",
        "        \"\\\\sin{\\\\left(x\\\\right)}\",\n",
        "        \"e^x\",\n",
        "        \"\\\\frac{1}{x}\",\n",
        "        \"x^3 + 2x\",\n",
        "        \"\\\\cos{\\\\left(x^2\\\\right)}\",\n",
        "        \"\\\\log{\\\\left(x\\\\right)}\",\n",
        "        \"x e^{-x}\"\n",
        "    ]\n",
        "    \n",
        "    print(\"Testing model on various functions:\")\n",
        "    for func in test_cases:\n",
        "        print(f\"\\nFunction: {func}\")\n",
        "        derivative = generate_derivative(func)\n",
        "        print(f\"Generated derivative: {derivative}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Run batch tests\n",
        "print(\"Running batch tests...\\n\")\n",
        "test_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def interactive_test():\n",
        "    \"\"\"Test the model interactively\"\"\"\n",
        "    while True:\n",
        "        function = input(\"\\nEnter a function (or 'q' to quit): \")\n",
        "        if function.lower() == 'q':\n",
        "            break\n",
        "            \n",
        "        print(\"\\nGenerating derivative...\")\n",
        "        derivative = generate_derivative(function)\n",
        "        print(f\"Generated derivative: {derivative}\")\n",
        "\n",
        "print(\"Starting interactive testing...\")\n",
        "print(\"Enter functions in LaTeX notation (e.g., x^2, \\\\sin{\\\\left(x\\\\right)})\")\n",
        "print(\"Enter 'q' to quit\")\n",
        "interactive_test()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
