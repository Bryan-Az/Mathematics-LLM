{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 29 key-value pairs and 256 tensors from Llama-3.2-3B-Instruct-uncensored-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Meta Llama\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Llama-3.2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type  f16:  198 tensors\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 24\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 3\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 3.61 B\n",
      "llm_load_print_meta: model size       = 6.72 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = Llama 3.2 3B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (f16) (and 282 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  6879.67 MiB\n",
      ".................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 512\n",
      "llama_new_context_with_model: n_ctx_per_seq = 512\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init:        CPU KV buffer size =    56.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   56.00 MiB, K (f16):   28.00 MiB, V (f16):   28.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   256.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 902\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'Llama 3.2 3B Instruct', 'general.architecture': 'llama', 'general.type': 'model', 'llama.context_length': '131072', 'general.organization': 'Meta Llama', 'llama.block_count': '28', 'general.basename': 'Llama-3.2', 'general.finetune': 'Instruct', 'general.size_label': '3B', 'llama.embedding_length': '3072', 'llama.feed_forward_length': '8192', 'llama.attention.head_count': '24', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '1', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '500000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.attention.key_length': '128', 'llama.attention.value_length': '128', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\\n    {%- else %}\\n        {%- set date_string = \"26 Jul 2024\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n        {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n        {{- \\'\"parameters\": \\' }}\\n        {{- tool_call.arguments | tojson }}\\n        {{- \"}\" }}\\n        {{- \"<|eot_id|>\" }}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "llm = Llama(model_path=\"Llama-3.2-3B-Instruct-uncensored-f16.gguf\")\n",
    "eval_df = pd.read_csv(\"training_data/addition_operations_eval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6541.95 ms /    72 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5947.37 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5924.40 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1322.42 ms /    14 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5904.37 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5908.17 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5979.87 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2023.90 ms /    20 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    33 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4134.62 ms /    37 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     870.52 ms /    10 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6057.35 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6042.99 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6104.18 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1504.34 ms /    15 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5980.45 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5935.81 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5927.55 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6040.71 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6006.71 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5940.41 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6057.35 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5957.04 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6111.22 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6144.93 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5948.07 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    31 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3868.71 ms /    35 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6099.36 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5919.00 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5988.83 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5911.20 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2398.67 ms /    23 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6000.71 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     855.31 ms /    10 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5913.83 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5922.40 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5915.59 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5950.31 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6118.44 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6363.20 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1739.44 ms /    16 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    29 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3789.53 ms /    33 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6162.56 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5994.04 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6096.81 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    17 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2166.87 ms /    21 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1578.52 ms /    16 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6172.09 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2076.36 ms /    20 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6104.67 ms /    53 tokens\n",
      "Llama.generate: 19 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     716.93 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6041.20 ms /    53 tokens\n",
      "C:\\Users\\Nickk\\AppData\\Local\\Temp\\ipykernel_33080\\332127825.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_subset['Raw_Result'] = eval_subset['Operation'].apply(\n",
      "C:\\Users\\Nickk\\AppData\\Local\\Temp\\ipykernel_33080\\332127825.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_subset['Model_Result'] = eval_subset['Raw_Result'].apply(extract_number)\n",
      "C:\\Users\\Nickk\\AppData\\Local\\Temp\\ipykernel_33080\\332127825.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_subset['Correct'] = eval_subset.apply(\n"
     ]
    }
   ],
   "source": [
    "num_problems = 50\n",
    "\n",
    "\n",
    "def solve_math_problem(math_problem):\n",
    "    response = llm(\n",
    "        f\"Solve this math problem, only print the final answer, do not provide steps: {math_problem}\",\n",
    "        max_tokens=50\n",
    "    )\n",
    "    solution = response['choices'][0]['text'].strip()\n",
    "    return solution\n",
    "\n",
    "\n",
    "def extract_number(text):\n",
    "    match = re.search(r'\\d+', text)\n",
    "    return int(match.group()) if match else None\n",
    "\n",
    "\n",
    "if num_problems <= 0:\n",
    "    num_problems = len(eval_df)\n",
    "elif num_problems > len(eval_df):\n",
    "    num_problems = len(eval_df)\n",
    "\n",
    "eval_subset = eval_df.head(num_problems)\n",
    "\n",
    "eval_subset['Raw_Result'] = eval_subset['Operation'].apply(\n",
    "    solve_math_problem)\n",
    "eval_subset['Model_Result'] = eval_subset['Raw_Result'].apply(extract_number)\n",
    "eval_subset['Correct'] = eval_subset.apply(\n",
    "    lambda row: row['Result'] == row['Model_Result'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Operation</th>\n",
       "      <th>Result</th>\n",
       "      <th>Raw_Result</th>\n",
       "      <th>Model_Result</th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>945 + 859</td>\n",
       "      <td>1804</td>\n",
       "      <td>.\\n\\n\\n## Step 1:  We are given the task of ad...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324 + 558</td>\n",
       "      <td>882</td>\n",
       "      <td>+ 215 + 987 + 106 + 23\\n\\n\\n## Step 1:  First,...</td>\n",
       "      <td>215</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>373 + 449</td>\n",
       "      <td>822</td>\n",
       "      <td>= \\n\\n## Step 1:  We need to add the numbers 3...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>339 + 51</td>\n",
       "      <td>390</td>\n",
       "      <td>= \\nThe final answer is: $398$</td>\n",
       "      <td>398</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>437 + 568</td>\n",
       "      <td>1005</td>\n",
       "      <td>+ 213 + 137.\\n\\n\\n## Step 1: Add the numbers 4...</td>\n",
       "      <td>213</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>747 + 322</td>\n",
       "      <td>1069</td>\n",
       "      <td>=\\n\\n\\n## Step 1: First, we need to add the nu...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>296 + 726</td>\n",
       "      <td>1022</td>\n",
       "      <td>+ 423 + 138 + 46   =  ?   .  . . .  .   .  .  ...</td>\n",
       "      <td>423</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>920 + 397</td>\n",
       "      <td>1317</td>\n",
       "      <td>Answer: 1317 920 + 397 = 1317.</td>\n",
       "      <td>1317</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>465 + 277</td>\n",
       "      <td>742</td>\n",
       "      <td>8 = 4243.   (Note: Do not include the equals s...</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>779 + 905</td>\n",
       "      <td>1684</td>\n",
       "      <td>= \\n\\n\\n= 1684</td>\n",
       "      <td>1684</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>266 + 862</td>\n",
       "      <td>1128</td>\n",
       "      <td>1 = ?  ## Step 1: We will add the two given nu...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>640 + 102</td>\n",
       "      <td>742</td>\n",
       "      <td>9 + 621 =  ?  (Answer should be 1978)  I'm rea...</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>268 + 883</td>\n",
       "      <td>1151</td>\n",
       "      <td>= \\n\\n## Step 1:  Add the numbers\\nFirst, we n...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>928 + 8</td>\n",
       "      <td>936</td>\n",
       "      <td>= ? 928 + 8 = 936.</td>\n",
       "      <td>928</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>102 + 146</td>\n",
       "      <td>248</td>\n",
       "      <td>+ 11 = 259\\n\\n\\n## Step 1: Add 102 and 146\\nWe...</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>693 + 185</td>\n",
       "      <td>878</td>\n",
       "      <td>+ 442 + 218 + 984\\nAnswer: 2222. ]]]]]]]]]]]]]...</td>\n",
       "      <td>442</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>262 + 922</td>\n",
       "      <td>1184</td>\n",
       "      <td>=   1184   = 1184\\nI made a mistake, let's sol...</td>\n",
       "      <td>1184</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>717 + 350</td>\n",
       "      <td>1067</td>\n",
       "      <td>0 = ?  ## Step 1: We are given two numbers to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>351 + 186</td>\n",
       "      <td>537</td>\n",
       "      <td>+ 4 = ?  ## Step 1:  First, we need to add 351...</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>680 + 212</td>\n",
       "      <td>892</td>\n",
       "      <td>, 35 - 27, 49 - 21, 98 - 47, 720 - 539, 300 - ...</td>\n",
       "      <td>35</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>689 + 581</td>\n",
       "      <td>1270</td>\n",
       "      <td>= ? 946  947  948  949  950\\nAnswer: 946 is th...</td>\n",
       "      <td>946</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>212 + 39</td>\n",
       "      <td>251</td>\n",
       "      <td>+ 47 - 32 + 46 - 18 =  ?  = 263 = 265. 265 is ...</td>\n",
       "      <td>47</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>256 + 560</td>\n",
       "      <td>816</td>\n",
       "      <td>=  $1,816.00. \\n\\n## Step 1: First, we add the...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>67 + 168</td>\n",
       "      <td>235</td>\n",
       "      <td>= ? 8 = 1 . . . . . . . . . . . . . . . . . . ...</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>184 + 829</td>\n",
       "      <td>1013</td>\n",
       "      <td>+ 450 = _______\\n\\n\\n## Step 1: First, add 184...</td>\n",
       "      <td>450</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>972 + 447</td>\n",
       "      <td>1419</td>\n",
       "      <td>## Step 1:  Add the two numbers, 972 and 447, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>704 + 752</td>\n",
       "      <td>1456</td>\n",
       "      <td>+ 727 + 749 =  ? 314  274  405  302  999   1  ...</td>\n",
       "      <td>727</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>960 + 923</td>\n",
       "      <td>1883</td>\n",
       "      <td>+ 891 + 764 + 720 + 651 + 612 + 564 + 523 + 47...</td>\n",
       "      <td>891</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1 + 928</td>\n",
       "      <td>929</td>\n",
       "      <td>1479250 + 0.00000000001\\n\\n\\n## Step 1: Identi...</td>\n",
       "      <td>1479250</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>480 + 627</td>\n",
       "      <td>1107</td>\n",
       "      <td>= ?  ## Step 1: Write the equation\\n480 + 627 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>475 + 861</td>\n",
       "      <td>1336</td>\n",
       "      <td>+ 548 + 92 + 41 = \\nThe final answer is: 1817</td>\n",
       "      <td>548</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>702 + 776</td>\n",
       "      <td>1478</td>\n",
       "      <td>= 1478  4 x (2 + 9) = 40  2(6 - 3) + 11 = ?  =...</td>\n",
       "      <td>1478</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>937 + 688</td>\n",
       "      <td>1625</td>\n",
       "      <td>Answer: 1625</td>\n",
       "      <td>1625</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>104 + 655</td>\n",
       "      <td>759</td>\n",
       "      <td>.   8.66025 x 0.0206.  0.0206.  0.999999999999...</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>168 + 810</td>\n",
       "      <td>978</td>\n",
       "      <td>+ 432 + 18 = ?\\n6,028 \\nAnswer: 6028 \\nSolutio...</td>\n",
       "      <td>432</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>200 + 64</td>\n",
       "      <td>264</td>\n",
       "      <td>+ 28 = 292,  15 x 11 = 165,  11^2 = 121,  121 ...</td>\n",
       "      <td>28</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>273 + 875</td>\n",
       "      <td>1148</td>\n",
       "      <td>= 1,148.  What is the correct interpretation o...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>668 + 924</td>\n",
       "      <td>1592</td>\n",
       "      <td>=  ????   15% of 1250 =  ??   50/4 =  ??   0.0...</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>922 + 143</td>\n",
       "      <td>1065</td>\n",
       "      <td>+ 217 + 92\\n\\n\\n\\n## Step 1: First, we will ad...</td>\n",
       "      <td>217</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>848 + 30</td>\n",
       "      <td>878</td>\n",
       "      <td>= \\nThe final answer is: $\\boxed{878}$</td>\n",
       "      <td>878</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>112 + 365</td>\n",
       "      <td>477</td>\n",
       "      <td>= \\n\\n## Step 1: Add the numbers\\n112 + 365 = ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>685 + 170</td>\n",
       "      <td>855</td>\n",
       "      <td>4 =  ? 645.   689.5\\nAnswer: 1389.5\\n645 + 138...</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>653 + 104</td>\n",
       "      <td>757</td>\n",
       "      <td>= 757   2.2. 1.1. 6.6   2.2. 1.1. 6.6   2.2. 1...</td>\n",
       "      <td>757</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>979 + 539</td>\n",
       "      <td>1518</td>\n",
       "      <td>+ 118 + 452 + 211\\n\\n\\n## Step 1: First, we ne...</td>\n",
       "      <td>118</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>231 + 66</td>\n",
       "      <td>297</td>\n",
       "      <td>= \\n\\n\\n= 297 \\n\\n\\n\\n\\n\\nThe final answer is:...</td>\n",
       "      <td>297</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>67 + 466</td>\n",
       "      <td>533</td>\n",
       "      <td>5281\\nThe final answer is: 4674548</td>\n",
       "      <td>5281</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>872 + 453</td>\n",
       "      <td>1325</td>\n",
       "      <td>= ?  ## Step 1: Write down the numbers to be a...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>105 + 521</td>\n",
       "      <td>626</td>\n",
       "      <td>= 626.  \\n\\nThe final answer is: $\\boxed{1147}$</td>\n",
       "      <td>626</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>416 + 514</td>\n",
       "      <td>930</td>\n",
       "      <td>= 930. .(1)\\n\\n## Step 1:  We are given the ta...</td>\n",
       "      <td>930</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>417 + 42</td>\n",
       "      <td>459</td>\n",
       "      <td>+ 0 = ?\\n1,500.00\\n9,177.50\\n9,600.00\\n9,722.5...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Operation  Result                                         Raw_Result  \\\n",
       "0   945 + 859    1804  .\\n\\n\\n## Step 1:  We are given the task of ad...   \n",
       "1   324 + 558     882  + 215 + 987 + 106 + 23\\n\\n\\n## Step 1:  First,...   \n",
       "2   373 + 449     822  = \\n\\n## Step 1:  We need to add the numbers 3...   \n",
       "3    339 + 51     390                     = \\nThe final answer is: $398$   \n",
       "4   437 + 568    1005  + 213 + 137.\\n\\n\\n## Step 1: Add the numbers 4...   \n",
       "5   747 + 322    1069  =\\n\\n\\n## Step 1: First, we need to add the nu...   \n",
       "6   296 + 726    1022  + 423 + 138 + 46   =  ?   .  . . .  .   .  .  ...   \n",
       "7   920 + 397    1317                     Answer: 1317 920 + 397 = 1317.   \n",
       "8   465 + 277     742  8 = 4243.   (Note: Do not include the equals s...   \n",
       "9   779 + 905    1684                                     = \\n\\n\\n= 1684   \n",
       "10  266 + 862    1128  1 = ?  ## Step 1: We will add the two given nu...   \n",
       "11  640 + 102     742  9 + 621 =  ?  (Answer should be 1978)  I'm rea...   \n",
       "12  268 + 883    1151  = \\n\\n## Step 1:  Add the numbers\\nFirst, we n...   \n",
       "13    928 + 8     936                                 = ? 928 + 8 = 936.   \n",
       "14  102 + 146     248  + 11 = 259\\n\\n\\n## Step 1: Add 102 and 146\\nWe...   \n",
       "15  693 + 185     878  + 442 + 218 + 984\\nAnswer: 2222. ]]]]]]]]]]]]]...   \n",
       "16  262 + 922    1184  =   1184   = 1184\\nI made a mistake, let's sol...   \n",
       "17  717 + 350    1067  0 = ?  ## Step 1: We are given two numbers to ...   \n",
       "18  351 + 186     537  + 4 = ?  ## Step 1:  First, we need to add 351...   \n",
       "19  680 + 212     892  , 35 - 27, 49 - 21, 98 - 47, 720 - 539, 300 - ...   \n",
       "20  689 + 581    1270  = ? 946  947  948  949  950\\nAnswer: 946 is th...   \n",
       "21   212 + 39     251  + 47 - 32 + 46 - 18 =  ?  = 263 = 265. 265 is ...   \n",
       "22  256 + 560     816  =  $1,816.00. \\n\\n## Step 1: First, we add the...   \n",
       "23   67 + 168     235  = ? 8 = 1 . . . . . . . . . . . . . . . . . . ...   \n",
       "24  184 + 829    1013  + 450 = _______\\n\\n\\n## Step 1: First, add 184...   \n",
       "25  972 + 447    1419  ## Step 1:  Add the two numbers, 972 and 447, ...   \n",
       "26  704 + 752    1456  + 727 + 749 =  ? 314  274  405  302  999   1  ...   \n",
       "27  960 + 923    1883  + 891 + 764 + 720 + 651 + 612 + 564 + 523 + 47...   \n",
       "28    1 + 928     929  1479250 + 0.00000000001\\n\\n\\n## Step 1: Identi...   \n",
       "29  480 + 627    1107  = ?  ## Step 1: Write the equation\\n480 + 627 ...   \n",
       "30  475 + 861    1336      + 548 + 92 + 41 = \\nThe final answer is: 1817   \n",
       "31  702 + 776    1478  = 1478  4 x (2 + 9) = 40  2(6 - 3) + 11 = ?  =...   \n",
       "32  937 + 688    1625                                       Answer: 1625   \n",
       "33  104 + 655     759  .   8.66025 x 0.0206.  0.0206.  0.999999999999...   \n",
       "34  168 + 810     978  + 432 + 18 = ?\\n6,028 \\nAnswer: 6028 \\nSolutio...   \n",
       "35   200 + 64     264  + 28 = 292,  15 x 11 = 165,  11^2 = 121,  121 ...   \n",
       "36  273 + 875    1148  = 1,148.  What is the correct interpretation o...   \n",
       "37  668 + 924    1592  =  ????   15% of 1250 =  ??   50/4 =  ??   0.0...   \n",
       "38  922 + 143    1065  + 217 + 92\\n\\n\\n\\n## Step 1: First, we will ad...   \n",
       "39   848 + 30     878             = \\nThe final answer is: $\\boxed{878}$   \n",
       "40  112 + 365     477  = \\n\\n## Step 1: Add the numbers\\n112 + 365 = ...   \n",
       "41  685 + 170     855  4 =  ? 645.   689.5\\nAnswer: 1389.5\\n645 + 138...   \n",
       "42  653 + 104     757  = 757   2.2. 1.1. 6.6   2.2. 1.1. 6.6   2.2. 1...   \n",
       "43  979 + 539    1518  + 118 + 452 + 211\\n\\n\\n## Step 1: First, we ne...   \n",
       "44   231 + 66     297  = \\n\\n\\n= 297 \\n\\n\\n\\n\\n\\nThe final answer is:...   \n",
       "45   67 + 466     533                 5281\\nThe final answer is: 4674548   \n",
       "46  872 + 453    1325  = ?  ## Step 1: Write down the numbers to be a...   \n",
       "47  105 + 521     626    = 626.  \\n\\nThe final answer is: $\\boxed{1147}$   \n",
       "48  416 + 514     930  = 930. .(1)\\n\\n## Step 1:  We are given the ta...   \n",
       "49   417 + 42     459  + 0 = ?\\n1,500.00\\n9,177.50\\n9,600.00\\n9,722.5...   \n",
       "\n",
       "    Model_Result  Correct  \n",
       "0              1    False  \n",
       "1            215    False  \n",
       "2              1    False  \n",
       "3            398    False  \n",
       "4            213    False  \n",
       "5              1    False  \n",
       "6            423    False  \n",
       "7           1317     True  \n",
       "8              8    False  \n",
       "9           1684     True  \n",
       "10             1    False  \n",
       "11             9    False  \n",
       "12             1    False  \n",
       "13           928    False  \n",
       "14            11    False  \n",
       "15           442    False  \n",
       "16          1184     True  \n",
       "17             0    False  \n",
       "18             4    False  \n",
       "19            35    False  \n",
       "20           946    False  \n",
       "21            47    False  \n",
       "22             1    False  \n",
       "23             8    False  \n",
       "24           450    False  \n",
       "25             1    False  \n",
       "26           727    False  \n",
       "27           891    False  \n",
       "28       1479250    False  \n",
       "29             1    False  \n",
       "30           548    False  \n",
       "31          1478     True  \n",
       "32          1625     True  \n",
       "33             8    False  \n",
       "34           432    False  \n",
       "35            28    False  \n",
       "36             1    False  \n",
       "37            15    False  \n",
       "38           217    False  \n",
       "39           878     True  \n",
       "40             1    False  \n",
       "41             4    False  \n",
       "42           757     True  \n",
       "43           118    False  \n",
       "44           297     True  \n",
       "45          5281    False  \n",
       "46             1    False  \n",
       "47           626     True  \n",
       "48           930     True  \n",
       "49             0    False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct results: 20.00%\n"
     ]
    }
   ],
   "source": [
    "correct_percentage = (eval_subset['Correct'].sum() / len(eval_subset)) * 100\n",
    "print(f\"Correct results: {correct_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nickk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Solve this math problem, only print the final answer: 373 + 449 ='}]\n",
      "[{'generated_text': 'Solve this math problem, only print the final answer: 747 + 322 ='}]\n",
      "[{'generated_text': 'Solve this math problem, only print the final answer: 324 + 558 ='}]\n",
      "[{'generated_text': 'Solve this math problem, only print the final answer: 437 + 568 ='}]\n",
      "[{'generated_text': 'Solve this math problem, only print the final answer: 339 + 518 ='}]\n",
      "[{'generated_text': 'Solve this math problem, only print the final answer: 920 + 397 + 1234 + 5678 ='}]\n",
      "[{'generated_text': 'Solve this math problem, only print the final answer: 296 + 726 + 1234 + 5678 ='}]\n",
      "[{'generated_text': 'Solve this math problem, only print the final answer: 945 + 859 + 763 + 642 + 531 + 420 + 311 + 200 + 100 ='}]\n",
      "[{'generated_text': 'Solve this math problem, only print the final answer: 779 + 905 + 863 + 741 + 629 + 517 + 405 + 393 + 381 + 379 + 377 + 375 + 373 + 371 + 369 + 367 + 365 + 363 + 361 + 359 + 357 + 355 + 353 + 351 + 349 + 347 + 345 + 34'}]\n",
      "[{'generated_text': 'Solve this math problem, only print the final answer: 465 + 277 + 129 + 44 + 23 + 56 + 89 + 34 + 19 + 98 + 76 + 54 + 32 + 11 + 45 + 96 + 28 + 13 + 67 + 84 + 31 + 22 + 43 + 62 + 81 + 33 + 24 + 55 + 97 + 75 + 46 + 27 + 12 + 45'}]\n",
      "   Operation  Result                                       Raw_Response  \\\n",
      "0  945 + 859    1804  Solve this math problem, only print the final ...   \n",
      "1  324 + 558     882  Solve this math problem, only print the final ...   \n",
      "2  373 + 449     822  Solve this math problem, only print the final ...   \n",
      "3   339 + 51     390  Solve this math problem, only print the final ...   \n",
      "4  437 + 568    1005  Solve this math problem, only print the final ...   \n",
      "5  747 + 322    1069  Solve this math problem, only print the final ...   \n",
      "6  296 + 726    1022  Solve this math problem, only print the final ...   \n",
      "7  920 + 397    1317  Solve this math problem, only print the final ...   \n",
      "8  465 + 277     742  Solve this math problem, only print the final ...   \n",
      "9  779 + 905    1684  Solve this math problem, only print the final ...   \n",
      "\n",
      "   Model_Result  Correct  \n",
      "0           945    False  \n",
      "1           324    False  \n",
      "2           373    False  \n",
      "3           339    False  \n",
      "4           437    False  \n",
      "5           747    False  \n",
      "6           296    False  \n",
      "7           920    False  \n",
      "8           465    False  \n",
      "9           779    False  \n",
      "Correct results: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nickk\\AppData\\Local\\Temp\\ipykernel_33080\\1346933340.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_subset['Raw_Response'], eval_subset['Model_Result'] = zip(*results)\n",
      "C:\\Users\\Nickk\\AppData\\Local\\Temp\\ipykernel_33080\\1346933340.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_subset['Raw_Response'], eval_subset['Model_Result'] = zip(*results)\n",
      "C:\\Users\\Nickk\\AppData\\Local\\Temp\\ipykernel_33080\\1346933340.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_subset['Correct'] = eval_subset.apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Load the evaluation data from the CSV file\n",
    "eval_df = pd.read_csv(\"training_data/addition_operations_eval.csv\")\n",
    "\n",
    "# Function to solve a math problem using the model\n",
    "\n",
    "\n",
    "def solve_math_problem(math_problem):\n",
    "    response = generator(\n",
    "        f\"Solve this math problem, only print the final answer: {math_problem}\",\n",
    "        max_length=150\n",
    "    )\n",
    "    print(response)\n",
    "    solution = response[0]['generated_text'].strip()\n",
    "    return solution\n",
    "\n",
    "# Function to extract only numbers from the model's result\n",
    "\n",
    "\n",
    "def extract_number(text):\n",
    "    match = re.search(r'\\d+', text)\n",
    "    return int(match.group()) if match else None\n",
    "\n",
    "\n",
    "# Set the number of problems to process\n",
    "num_problems = 10\n",
    "\n",
    "# Ensure num_problems is within the valid range\n",
    "if num_problems <= 0:\n",
    "    num_problems = len(eval_df)\n",
    "elif num_problems > len(eval_df):\n",
    "    num_problems = len(eval_df)\n",
    "\n",
    "# Process only first num_problems rows\n",
    "eval_subset = eval_df.head(num_problems)\n",
    "\n",
    "# Function to process each row\n",
    "\n",
    "\n",
    "def process_row(operation):\n",
    "    raw_response = solve_math_problem(operation)\n",
    "    model_result = extract_number(raw_response)\n",
    "    return raw_response, model_result\n",
    "\n",
    "\n",
    "# Use ThreadPoolExecutor to process rows in parallel\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(process_row, eval_subset['Operation']))\n",
    "\n",
    "# Unpack the results and add them to the DataFrame\n",
    "eval_subset['Raw_Response'], eval_subset['Model_Result'] = zip(*results)\n",
    "eval_subset['Correct'] = eval_subset.apply(\n",
    "    lambda row: row['Result'] == row['Model_Result'], axis=1)\n",
    "\n",
    "# Calculate the percentage of correct results\n",
    "correct_percentage = (eval_subset['Correct'].sum() / len(eval_subset)) * 100\n",
    "\n",
    "# Print the comparison results and the percentage of correct results\n",
    "print(eval_subset[['Operation', 'Result',\n",
    "      'Raw_Response', 'Model_Result', 'Correct']])\n",
    "print(f\"Correct results: {correct_percentage:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
