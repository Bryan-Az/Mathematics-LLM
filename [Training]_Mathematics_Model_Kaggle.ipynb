{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.15",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "tpu1vmV38",
      "dataSources": [
        {
          "sourceId": 9910295,
          "sourceType": "datasetVersion",
          "datasetId": 6089178
        }
      ],
      "dockerImageVersionId": 30788,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bryan-Az/Mathematics-LLM/blob/main/%5BTraining%5D_Mathematics_Model_Kaggle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the 'Storybook' Story Generation Model on a Distributed Environment\n",
        "This notebook is running on the TPUv2-8 GPU environment in Kaggle. The pre-trained foundation model we are using is the gated meta-llama/Meta-Llama-3-8B-Instruct, requiring authentication with HuggingFace."
      ],
      "metadata": {
        "id": "Egkwbn23u0-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Installs"
      ],
      "metadata": {
        "id": "4JYklhERu0-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip3 install transformers zstandard jsonlines peft wandb bitsandbytes -q\n",
        "!pip3 install accelerate datasets sentencepiece langchain torch_xla[tpuvm] -q\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow-cpu -q\n",
        "!git clone https://github.com/IsNoobgrammer/Pytorch-Optimizers optims"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-15T18:48:50.615503Z",
          "iopub.execute_input": "2024-11-15T18:48:50.616497Z",
          "iopub.status.idle": "2024-11-15T18:50:14.434997Z",
          "shell.execute_reply.started": "2024-11-15T18:48:50.616462Z",
          "shell.execute_reply": "2024-11-15T18:50:14.433860Z"
        },
        "id": "J2s-6Samu0-d",
        "outputId": "08ecb769-ad02-4e5e-f78a-abaaecf392f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting datasets\n  Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from datasets) (24.1)\nRequirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from datasets) (2024.9.0)\nCollecting dill<0.3.9,>=0.3.0\n  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets) (3.16.1)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/site-packages (from datasets) (0.25.1)\nCollecting multiprocess<0.70.17\n  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/site-packages (from datasets) (4.66.5)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets) (2.2.3)\nCollecting aiohttp\n  Downloading aiohttp-3.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/site-packages (from datasets) (17.0.0)\nCollecting xxhash\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting multidict<7.0,>=4.5\n  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting propcache>=0.2.0\n  Downloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.9/208.9 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting yarl<2.0,>=1.17.0\n  Downloading yarl-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting async-timeout<6.0,>=4.0\n  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\nCollecting aiosignal>=1.1.2\n  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\nCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\nCollecting aiohappyeyeballs>=2.3.0\n  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nInstalling collected packages: xxhash, propcache, multidict, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.9\n    Uninstalling dill-0.3.9:\n      Successfully uninstalled dill-0.3.9\nSuccessfully installed aiohappyeyeballs-2.4.3 aiohttp-3.11.2 aiosignal-1.3.1 async-timeout-5.0.1 datasets-3.1.0 dill-0.3.8 frozenlist-1.5.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.0 xxhash-3.5.0 yarl-1.17.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: torch-xla 2.4.0+libtpu does not provide the extra 'tpuvm'\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nFound existing installation: tensorflow 2.16.1\nUninstalling tensorflow-2.16.1:\n  Successfully uninstalled tensorflow-2.16.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCloning into 'optims'...\nremote: Enumerating objects: 85, done.\u001b[K\nremote: Counting objects: 100% (85/85), done.\u001b[K\nremote: Compressing objects: 100% (84/84), done.\u001b[K:  96% (81/84)\u001b[K\nremote: Total 85 (delta 42), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (85/85), 1.18 MiB | 20.75 MiB/s, done.\nResolving deltas: 100% (42/42), done.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datasets\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_xla.test.test_utils as test_utils\n",
        "import torch_xla.experimental.xla_sharding as xs\n",
        "import torch_xla.core.xla_model as xm\n",
        "from transformers import (\n",
        " AutoTokenizer, AutoModelForCausalLM, set_seed, DataCollatorWithPadding, AutoConfig\n",
        ")\n",
        "\n",
        "from transformers import logging as hf_logging\n",
        "import torch_xla.runtime as xr\n",
        "\n",
        "xr.use_spmd()\n",
        "\n",
        "from torch_xla.experimental.xla_sharding import Mesh\n",
        "\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from datasets import  load_dataset, concatenate_datasets\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.utils.data import Dataset as TorchDataset\n",
        "from torch_xla.utils.checkpoint import checkpoint\n",
        "\n",
        "try:\n",
        "    !export USE_TORCH=True #If we don't do this, transformers will seemingly bork the session upon import. Really weird error.\n",
        "    os.environ[\"PJRT_DEVICE\"] = \"TPU\"\n",
        "    os.environ.pop('TPU_PROCESS_ADDRESSES')\n",
        "    os.environ.pop('CLOUD_TPU_TASK_ID')\n",
        "    hf_logging.set_verbosity_error() # It can still display warnings which is a bit annoying but whatever\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-15T18:50:14.437062Z",
          "iopub.execute_input": "2024-11-15T18:50:14.437485Z",
          "iopub.status.idle": "2024-11-15T18:50:56.537456Z",
          "shell.execute_reply.started": "2024-11-15T18:50:14.437450Z",
          "shell.execute_reply": "2024-11-15T18:50:56.535898Z"
        },
        "id": "DDAdn78Fu0-e",
        "outputId": "7902d6f7-8a7d-4650-924f-028893056387"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/tmp/ipykernel_13/3511949292.py:10: DeprecationWarning: Importing from `torch_xla.experimental.xla_sharding` will be deprecated after 2.2 release. Please use `torch_xla.distributed.spmd` instead.\n  import torch_xla.experimental.xla_sharding as xs\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
        "# login with huggginface for using gated LLaMA foundational models\n",
        "!huggingface-cli login --token $hf_token"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-15T18:50:56.539411Z",
          "iopub.execute_input": "2024-11-15T18:50:56.540670Z",
          "iopub.status.idle": "2024-11-15T18:50:57.593969Z",
          "shell.execute_reply.started": "2024-11-15T18:50:56.540630Z",
          "shell.execute_reply": "2024-11-15T18:50:57.593018Z"
        },
        "id": "J2cdCIKAu0-e",
        "outputId": "43fed299-685a-4798-c420-b22122ff9ab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Tokenizer of the Pre-trained LlaMA 8B Model\n",
        "It's necessary to import the tokenizer of the model for loading the dataset."
      ],
      "metadata": {
        "id": "2gM7be1su0-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_INPUT=4096 #128*32\n",
        "MODEL = \"meta-llama/Meta-Llama-3-8B-Instruct\" #You should be able to use 7B model with no changes! There should be enough HBM\n",
        "SAVED_MODEL = \"Alexis-Az/Story-Generation-LlaMA-8B\"\n",
        "# !export XLA_TENSOR_ALLOCATOR_MAXSIZE=1000000"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-15T18:50:57.596394Z",
          "iopub.execute_input": "2024-11-15T18:50:57.596685Z",
          "iopub.status.idle": "2024-11-15T18:50:57.601046Z",
          "shell.execute_reply.started": "2024-11-15T18:50:57.596656Z",
          "shell.execute_reply": "2024-11-15T18:50:57.600355Z"
        },
        "id": "DaXKh2vVu0-e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "if 'pad_token' not in tokenizer.special_tokens_map:\n",
        "  tokenizer.pad_token=tokenizer.eos_token\n",
        "\n",
        "\n",
        "print(f\"Tokens :\\n {tokenizer.special_tokens_map} \\n\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-15T18:50:57.601910Z",
          "iopub.execute_input": "2024-11-15T18:50:57.602155Z",
          "iopub.status.idle": "2024-11-15T18:50:59.756142Z",
          "shell.execute_reply.started": "2024-11-15T18:50:57.602129Z",
          "shell.execute_reply": "2024-11-15T18:50:59.755246Z"
        },
        "id": "6zSG68mou0-e",
        "outputId": "0a1a8e80-0106-486c-a9b6-0d144d542bf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Tokens :\n {'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>', 'pad_token': '<|eot_id|>'} \n\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Dataset"
      ],
      "metadata": {
        "id": "nysqjvrmu0-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile spmd_util.py\n",
        "import math\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import torch_xla.experimental.xla_sharding as xs\n",
        "import torch_xla.core.xla_model as xm\n",
        "from transformers import LlamaConfig\n",
        "\n",
        "# these are model architecture formatting rules specific to LLaMA\n",
        "LLAMA_RULES = (\n",
        "    (\"model\\\\.embed_tokens\", (\"mp\", \"fsdp\")),\n",
        "    (\"self_attn\\\\.(q_proj|k_proj|v_proj)\", (\"fsdp\", \"mp\")),\n",
        "    (\"self_attn\\\\.o_proj\", (\"mp\", \"fsdp\")),\n",
        "    (\"mlp\\\\.gate_proj\", (\"fsdp\", \"mp\")),\n",
        "    (\"mlp\\\\.down_proj\", (\"mp\", \"fsdp\")),\n",
        "    (\"mlp\\\\.up_proj\", (\"fsdp\", \"mp\")),\n",
        "    (\"lm_head\", (\"fsdp\", \"mp\")),\n",
        ")\n",
        "\n",
        "strkey2id = {\n",
        "    \"dp\": 0, ## usefull for sharding inputs\n",
        "    \"fsdp\": 1, ## Pytorch-Xla (2D-sharding) axis to shard data (mostly mesh shape will be (8,1)) data will be sharded 8 way\n",
        "    \"mp\": 2 ## axis to shard model model will be sharded one way\n",
        "               ## Recommened checking Pytorch-tpu/transfomers on github (xla-fork of transformers)\n",
        "}\n",
        "\n",
        "def partition_module(model, mesh, device=xm.xla_device(), verbose=False):\n",
        "    partition_specs = LLAMA_RULES\n",
        "    rule = [(k, tuple([strkey2id[x] for x in v])) for k, v in partition_specs]\n",
        "\n",
        "    # print(rule)\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        module.to(device)\n",
        "#         print(name, module.__class__.__name__)\n",
        "        if isinstance(module, (nn.Embedding, nn.Linear)):\n",
        "            for rule_pattern, spec in rule:\n",
        "                if re.findall(rule_pattern, name.lower())  : # and (\"lora\" not in name.lower()):\n",
        "                    if verbose:\n",
        "                        print(\"match\", rule_pattern, name)\n",
        "\n",
        "                    xs.mark_sharding(module.weight, mesh, spec)\n",
        "                    break"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-15T18:50:59.757323Z",
          "iopub.execute_input": "2024-11-15T18:50:59.757599Z",
          "iopub.status.idle": "2024-11-15T18:50:59.770023Z",
          "shell.execute_reply.started": "2024-11-15T18:50:59.757572Z",
          "shell.execute_reply": "2024-11-15T18:50:59.769224Z"
        },
        "id": "f9glGcT3u0-e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class InstructionDataset(TorchDataset):\n",
        "    def __init__(self, tokenizer, max_length=1024, dataset=None):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        prompts = self.dataset[idx]\n",
        "        text = \"\"\n",
        "        for prompt in prompts:\n",
        "            #print(prompt)\n",
        "            data = prompts[prompt]\n",
        "            if prompt == 'instruction':\n",
        "                #print(data)\n",
        "                text += f\"<|im_start|>user\\n{data['prompt:']}<|im_end|>\\n\"\n",
        "                ## TO-DO: add prompting for these\n",
        "                #text += f\"\\n{data['words']}\\n\"\n",
        "                #text += f\"\\n{data['features']}\\n\"\n",
        "            if prompt == 'summary':\n",
        "                text += f\"<|im_start|>assistant\\n Here's a really short story: {data}<|im_end|>\"\n",
        "            if prompt == 'story':\\\n",
        "                text += f\"<|im_start|>assistant\\n Here's the full story:{data}<|im_end|>\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        input_ids = self.tokenizer(text, add_special_tokens=True, max_length=self.max_length, truncation=True, padding=\"max_length\", return_attention_mask=True, return_tensors=\"pt\")\n",
        "        return {\n",
        "            \"input_ids\": input_ids[\"input_ids\"].squeeze(0),\n",
        "            \"labels\": input_ids[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\":input_ids[\"attention_mask\"].squeeze(0),\n",
        "        }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-15T18:50:59.771228Z",
          "iopub.execute_input": "2024-11-15T18:50:59.771501Z",
          "iopub.status.idle": "2024-11-15T18:50:59.789867Z",
          "shell.execute_reply.started": "2024-11-15T18:50:59.771473Z",
          "shell.execute_reply": "2024-11-15T18:50:59.789103Z"
        },
        "id": "Y0K5kXltu0-e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=\"/kaggle/input/tinystories\"\n",
        "test_dataset=\"/kaggle/input/tinystories\"\n",
        "# ~1/5 of the dataset is used for validation\n",
        "train_data = load_dataset(train_dataset, split=\"train\").shuffle(seed=69)\n",
        "val = (load_dataset(test_dataset, split=\"train[:1000000]\")).shuffle(seed=420)"
      ],
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2024-11-15T18:50:59.790916Z",
          "iopub.execute_input": "2024-11-15T18:50:59.791175Z",
          "iopub.status.idle": "2024-11-15T18:55:44.474789Z",
          "shell.execute_reply.started": "2024-11-15T18:50:59.791148Z",
          "shell.execute_reply": "2024-11-15T18:55:44.473791Z"
        },
        "id": "S4Uulk-Uu0-e",
        "outputId": "64aa3184-0473-40a2-debe-a5b3dccb233a"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Downloading data: 100%|██████████| 50/50 [00:00<00:00, 23640.54files/s]\nGenerating train split: 4967871 examples [04:41, 17670.52 examples/s]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "FLAGS = {'MAX_INPUT': MAX_INPUT,\n",
        "         'LOGGING_STEPS': 1,\n",
        "         'NUM_EPOCHS': 1,\n",
        "         'PAUSE_STEPS':1000, # asks to exit training after x steps #todo checkpoints\n",
        "         'MAX_STEPS': -1,#Ooverides num epochs\n",
        "         'BATCH_SIZE': 2, #Making batch_size lower then 8 will result in slower training, but will allow for larger models\\context. Fortunately, we have 128GBs. Setting higher batch_size doesn't seem to improve time.\n",
        "          'LEN_TRAIN_DATA': len(train_data),\n",
        "         'VAL_STEPS': 20,\n",
        "         'VAL_BATCH': 5,\n",
        "         'GRAD_ACCUMULATION_STEP':1,\n",
        "         'MAX_GRAD_CLIP':1,\n",
        "        'LEARNING_RATE':6e-5,\n",
        "         'WARMUP_RATIO':0.01,\n",
        "         'OPTIMIZER':'SM3', # default = 'adamw'  options->  ['adamw','SM3','came','adafactor','lion']\n",
        "         'SCHEDULAR':'cosine', # default= 'cosine'     options:-> ['linear','cosine']\n",
        "         'WEIGHT_DECAY':0.1,\n",
        "         'TRAIN_DATASET':train_dataset,\n",
        "         \"TEST_DATASET\":test_dataset,\n",
        "         'WANDB':True,\n",
        "        'PROJECT':'Storybook-Model',\n",
        "        }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-15T18:55:44.476329Z",
          "iopub.execute_input": "2024-11-15T18:55:44.476607Z",
          "iopub.status.idle": "2024-11-15T18:55:44.482235Z",
          "shell.execute_reply.started": "2024-11-15T18:55:44.476580Z",
          "shell.execute_reply": "2024-11-15T18:55:44.481520Z"
        },
        "id": "8mQHcHYPu0-e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = InstructionDataset(tokenizer, dataset=train_data, max_length=FLAGS['MAX_INPUT'])\n",
        "val = InstructionDataset(tokenizer, dataset=val)\n",
        "train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    train_data, num_replicas=8, rank=xm.get_ordinal(), shuffle=True,drop_last=True)\n",
        "training_loader = torch.utils.data.DataLoader(train_data, batch_size=FLAGS[\"BATCH_SIZE\"], sampler=train_sampler)\n",
        "val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    val, num_replicas=8, rank=xm.get_ordinal(), shuffle=True,drop_last=True)\n",
        "testing_loader = torch.utils.data.DataLoader(val, batch_size=FLAGS[\"BATCH_SIZE\"], sampler=val_sampler)\n",
        "\n",
        "print(f\"Max Steps: {len(training_loader)}, Batch size: {8*FLAGS['BATCH_SIZE']}\")\n",
        "print(f\"Val Size: {len(testing_loader)}, Batch Size: {8*FLAGS['BATCH_SIZE']}\")\n",
        "FLAGS['STEPS']=len(training_loader)\n",
        "FLAGS['BATCH_DATA']=FLAGS['BATCH_SIZE']*8 ## 8 CORES ON TPU"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-15T18:55:44.484666Z",
          "iopub.execute_input": "2024-11-15T18:55:44.484925Z",
          "iopub.status.idle": "2024-11-15T18:55:44.525558Z",
          "shell.execute_reply.started": "2024-11-15T18:55:44.484900Z",
          "shell.execute_reply": "2024-11-15T18:55:44.524815Z"
        },
        "id": "Ge_8qEe1u0-f",
        "outputId": "4dade354-304b-4b48-a657-2964da3afe07"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Max Steps: 310492, Batch size: 16\nVal Size: 62500, Batch Size: 16\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Pre-trained Model with LoRa Adapters\n",
        "Adding LoRa will allow us to fine-tune the model on our story dataset."
      ],
      "metadata": {
        "id": "3Rtls26_u0-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(MODEL,torch_dtype=torch.bfloat16)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-15T18:55:44.526495Z",
          "iopub.execute_input": "2024-11-15T18:55:44.526785Z",
          "iopub.status.idle": "2024-11-15T19:02:08.999474Z",
          "shell.execute_reply.started": "2024-11-15T18:55:44.526757Z",
          "shell.execute_reply": "2024-11-15T19:02:08.998813Z"
        },
        "id": "sv-ZUQLju0-f",
        "outputId": "8ddfaad2-d4b1-4c44-e28b-a2513f19f02b"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Downloading shards: 100%|██████████| 4/4 [06:21<00:00, 95.38s/it] \nLoading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.25it/s]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ls=LoraConfig(\n",
        "    r = 48, # Lora Rank ,I would prefer 8-32 for smaller models like 7b\n",
        "    target_modules = ['q_proj', 'down_proj', 'up_proj', 'o_proj', 'v_proj', 'gate_proj', 'k_proj'],\n",
        "    lora_alpha = 16, #weight_scaling\n",
        "    lora_dropout = 0.05, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimize\n",
        "    # modules_to_save = [\"lm_head\", \"embed_tokens\"] ## if you use new chat formats or embedding tokens\n",
        ")\n",
        "model = get_peft_model(model, ls)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-15T19:02:09.000369Z",
          "iopub.execute_input": "2024-11-15T19:02:09.000622Z",
          "iopub.status.idle": "2024-11-15T19:02:10.966133Z",
          "shell.execute_reply.started": "2024-11-15T19:02:09.000596Z",
          "shell.execute_reply": "2024-11-15T19:02:10.965033Z"
        },
        "id": "MPqmhKFBu0-f",
        "outputId": "43c2b063-d371-42fb-b515-5159246b4609"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "trainable params: 125,829,120 || all params: 8,156,090,368 || trainable%: 1.5428\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "device = xm.xla_device()\n",
        "model = model.to(device)\n",
        "from torch_xla.distributed.fsdp.utils import apply_xla_patch_to_nn_linear\n",
        "model = apply_xla_patch_to_nn_linear(model, xs.xla_patched_nn_linear_forward)  #for patching linear layer to use einsum instead of matmul\n",
        "num_devices = xr.global_runtime_device_count()\n",
        "model_axis=1\n",
        "data_axis=num_devices//model_axis\n",
        "mesh_shape = (1,data_axis, model_axis )\n",
        "device_ids = np.array(range(num_devices))\n",
        "mesh = Mesh(device_ids, mesh_shape, ('dp','fsdp', 'mp'))\n",
        "partition_module(model, mesh)\n",
        "training_loader = pl.MpDeviceLoader(training_loader, device)\n",
        "testing_loader = pl.MpDeviceLoader(testing_loader, device)\n",
        "mesh_shape"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-15T19:02:10.967419Z",
          "iopub.execute_input": "2024-11-15T19:02:10.967765Z",
          "iopub.status.idle": "2024-11-15T19:02:23.869018Z",
          "shell.execute_reply.started": "2024-11-15T19:02:10.967736Z",
          "shell.execute_reply": "2024-11-15T19:02:23.868226Z"
        },
        "id": "bol_Oxtzu0-f",
        "outputId": "e27a164c-f3ee-4c39-ee97-212ee16131ad"
      },
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(1, 8, 1)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model"
      ],
      "metadata": {
        "id": "m65WvteNu0-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export XLA_USE_BF16=1\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "__wandb__=FLAGS['WANDB']\n",
        "from torch_xla.amp.syncfree import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup\n",
        "from optims.optim import SM3, CAME , Adafactor\n",
        "# from random import randrange\n",
        "# from bitsandbytes.optim import AdamW8bit\n",
        "# from torchdistx.optimizers import AnyPrecisionAdamW\n",
        "\n",
        "val_step=0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_loss(outputs,labels,pad_id=tokenizer.pad_token_id):\n",
        "  epsilon=1e-8\n",
        "  logits=outputs.logits\n",
        "  logits = logits[..., :-1, :].contiguous()\n",
        "  labels = labels[..., 1:].contiguous()\n",
        "  log_probs = -nn.functional.log_softmax(logits, dim=-1)\n",
        "  if labels.dim() == log_probs.dim() - 1:\n",
        "    labels = labels.unsqueeze(-1)\n",
        "  padding_mask = labels.eq(pad_id)\n",
        "  labels = torch.clamp(labels, min=0)\n",
        "  nll_loss = log_probs.gather(dim=-1, index=labels)\n",
        "  smoothed_loss = log_probs.sum(dim=-1, keepdim=True, dtype=torch.bfloat16)\n",
        "  nll_loss.masked_fill_(padding_mask, 0.0)\n",
        "  smoothed_loss.masked_fill_(padding_mask, 0.0)\n",
        "  num_active_elements = padding_mask.numel() - padding_mask.long().sum()\n",
        "  nll_loss = nll_loss.sum() / num_active_elements\n",
        "  smoothed_loss = smoothed_loss.sum() / (num_active_elements * log_probs.shape[-1])\n",
        "  del labels,logits,padding_mask\n",
        "  return (1-epsilon)*nll_loss + epsilon*smoothed_loss\n",
        "\n",
        "\n",
        "\n",
        "def train(FLAGS):\n",
        "\n",
        "\n",
        "    ### Configuring Training\n",
        "    global val_step\n",
        "    update_params= filter(lambda p: p.requires_grad, model.parameters())\n",
        "    num_iterations = int((FLAGS[\"NUM_EPOCHS\"] * FLAGS['STEPS'] ) // FLAGS['GRAD_ACCUMULATION_STEP'])\n",
        "    warmup_steps = int(num_iterations * FLAGS['WARMUP_RATIO'])\n",
        "\n",
        "    if __wandb__:\n",
        "        wandb.init(project=FLAGS['PROJECT'],config=FLAGS)\n",
        "        wandb.define_metric(\"Validation_loss\", step_metric=\"val_step\")\n",
        "        wandb.define_metric(\"Learning_rate\",step_metric=\"train_step\")\n",
        "        wandb.define_metric(\"train_loss\",step_metric=\"train_step\")\n",
        "\n",
        "    ### Optimizers\n",
        "\n",
        "    if (FLAGS['OPTIMIZER']).lower()=='adamw':\n",
        "        optimizer = AdamW(update_params, eps=1e-8, lr=FLAGS['LEARNING_RATE'], betas=(0.9, 0.999),weight_decay=FLAGS['WEIGHT_DECAY'])\n",
        "    elif (FLAGS['OPTIMIZER']).lower()=='lion':\n",
        "        optimizer = Lion(update_params, lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'])\n",
        "    elif (FLAGS['OPTIMIZER']).lower()=='adafactor':\n",
        "        optimizer = Adafactor(update_params,lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'],scale_parameter=False,relative_step=False)\n",
        "    elif (FLAGS['OPTIMIZER']).lower()=='came':\n",
        "        optimizer = CAME(model.parameters(),lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'],betas=(0.9, 0.999, 0.9999),eps=(1e-30, 1e-16))\n",
        "    else:\n",
        "#         optimizer = Lilith(update_params, eps=1e-8, lr=FLAGS['LEARNING_RATE'],weight_decay=FLAGS['WEIGHT_DECAY'])\n",
        "        optimizer=SM3(update_params,lr=FLAGS['LEARNING_RATE'])\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        if len(param_group[\"params\"]) > 0:\n",
        "            print(param_group[\"params\"][0].device)\n",
        "            break\n",
        "\n",
        "\n",
        "    ### Schedulars\n",
        "\n",
        "    if (FLAGS['SCHEDULAR']).lower()=='linear':\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,num_iterations)\n",
        "    else:\n",
        "        scheduler = get_cosine_schedule_with_warmup(optimizer,warmup_steps,num_iterations)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### Training Loop\n",
        "    val_step=0\n",
        "    check=False #for brakes\n",
        "    for epoch in range(1, FLAGS['NUM_EPOCHS'] + 1):\n",
        "        if check:\n",
        "            break\n",
        "        model.train()\n",
        "        xm.master_print('Epoch {} train begin {}'.format(epoch, test_utils.now()))\n",
        "        for step, batch in enumerate(training_loader):\n",
        "            input_ids, labels,attention_mask = batch[\"input_ids\"].to(device),  batch[\"labels\"].to(device),batch['attention_mask'].to(device)\n",
        "            xs.mark_sharding(input_ids, mesh, (0,1))  ### earlier:-> (0,1) according to pytorch-xla , input/dataloaders must be sharded across ('data',None)\n",
        "            xs.mark_sharding( labels,  mesh, (0,1))  ###\n",
        "            xs.mark_sharding(  attention_mask,  mesh, (0, 1)) ###\n",
        "            outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n",
        "            loss = evaluate_loss(outputs,labels)\n",
        "\n",
        "\n",
        "            if (step + 1) % FLAGS['LOGGING_STEPS'] == 0:\n",
        "                xm.master_print(f'loss: {loss.detach().cpu().item()}, time: {test_utils.now()}, step: {step+1}')\n",
        "            if __wandb__:\n",
        "                wandb.log({\n",
        "                'Learning_rate': optimizer.param_groups[0]['lr'],\n",
        "                'train_loss':  loss.detach().cpu().item(),\n",
        "                'train_step': step + 1 + ((epoch-1) * FLAGS[\"STEPS\"]),\n",
        "                        })\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            del input_ids , attention_mask\n",
        "            loss.backward()\n",
        "            del outputs,loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            if (step+1) % FLAGS['GRAD_ACCUMULATION_STEP'] == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=FLAGS['MAX_GRAD_CLIP']*8)\n",
        "                scheduler.step()\n",
        "                xm.optimizer_step(optimizer,pin_layout=True,barrier=True) ## performs xm.reduce_gradient() , optimizer.step() , xm.mark_step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            if (step+1)% FLAGS['VAL_STEPS'] == 0:\n",
        "                end_index=FLAGS[\"VAL_BATCH\"]\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    total_loss = 0\n",
        "                    total_step = 0\n",
        "                    for stepx, batchx in enumerate(testing_loader):\n",
        "                        input_ids = batchx[\"input_ids\"].to(device)\n",
        "                        labels = batchx[\"labels\"].to(device)\n",
        "                        attention_mask = batchx[\"attention_mask\"].to(device)\n",
        "                        xs.mark_sharding(input_ids, mesh, (0, None))\n",
        "                        xs.mark_sharding(labels, mesh, (0, None))\n",
        "                        xs.mark_sharding( attention_mask,    mesh, (0, None))\n",
        "                        outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n",
        "                        loss = evaluate_loss(outputs,labels)\n",
        "                        total_loss += loss.item()\n",
        "                        total_step +=1\n",
        "                        xm.master_print('----- Time -> {} ----- Validation Batch -> {} ----  Validation Loss -> {:.4f}'.format(test_utils.now(), total_step , loss.item()))\n",
        "                        if __wandb__:\n",
        "                            val_step+=1\n",
        "                            wandb.log({\n",
        "                                'Validation_loss': loss.item(),\n",
        "                                'val_step':val_step,\n",
        "                                    })\n",
        "                        if (stepx+1)%end_index==0:\n",
        "                            break\n",
        "                    model.train()\n",
        "                    average_loss=total_loss/total_step\n",
        "                    xm.master_print('----- Time -> {} ----- Validation Batch Size -> {} ----  Validation Loss -> {:.7f}'.format(test_utils.now(), total_step , average_loss))\n",
        "\n",
        "            if (step+1)% FLAGS['PAUSE_STEPS']==0:\n",
        "                inp=input('want to continue training after {} steps'.format(step+1))\n",
        "                check = bool(\"no\" in inp.lower())\n",
        "                if check:\n",
        "                    break\n",
        "                else:\n",
        "                    pass"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-15T19:02:23.869971Z",
          "iopub.execute_input": "2024-11-15T19:02:23.870224Z",
          "iopub.status.idle": "2024-11-15T19:02:24.615412Z",
          "shell.execute_reply.started": "2024-11-15T19:02:23.870199Z",
          "shell.execute_reply": "2024-11-15T19:02:24.614445Z"
        },
        "id": "1P0FUwdMu0-f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train(FLAGS)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-15T19:02:24.616671Z",
          "iopub.execute_input": "2024-11-15T19:02:24.616919Z",
          "iopub.status.idle": "2024-11-15T22:21:29.845300Z",
          "shell.execute_reply.started": "2024-11-15T19:02:24.616894Z",
          "shell.execute_reply": "2024-11-15T22:21:29.843425Z"
        },
        "id": "yg0Qw3Hxu0-f",
        "outputId": "a4680a68-66a3-42ab-eb14-20cb627453c6"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "  ········\n"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.18.7"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20241115_190659-r9z8avon</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/alebrije-san-jose-state-university/Storybook-Model/runs/r9z8avon' target=\"_blank\">sparkling-eon-21</a></strong> to <a href='https://wandb.ai/alebrije-san-jose-state-university/Storybook-Model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/alebrije-san-jose-state-university/Storybook-Model' target=\"_blank\">https://wandb.ai/alebrije-san-jose-state-university/Storybook-Model</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/alebrije-san-jose-state-university/Storybook-Model/runs/r9z8avon' target=\"_blank\">https://wandb.ai/alebrije-san-jose-state-university/Storybook-Model/runs/r9z8avon</a>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "xla:0\nEpoch 1 train begin 19:07:00\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "loss: 2.0557291507720947, time: 19:07:45, step: 1\nloss: 2.1863489151000977, time: 19:09:15, step: 2\nloss: 2.0923361778259277, time: 19:10:58, step: 3\nloss: 2.152149200439453, time: 19:11:04, step: 4\nloss: 2.03597092628479, time: 19:11:09, step: 5\nloss: 2.006934404373169, time: 19:11:15, step: 6\nloss: 1.9849658012390137, time: 19:11:20, step: 7\nloss: 2.0092203617095947, time: 19:11:26, step: 8\nloss: 2.1371185779571533, time: 19:11:31, step: 9\nloss: 1.9907788038253784, time: 19:11:37, step: 10\nloss: 1.9026447534561157, time: 19:11:42, step: 11\nloss: 1.9469952583312988, time: 19:11:48, step: 12\nloss: 1.917063593864441, time: 19:11:53, step: 13\nloss: 1.8778440952301025, time: 19:11:58, step: 14\nloss: 2.0275919437408447, time: 19:12:04, step: 15\nloss: 2.18735933303833, time: 19:12:09, step: 16\nloss: 2.0178208351135254, time: 19:12:15, step: 17\nloss: 1.9779554605484009, time: 19:12:20, step: 18\nloss: 2.1814520359039307, time: 19:12:26, step: 19\nloss: 1.79534912109375, time: 19:12:31, step: 20\n----- Time -> 19:12:56 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1837\n----- Time -> 19:13:14 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0475\n----- Time -> 19:13:15 ----- Validation Batch -> 3 ----  Validation Loss -> 1.7092\n----- Time -> 19:13:15 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8765\n----- Time -> 19:13:16 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0478\n----- Time -> 19:13:16 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9729154\nloss: 2.125802993774414, time: 19:13:17, step: 21\nloss: 2.106996774673462, time: 19:13:23, step: 22\nloss: 2.022890090942383, time: 19:13:28, step: 23\nloss: 1.986005187034607, time: 19:13:34, step: 24\nloss: 2.000138282775879, time: 19:13:39, step: 25\nloss: 1.9379510879516602, time: 19:13:45, step: 26\nloss: 1.9847849607467651, time: 19:13:50, step: 27\nloss: 1.7909184694290161, time: 19:13:56, step: 28\nloss: 2.242060899734497, time: 19:14:01, step: 29\nloss: 2.0686590671539307, time: 19:14:06, step: 30\nloss: 2.074307441711426, time: 19:14:12, step: 31\nloss: 2.036635637283325, time: 19:14:17, step: 32\nloss: 1.8574237823486328, time: 19:14:23, step: 33\nloss: 2.042170286178589, time: 19:14:28, step: 34\nloss: 1.867201805114746, time: 19:14:34, step: 35\nloss: 2.1938490867614746, time: 19:14:39, step: 36\nloss: 2.057119369506836, time: 19:14:45, step: 37\nloss: 2.047778844833374, time: 19:14:50, step: 38\nloss: 1.881002426147461, time: 19:14:55, step: 39\nloss: 2.076843500137329, time: 19:15:01, step: 40\n----- Time -> 19:15:05 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1807\n----- Time -> 19:15:06 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0431\n----- Time -> 19:15:06 ----- Validation Batch -> 3 ----  Validation Loss -> 1.7078\n----- Time -> 19:15:07 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8703\n----- Time -> 19:15:08 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0389\n----- Time -> 19:15:08 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9681515\nloss: 1.765630841255188, time: 19:15:09, step: 41\nloss: 1.8957148790359497, time: 19:15:15, step: 42\nloss: 2.00846004486084, time: 19:15:20, step: 43\nloss: 1.9282079935073853, time: 19:15:26, step: 44\nloss: 1.9895052909851074, time: 19:15:31, step: 45\nloss: 2.120845079421997, time: 19:15:36, step: 46\nloss: 2.152872085571289, time: 19:15:42, step: 47\nloss: 1.858435869216919, time: 19:15:47, step: 48\nloss: 1.9042311906814575, time: 19:15:53, step: 49\nloss: 2.2140371799468994, time: 19:15:58, step: 50\nloss: 1.9989145994186401, time: 19:16:04, step: 51\nloss: 1.906017541885376, time: 19:16:09, step: 52\nloss: 1.8333196640014648, time: 19:16:15, step: 53\nloss: 2.031994342803955, time: 19:16:20, step: 54\nloss: 1.5781136751174927, time: 19:16:25, step: 55\nloss: 1.8272475004196167, time: 19:16:31, step: 56\nloss: 1.9089046716690063, time: 19:16:36, step: 57\nloss: 1.952633261680603, time: 19:16:42, step: 58\nloss: 1.9686188697814941, time: 19:16:47, step: 59\nloss: 2.032061815261841, time: 19:16:53, step: 60\n----- Time -> 19:16:57 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1792\n----- Time -> 19:16:58 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0418\n----- Time -> 19:16:58 ----- Validation Batch -> 3 ----  Validation Loss -> 1.7053\n----- Time -> 19:16:59 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8681\n----- Time -> 19:16:59 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0415\n----- Time -> 19:16:59 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9671791\nloss: 2.000704050064087, time: 19:17:01, step: 61\nloss: 1.7957587242126465, time: 19:17:06, step: 62\nloss: 1.9754414558410645, time: 19:17:12, step: 63\nloss: 2.088224172592163, time: 19:17:17, step: 64\nloss: 1.7902820110321045, time: 19:17:23, step: 65\nloss: 1.9498635530471802, time: 19:17:28, step: 66\nloss: 1.799307107925415, time: 19:17:34, step: 67\nloss: 1.9435480833053589, time: 19:17:39, step: 68\nloss: 2.0335280895233154, time: 19:17:45, step: 69\nloss: 2.1420881748199463, time: 19:17:50, step: 70\nloss: 2.079869508743286, time: 19:17:55, step: 71\nloss: 2.0501017570495605, time: 19:18:01, step: 72\nloss: 2.1203925609588623, time: 19:18:06, step: 73\nloss: 2.0273334980010986, time: 19:18:12, step: 74\nloss: 2.1327223777770996, time: 19:18:17, step: 75\nloss: 2.0675830841064453, time: 19:18:23, step: 76\nloss: 1.829092264175415, time: 19:18:28, step: 77\nloss: 1.8652344942092896, time: 19:18:34, step: 78\nloss: 1.9886335134506226, time: 19:18:39, step: 79\nloss: 1.9345674514770508, time: 19:18:45, step: 80\n----- Time -> 19:18:49 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1793\n----- Time -> 19:18:50 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0388\n----- Time -> 19:18:50 ----- Validation Batch -> 3 ----  Validation Loss -> 1.7025\n----- Time -> 19:18:51 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8661\n----- Time -> 19:18:51 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0384\n----- Time -> 19:18:51 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9650510\nloss: 1.8677620887756348, time: 19:18:53, step: 81\nloss: 1.8204433917999268, time: 19:18:58, step: 82\nloss: 1.555048942565918, time: 19:19:04, step: 83\nloss: 1.8953567743301392, time: 19:19:09, step: 84\nloss: 2.0430757999420166, time: 19:19:15, step: 85\nloss: 1.8599470853805542, time: 19:19:20, step: 86\nloss: 2.1228928565979004, time: 19:19:26, step: 87\nloss: 1.8101216554641724, time: 19:19:31, step: 88\nloss: 1.8592885732650757, time: 19:19:36, step: 89\nloss: 1.8413031101226807, time: 19:19:42, step: 90\nloss: 2.0489256381988525, time: 19:19:47, step: 91\nloss: 1.8844753503799438, time: 19:19:53, step: 92\nloss: 1.8855576515197754, time: 19:19:58, step: 93\nloss: 2.135941743850708, time: 19:20:04, step: 94\nloss: 2.060351610183716, time: 19:20:09, step: 95\nloss: 1.9843827486038208, time: 19:20:15, step: 96\nloss: 2.1559863090515137, time: 19:20:20, step: 97\nloss: 2.0186939239501953, time: 19:20:26, step: 98\nloss: 1.8558990955352783, time: 19:20:31, step: 99\nloss: 1.6845786571502686, time: 19:20:37, step: 100\n----- Time -> 19:20:41 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1776\n----- Time -> 19:20:42 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0368\n----- Time -> 19:20:42 ----- Validation Batch -> 3 ----  Validation Loss -> 1.7025\n----- Time -> 19:20:43 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8649\n----- Time -> 19:20:43 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0380\n----- Time -> 19:20:43 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9639625\nloss: 2.0334393978118896, time: 19:20:45, step: 101\nloss: 1.9971426725387573, time: 19:20:50, step: 102\nloss: 1.9035310745239258, time: 19:20:56, step: 103\nloss: 1.9529293775558472, time: 19:21:01, step: 104\nloss: 1.9649980068206787, time: 19:21:07, step: 105\nloss: 1.8966041803359985, time: 19:21:12, step: 106\nloss: 2.2396819591522217, time: 19:21:18, step: 107\nloss: 2.241466522216797, time: 19:21:23, step: 108\nloss: 2.142343282699585, time: 19:21:29, step: 109\nloss: 1.7895311117172241, time: 19:21:34, step: 110\nloss: 1.9024549722671509, time: 19:21:39, step: 111\nloss: 1.9097087383270264, time: 19:21:45, step: 112\nloss: 2.087989330291748, time: 19:21:50, step: 113\nloss: 2.119981527328491, time: 19:21:56, step: 114\nloss: 2.0928919315338135, time: 19:22:01, step: 115\nloss: 1.9822399616241455, time: 19:22:07, step: 116\nloss: 1.8290777206420898, time: 19:22:12, step: 117\nloss: 1.9263767004013062, time: 19:22:18, step: 118\nloss: 1.9879463911056519, time: 19:22:23, step: 119\nloss: 2.0873970985412598, time: 19:22:29, step: 120\n----- Time -> 19:22:33 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1767\n----- Time -> 19:22:34 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0360\n----- Time -> 19:22:34 ----- Validation Batch -> 3 ----  Validation Loss -> 1.7018\n----- Time -> 19:22:35 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8638\n----- Time -> 19:22:35 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0366\n----- Time -> 19:22:35 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9629770\nloss: 1.9730292558670044, time: 19:22:37, step: 121\nloss: 2.016780138015747, time: 19:22:42, step: 122\nloss: 1.8851724863052368, time: 19:22:48, step: 123\nloss: 1.7089799642562866, time: 19:22:53, step: 124\nloss: 1.966139793395996, time: 19:22:59, step: 125\nloss: 1.9739651679992676, time: 19:23:04, step: 126\nloss: 1.8522958755493164, time: 19:23:10, step: 127\nloss: 1.984482765197754, time: 19:23:15, step: 128\nloss: 1.985674500465393, time: 19:23:20, step: 129\nloss: 2.1608164310455322, time: 19:23:26, step: 130\nloss: 1.8968801498413086, time: 19:23:31, step: 131\nloss: 1.8672651052474976, time: 19:23:37, step: 132\nloss: 2.055259943008423, time: 19:23:42, step: 133\nloss: 2.142052173614502, time: 19:23:48, step: 134\nloss: 1.8333736658096313, time: 19:23:53, step: 135\nloss: 2.108691930770874, time: 19:23:59, step: 136\nloss: 2.009861469268799, time: 19:24:04, step: 137\nloss: 1.9560035467147827, time: 19:24:10, step: 138\nloss: 1.8722339868545532, time: 19:24:15, step: 139\nloss: 2.0392513275146484, time: 19:24:21, step: 140\n----- Time -> 19:24:25 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1759\n----- Time -> 19:24:26 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0362\n----- Time -> 19:24:26 ----- Validation Batch -> 3 ----  Validation Loss -> 1.7007\n----- Time -> 19:24:27 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8625\n----- Time -> 19:24:27 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0339\n----- Time -> 19:24:27 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9618353\nloss: 1.9801194667816162, time: 19:24:29, step: 141\nloss: 2.0051560401916504, time: 19:24:34, step: 142\nloss: 1.8735945224761963, time: 19:24:40, step: 143\nloss: 2.0592031478881836, time: 19:24:45, step: 144\nloss: 2.018012285232544, time: 19:24:51, step: 145\nloss: 1.9194003343582153, time: 19:24:56, step: 146\nloss: 2.1280038356781006, time: 19:25:02, step: 147\nloss: 1.9054025411605835, time: 19:25:07, step: 148\nloss: 2.0256307125091553, time: 19:25:12, step: 149\nloss: 1.9802286624908447, time: 19:25:18, step: 150\nloss: 2.0779263973236084, time: 19:25:23, step: 151\nloss: 1.9776110649108887, time: 19:25:29, step: 152\nloss: 2.123450756072998, time: 19:25:34, step: 153\nloss: 1.893123984336853, time: 19:25:40, step: 154\nloss: 2.1856906414031982, time: 19:25:45, step: 155\nloss: 1.9045768976211548, time: 19:25:51, step: 156\nloss: 2.0709829330444336, time: 19:25:56, step: 157\nloss: 1.8140428066253662, time: 19:26:01, step: 158\nloss: 2.164565324783325, time: 19:26:07, step: 159\nloss: 1.978319764137268, time: 19:26:12, step: 160\n----- Time -> 19:26:17 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1745\n----- Time -> 19:26:17 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0368\n----- Time -> 19:26:18 ----- Validation Batch -> 3 ----  Validation Loss -> 1.7011\n----- Time -> 19:26:18 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8641\n----- Time -> 19:26:19 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0353\n----- Time -> 19:26:19 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9623656\nloss: 1.9549169540405273, time: 19:26:21, step: 161\nloss: 2.0172817707061768, time: 19:26:26, step: 162\nloss: 2.0552847385406494, time: 19:26:32, step: 163\nloss: 1.9600275754928589, time: 19:26:37, step: 164\nloss: 2.2689690589904785, time: 19:26:42, step: 165\nloss: 1.9915540218353271, time: 19:26:48, step: 166\nloss: 2.026397705078125, time: 19:26:53, step: 167\nloss: 1.9677625894546509, time: 19:26:59, step: 168\nloss: 1.8972127437591553, time: 19:27:04, step: 169\nloss: 1.9263622760772705, time: 19:27:10, step: 170\nloss: 1.8790141344070435, time: 19:27:15, step: 171\nloss: 2.080190420150757, time: 19:27:21, step: 172\nloss: 1.9674057960510254, time: 19:27:26, step: 173\nloss: 2.060121536254883, time: 19:27:32, step: 174\nloss: 1.8147633075714111, time: 19:27:37, step: 175\nloss: 1.814862847328186, time: 19:27:42, step: 176\nloss: 2.047666549682617, time: 19:27:48, step: 177\nloss: 1.932015299797058, time: 19:27:53, step: 178\nloss: 2.060898780822754, time: 19:27:59, step: 179\nloss: 1.8601735830307007, time: 19:28:04, step: 180\n----- Time -> 19:28:09 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1736\n----- Time -> 19:28:09 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0366\n----- Time -> 19:28:10 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6997\n----- Time -> 19:28:10 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8612\n----- Time -> 19:28:11 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0326\n----- Time -> 19:28:11 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9607422\nloss: 1.8965184688568115, time: 19:28:12, step: 181\nloss: 1.9640367031097412, time: 19:28:18, step: 182\nloss: 2.1172943115234375, time: 19:28:23, step: 183\nloss: 2.115906000137329, time: 19:28:29, step: 184\nloss: 1.8721001148223877, time: 19:28:34, step: 185\nloss: 1.9936339855194092, time: 19:28:40, step: 186\nloss: 1.8817631006240845, time: 19:28:45, step: 187\nloss: 2.133779525756836, time: 19:28:51, step: 188\nloss: 2.0601086616516113, time: 19:28:56, step: 189\nloss: 2.048492908477783, time: 19:29:02, step: 190\nloss: 2.175079107284546, time: 19:29:07, step: 191\nloss: 2.240431547164917, time: 19:29:13, step: 192\nloss: 1.9797086715698242, time: 19:29:18, step: 193\nloss: 2.0135295391082764, time: 19:29:23, step: 194\nloss: 1.9050990343093872, time: 19:29:29, step: 195\nloss: 1.879366159439087, time: 19:29:34, step: 196\nloss: 2.0020980834960938, time: 19:29:40, step: 197\nloss: 1.9825459718704224, time: 19:29:45, step: 198\nloss: 1.9646745920181274, time: 19:29:51, step: 199\nloss: 2.1602377891540527, time: 19:29:56, step: 200\n----- Time -> 19:30:01 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1745\n----- Time -> 19:30:01 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0372\n----- Time -> 19:30:02 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6988\n----- Time -> 19:30:02 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8611\n----- Time -> 19:30:03 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0326\n----- Time -> 19:30:03 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9608483\nloss: 1.9669207334518433, time: 19:30:04, step: 201\nloss: 2.0605597496032715, time: 19:30:10, step: 202\nloss: 2.0289080142974854, time: 19:30:15, step: 203\nloss: 2.018908977508545, time: 19:30:21, step: 204\nloss: 2.1218175888061523, time: 19:30:26, step: 205\nloss: 1.8809794187545776, time: 19:30:32, step: 206\nloss: 1.9748698472976685, time: 19:30:37, step: 207\nloss: 1.9756118059158325, time: 19:30:43, step: 208\nloss: 1.7463579177856445, time: 19:30:48, step: 209\nloss: 1.9678579568862915, time: 19:30:54, step: 210\nloss: 1.9303678274154663, time: 19:30:59, step: 211\nloss: 2.1171064376831055, time: 19:31:04, step: 212\nloss: 2.1633360385894775, time: 19:31:10, step: 213\nloss: 1.8859976530075073, time: 19:31:15, step: 214\nloss: 2.0348358154296875, time: 19:31:21, step: 215\nloss: 1.7584903240203857, time: 19:31:26, step: 216\nloss: 2.0985541343688965, time: 19:31:32, step: 217\nloss: 2.07572865486145, time: 19:31:37, step: 218\nloss: 1.6504292488098145, time: 19:31:43, step: 219\nloss: 1.9688754081726074, time: 19:31:48, step: 220\n----- Time -> 19:31:52 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1709\n----- Time -> 19:31:53 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0317\n----- Time -> 19:31:54 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6990\n----- Time -> 19:31:54 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8591\n----- Time -> 19:31:55 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0312\n----- Time -> 19:31:55 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9583779\nloss: 1.9361504316329956, time: 19:31:56, step: 221\nloss: 1.885025978088379, time: 19:32:02, step: 222\nloss: 1.9491965770721436, time: 19:32:07, step: 223\nloss: 1.928341269493103, time: 19:32:13, step: 224\nloss: 1.9236088991165161, time: 19:32:18, step: 225\nloss: 1.8290235996246338, time: 19:32:24, step: 226\nloss: 1.960411787033081, time: 19:32:29, step: 227\nloss: 1.9201682806015015, time: 19:32:35, step: 228\nloss: 2.0631606578826904, time: 19:32:40, step: 229\nloss: 1.9919185638427734, time: 19:32:45, step: 230\nloss: 1.8268015384674072, time: 19:32:51, step: 231\nloss: 1.8202828168869019, time: 19:32:56, step: 232\nloss: 1.927521824836731, time: 19:33:02, step: 233\nloss: 2.027769088745117, time: 19:33:07, step: 234\nloss: 2.066406726837158, time: 19:33:13, step: 235\nloss: 2.186659574508667, time: 19:33:18, step: 236\nloss: 1.9766288995742798, time: 19:33:24, step: 237\nloss: 2.0336196422576904, time: 19:33:29, step: 238\nloss: 1.9927598237991333, time: 19:33:34, step: 239\nloss: 1.93700110912323, time: 19:33:40, step: 240\n----- Time -> 19:33:44 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1726\n----- Time -> 19:33:45 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0333\n----- Time -> 19:33:45 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6985\n----- Time -> 19:33:46 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8604\n----- Time -> 19:33:46 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0305\n----- Time -> 19:33:46 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9590488\nloss: 2.1075196266174316, time: 19:33:48, step: 241\nloss: 2.1440913677215576, time: 19:33:54, step: 242\nloss: 1.9721969366073608, time: 19:33:59, step: 243\nloss: 1.942121982574463, time: 19:34:04, step: 244\nloss: 2.156414031982422, time: 19:34:10, step: 245\nloss: 1.996863603591919, time: 19:34:15, step: 246\nloss: 1.844609022140503, time: 19:34:21, step: 247\nloss: 2.1399459838867188, time: 19:34:26, step: 248\nloss: 1.8190031051635742, time: 19:34:32, step: 249\nloss: 1.8494112491607666, time: 19:34:37, step: 250\nloss: 2.1202077865600586, time: 19:34:43, step: 251\nloss: 1.8856889009475708, time: 19:34:48, step: 252\nloss: 2.154684543609619, time: 19:34:54, step: 253\nloss: 2.057343006134033, time: 19:34:59, step: 254\nloss: 1.8827612400054932, time: 19:35:05, step: 255\nloss: 2.166106700897217, time: 19:35:10, step: 256\nloss: 1.858378291130066, time: 19:35:15, step: 257\nloss: 2.026440143585205, time: 19:35:21, step: 258\nloss: 1.9722295999526978, time: 19:35:26, step: 259\nloss: 2.083627939224243, time: 19:35:32, step: 260\n----- Time -> 19:35:36 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1705\n----- Time -> 19:35:37 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0315\n----- Time -> 19:35:37 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6975\n----- Time -> 19:35:38 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8558\n----- Time -> 19:35:38 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0306\n----- Time -> 19:35:38 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9571940\nloss: 1.9520094394683838, time: 19:35:40, step: 261\nloss: 1.8745912313461304, time: 19:35:45, step: 262\nloss: 1.8949896097183228, time: 19:35:51, step: 263\nloss: 1.7226884365081787, time: 19:35:56, step: 264\nloss: 1.9617685079574585, time: 19:36:02, step: 265\nloss: 1.9679688215255737, time: 19:36:07, step: 266\nloss: 2.1156396865844727, time: 19:36:13, step: 267\nloss: 2.0536067485809326, time: 19:36:18, step: 268\nloss: 2.0724267959594727, time: 19:36:24, step: 269\nloss: 2.002035617828369, time: 19:36:29, step: 270\nloss: 1.9700297117233276, time: 19:36:34, step: 271\nloss: 1.8093265295028687, time: 19:36:40, step: 272\nloss: 2.1592161655426025, time: 19:36:45, step: 273\nloss: 1.7061837911605835, time: 19:36:51, step: 274\nloss: 1.9293115139007568, time: 19:36:56, step: 275\nloss: 2.012821674346924, time: 19:37:02, step: 276\nloss: 1.9584746360778809, time: 19:37:07, step: 277\nloss: 1.9889322519302368, time: 19:37:13, step: 278\nloss: 1.9052857160568237, time: 19:37:18, step: 279\nloss: 1.9780972003936768, time: 19:37:24, step: 280\n----- Time -> 19:37:28 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1691\n----- Time -> 19:37:29 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0323\n----- Time -> 19:37:29 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6967\n----- Time -> 19:37:30 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8572\n----- Time -> 19:37:30 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0280\n----- Time -> 19:37:30 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9566604\nloss: 1.9184800386428833, time: 19:37:32, step: 281\nloss: 1.5611193180084229, time: 19:37:37, step: 282\nloss: 1.969461441040039, time: 19:37:43, step: 283\nloss: 1.7339203357696533, time: 19:37:48, step: 284\nloss: 2.089998483657837, time: 19:37:54, step: 285\nloss: 2.2726006507873535, time: 19:37:59, step: 286\nloss: 2.1197147369384766, time: 19:38:05, step: 287\nloss: 2.0324954986572266, time: 19:38:10, step: 288\nloss: 2.033132553100586, time: 19:38:15, step: 289\nloss: 2.0583887100219727, time: 19:38:21, step: 290\nloss: 2.1532809734344482, time: 19:38:26, step: 291\nloss: 1.9652431011199951, time: 19:38:32, step: 292\nloss: 1.8886523246765137, time: 19:38:37, step: 293\nloss: 2.1437134742736816, time: 19:38:43, step: 294\nloss: 2.036529779434204, time: 19:38:48, step: 295\nloss: 1.9049601554870605, time: 19:38:54, step: 296\nloss: 2.052494764328003, time: 19:38:59, step: 297\nloss: 2.1429338455200195, time: 19:39:05, step: 298\nloss: 1.9742430448532104, time: 19:39:10, step: 299\nloss: 2.0790607929229736, time: 19:39:16, step: 300\n----- Time -> 19:39:20 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1700\n----- Time -> 19:39:21 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0302\n----- Time -> 19:39:21 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6962\n----- Time -> 19:39:22 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8533\n----- Time -> 19:39:22 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0269\n----- Time -> 19:39:22 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9553170\nloss: 1.9242348670959473, time: 19:39:24, step: 301\nloss: 1.8954839706420898, time: 19:39:29, step: 302\nloss: 1.8774296045303345, time: 19:39:35, step: 303\nloss: 2.3058266639709473, time: 19:39:40, step: 304\nloss: 1.9028675556182861, time: 19:39:46, step: 305\nloss: 1.9950772523880005, time: 19:39:51, step: 306\nloss: 2.0188064575195312, time: 19:39:57, step: 307\nloss: 2.0061328411102295, time: 19:40:02, step: 308\nloss: 1.9792159795761108, time: 19:40:07, step: 309\nloss: 2.045820713043213, time: 19:40:13, step: 310\nloss: 1.8920810222625732, time: 19:40:18, step: 311\nloss: 1.8755275011062622, time: 19:40:24, step: 312\nloss: 2.1636061668395996, time: 19:40:29, step: 313\nloss: 2.0002005100250244, time: 19:40:35, step: 314\nloss: 1.8440937995910645, time: 19:40:40, step: 315\nloss: 2.0145020484924316, time: 19:40:46, step: 316\nloss: 2.043159246444702, time: 19:40:51, step: 317\nloss: 1.964846134185791, time: 19:40:57, step: 318\nloss: 1.9826542139053345, time: 19:41:02, step: 319\nloss: 1.954805612564087, time: 19:41:07, step: 320\n----- Time -> 19:41:12 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1664\n----- Time -> 19:41:12 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0303\n----- Time -> 19:41:13 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6943\n----- Time -> 19:41:14 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8553\n----- Time -> 19:41:14 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0274\n----- Time -> 19:41:14 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9547662\nloss: 2.165952682495117, time: 19:41:16, step: 321\nloss: 1.950669527053833, time: 19:41:21, step: 322\nloss: 1.8721330165863037, time: 19:41:27, step: 323\nloss: 1.8619250059127808, time: 19:41:32, step: 324\nloss: 1.964345097541809, time: 19:41:37, step: 325\nloss: 2.065495252609253, time: 19:41:43, step: 326\nloss: 2.1155872344970703, time: 19:41:48, step: 327\nloss: 1.9480748176574707, time: 19:41:54, step: 328\nloss: 1.768597960472107, time: 19:41:59, step: 329\nloss: 1.8603832721710205, time: 19:42:05, step: 330\nloss: 2.0152077674865723, time: 19:42:10, step: 331\nloss: 2.0032825469970703, time: 19:42:16, step: 332\nloss: 2.0964198112487793, time: 19:42:21, step: 333\nloss: 2.0167160034179688, time: 19:42:26, step: 334\nloss: 2.109351634979248, time: 19:42:32, step: 335\nloss: 1.9979463815689087, time: 19:42:37, step: 336\nloss: 1.998585820198059, time: 19:42:43, step: 337\nloss: 1.9798779487609863, time: 19:42:48, step: 338\nloss: 1.9347667694091797, time: 19:42:54, step: 339\nloss: 1.8771315813064575, time: 19:42:59, step: 340\n----- Time -> 19:43:04 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1651\n----- Time -> 19:43:04 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0274\n----- Time -> 19:43:05 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6947\n----- Time -> 19:43:05 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8533\n----- Time -> 19:43:06 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0271\n----- Time -> 19:43:06 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9535148\nloss: 1.9742072820663452, time: 19:43:07, step: 341\nloss: 1.9056214094161987, time: 19:43:13, step: 342\nloss: 1.8949646949768066, time: 19:43:18, step: 343\nloss: 1.9649348258972168, time: 19:43:24, step: 344\nloss: 2.065030574798584, time: 19:43:29, step: 345\nloss: 1.9314786195755005, time: 19:43:35, step: 346\nloss: 2.1780307292938232, time: 19:43:40, step: 347\nloss: 1.719754695892334, time: 19:43:46, step: 348\nloss: 1.929106593132019, time: 19:43:51, step: 349\nloss: 1.8672394752502441, time: 19:43:57, step: 350\nloss: 1.931885838508606, time: 19:44:02, step: 351\nloss: 1.9618595838546753, time: 19:44:08, step: 352\nloss: 1.7112700939178467, time: 19:44:13, step: 353\nloss: 2.0086793899536133, time: 19:44:18, step: 354\nloss: 1.967212438583374, time: 19:44:24, step: 355\nloss: 1.883890986442566, time: 19:44:29, step: 356\nloss: 2.040088176727295, time: 19:44:35, step: 357\nloss: 1.7837802171707153, time: 19:44:40, step: 358\nloss: 1.9853565692901611, time: 19:44:46, step: 359\nloss: 1.8991060256958008, time: 19:44:51, step: 360\n----- Time -> 19:44:56 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1645\n----- Time -> 19:44:56 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0263\n----- Time -> 19:44:57 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6933\n----- Time -> 19:44:57 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8546\n----- Time -> 19:44:58 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0262\n----- Time -> 19:44:58 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9529591\nloss: 1.9513304233551025, time: 19:44:59, step: 361\nloss: 2.023921251296997, time: 19:45:05, step: 362\nloss: 1.9398107528686523, time: 19:45:10, step: 363\nloss: 2.032241106033325, time: 19:45:16, step: 364\nloss: 1.8616161346435547, time: 19:45:21, step: 365\nloss: 1.9210193157196045, time: 19:45:27, step: 366\nloss: 2.254577398300171, time: 19:45:32, step: 367\nloss: 1.8970109224319458, time: 19:45:38, step: 368\nloss: 2.020470380783081, time: 19:45:43, step: 369\nloss: 2.13188099861145, time: 19:45:48, step: 370\nloss: 2.1193594932556152, time: 19:45:54, step: 371\nloss: 1.779984474182129, time: 19:45:59, step: 372\nloss: 2.2168712615966797, time: 19:46:05, step: 373\nloss: 1.92538583278656, time: 19:46:10, step: 374\nloss: 1.9616056680679321, time: 19:46:16, step: 375\nloss: 2.007610559463501, time: 19:46:21, step: 376\nloss: 1.9255434274673462, time: 19:46:27, step: 377\nloss: 1.8323330879211426, time: 19:46:32, step: 378\nloss: 1.7047829627990723, time: 19:46:37, step: 379\nloss: 1.9497328996658325, time: 19:46:43, step: 380\n----- Time -> 19:46:47 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1627\n----- Time -> 19:46:48 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0267\n----- Time -> 19:46:48 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6933\n----- Time -> 19:46:49 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8541\n----- Time -> 19:46:49 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0235\n----- Time -> 19:46:49 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9520800\nloss: 1.9202834367752075, time: 19:46:51, step: 381\nloss: 2.118791341781616, time: 19:46:57, step: 382\nloss: 1.9935990571975708, time: 19:47:02, step: 383\nloss: 2.001556634902954, time: 19:47:07, step: 384\nloss: 2.113659381866455, time: 19:47:13, step: 385\nloss: 1.9115833044052124, time: 19:47:18, step: 386\nloss: 1.9632567167282104, time: 19:47:24, step: 387\nloss: 2.166146755218506, time: 19:47:29, step: 388\nloss: 1.888206124305725, time: 19:47:35, step: 389\nloss: 1.941840648651123, time: 19:47:40, step: 390\nloss: 1.765268325805664, time: 19:47:46, step: 391\nloss: 2.022343397140503, time: 19:47:51, step: 392\nloss: 1.9200334548950195, time: 19:47:57, step: 393\nloss: 1.904725193977356, time: 19:48:02, step: 394\nloss: 1.7574797868728638, time: 19:48:07, step: 395\nloss: 1.8206676244735718, time: 19:48:13, step: 396\nloss: 2.0384457111358643, time: 19:48:18, step: 397\nloss: 1.995573878288269, time: 19:48:24, step: 398\nloss: 1.9071639776229858, time: 19:48:29, step: 399\nloss: 1.9317365884780884, time: 19:48:35, step: 400\n----- Time -> 19:48:39 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1629\n----- Time -> 19:48:40 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0226\n----- Time -> 19:48:40 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6892\n----- Time -> 19:48:41 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8495\n----- Time -> 19:48:41 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0210\n----- Time -> 19:48:41 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9490315\nloss: 2.093935012817383, time: 19:48:43, step: 401\nloss: 2.104694128036499, time: 19:48:48, step: 402\nloss: 1.9261127710342407, time: 19:48:54, step: 403\nloss: 1.9240013360977173, time: 19:48:59, step: 404\nloss: 1.9621467590332031, time: 19:49:05, step: 405\nloss: 2.0279510021209717, time: 19:49:10, step: 406\nloss: 1.9193531274795532, time: 19:49:16, step: 407\nloss: 2.09015154838562, time: 19:49:21, step: 408\nloss: 2.0695526599884033, time: 19:49:27, step: 409\nloss: 1.9631963968276978, time: 19:49:32, step: 410\nloss: 2.119502067565918, time: 19:49:37, step: 411\nloss: 1.8974782228469849, time: 19:49:43, step: 412\nloss: 1.8803194761276245, time: 19:49:48, step: 413\nloss: 1.8820388317108154, time: 19:49:54, step: 414\nloss: 2.11312198638916, time: 19:49:59, step: 415\nloss: 2.1035611629486084, time: 19:50:05, step: 416\nloss: 1.9613769054412842, time: 19:50:10, step: 417\nloss: 1.9722222089767456, time: 19:50:16, step: 418\nloss: 1.8866589069366455, time: 19:50:21, step: 419\nloss: 1.9846292734146118, time: 19:50:27, step: 420\n----- Time -> 19:50:31 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1583\n----- Time -> 19:50:32 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0198\n----- Time -> 19:50:32 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6878\n----- Time -> 19:50:33 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8488\n----- Time -> 19:50:33 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0190\n----- Time -> 19:50:33 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9467489\nloss: 2.19887113571167, time: 19:50:35, step: 421\nloss: 1.9167826175689697, time: 19:50:40, step: 422\nloss: 1.9483551979064941, time: 19:50:46, step: 423\nloss: 1.9363336563110352, time: 19:50:51, step: 424\nloss: 2.0244836807250977, time: 19:50:57, step: 425\nloss: 1.977108120918274, time: 19:51:02, step: 426\nloss: 1.9418240785598755, time: 19:51:07, step: 427\nloss: 1.8690754175186157, time: 19:51:13, step: 428\nloss: 1.4143505096435547, time: 19:51:18, step: 429\nloss: 1.9154163599014282, time: 19:51:24, step: 430\nloss: 2.008474826812744, time: 19:51:29, step: 431\nloss: 1.9778975248336792, time: 19:51:35, step: 432\nloss: 1.9143565893173218, time: 19:51:40, step: 433\nloss: 2.110541343688965, time: 19:51:46, step: 434\nloss: 1.9544718265533447, time: 19:51:51, step: 435\nloss: 1.967311143875122, time: 19:51:57, step: 436\nloss: 1.8157715797424316, time: 19:52:02, step: 437\nloss: 2.1296756267547607, time: 19:52:08, step: 438\nloss: 1.9460508823394775, time: 19:52:13, step: 439\nloss: 2.071704149246216, time: 19:52:18, step: 440\n----- Time -> 19:52:23 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1549\n----- Time -> 19:52:23 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0175\n----- Time -> 19:52:24 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6867\n----- Time -> 19:52:24 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8460\n----- Time -> 19:52:25 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0137\n----- Time -> 19:52:25 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9437719\nloss: 1.9806008338928223, time: 19:52:27, step: 441\nloss: 1.960292935371399, time: 19:52:32, step: 442\nloss: 2.340841054916382, time: 19:52:37, step: 443\nloss: 2.1017260551452637, time: 19:52:43, step: 444\nloss: 2.1277174949645996, time: 19:52:48, step: 445\nloss: 1.910573959350586, time: 19:52:54, step: 446\nloss: 2.032442331314087, time: 19:52:59, step: 447\nloss: 1.9202853441238403, time: 19:53:05, step: 448\nloss: 2.076669216156006, time: 19:53:10, step: 449\nloss: 1.9136722087860107, time: 19:53:16, step: 450\nloss: 1.7678961753845215, time: 19:53:21, step: 451\nloss: 1.8480552434921265, time: 19:53:27, step: 452\nloss: 1.8500736951828003, time: 19:53:32, step: 453\nloss: 1.8653135299682617, time: 19:53:38, step: 454\nloss: 2.121199131011963, time: 19:53:43, step: 455\nloss: 1.7508946657180786, time: 19:53:48, step: 456\nloss: 1.8148335218429565, time: 19:53:54, step: 457\nloss: 1.899544358253479, time: 19:53:59, step: 458\nloss: 1.9712374210357666, time: 19:54:05, step: 459\nloss: 1.9952772855758667, time: 19:54:10, step: 460\n----- Time -> 19:54:15 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1520\n----- Time -> 19:54:15 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0143\n----- Time -> 19:54:16 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6860\n----- Time -> 19:54:16 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8428\n----- Time -> 19:54:17 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0106\n----- Time -> 19:54:17 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9411299\nloss: 1.9494930505752563, time: 19:54:19, step: 461\nloss: 1.7875590324401855, time: 19:54:24, step: 462\nloss: 1.996765375137329, time: 19:54:29, step: 463\nloss: 2.006657123565674, time: 19:54:35, step: 464\nloss: 2.1771366596221924, time: 19:54:40, step: 465\nloss: 1.8853893280029297, time: 19:54:46, step: 466\nloss: 1.876455545425415, time: 19:54:51, step: 467\nloss: 1.977973461151123, time: 19:54:57, step: 468\nloss: 1.8434313535690308, time: 19:55:02, step: 469\nloss: 2.1328353881835938, time: 19:55:08, step: 470\nloss: 1.9006327390670776, time: 19:55:13, step: 471\nloss: 1.9464038610458374, time: 19:55:18, step: 472\nloss: 1.9237380027770996, time: 19:55:24, step: 473\nloss: 2.003965377807617, time: 19:55:29, step: 474\nloss: 2.065908432006836, time: 19:55:35, step: 475\nloss: 1.7443420886993408, time: 19:55:40, step: 476\nloss: 2.1512069702148438, time: 19:55:46, step: 477\nloss: 1.95646071434021, time: 19:55:51, step: 478\nloss: 1.6842344999313354, time: 19:55:57, step: 479\nloss: 2.017829656600952, time: 19:56:02, step: 480\n----- Time -> 19:56:06 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1492\n----- Time -> 19:56:07 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0118\n----- Time -> 19:56:07 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6816\n----- Time -> 19:56:08 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8373\n----- Time -> 19:56:08 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0082\n----- Time -> 19:56:08 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9376270\nloss: 1.8510971069335938, time: 19:56:10, step: 481\nloss: 1.8289604187011719, time: 19:56:15, step: 482\nloss: 1.828790545463562, time: 19:56:21, step: 483\nloss: 1.7786803245544434, time: 19:56:26, step: 484\nloss: 2.0781140327453613, time: 19:56:32, step: 485\nloss: 2.2055137157440186, time: 19:56:37, step: 486\nloss: 1.8049615621566772, time: 19:56:43, step: 487\nloss: 2.1470727920532227, time: 19:56:48, step: 488\nloss: 1.560005784034729, time: 19:56:54, step: 489\nloss: 2.0368218421936035, time: 19:56:59, step: 490\nloss: 1.7713301181793213, time: 19:57:04, step: 491\nloss: 1.9532438516616821, time: 19:57:10, step: 492\nloss: 1.928673505783081, time: 19:57:15, step: 493\nloss: 1.8387442827224731, time: 19:57:21, step: 494\nloss: 1.9243452548980713, time: 19:57:26, step: 495\nloss: 2.0597641468048096, time: 19:57:32, step: 496\nloss: 2.0838165283203125, time: 19:57:37, step: 497\nloss: 2.0431976318359375, time: 19:57:43, step: 498\nloss: 1.9376627206802368, time: 19:57:48, step: 499\nloss: 2.1164917945861816, time: 19:57:54, step: 500\n----- Time -> 19:57:58 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1451\n----- Time -> 19:57:59 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0113\n----- Time -> 19:57:59 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6819\n----- Time -> 19:58:00 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8352\n----- Time -> 19:58:00 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0072\n----- Time -> 19:58:00 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9361426\nloss: 1.982277750968933, time: 19:58:02, step: 501\nloss: 2.002786636352539, time: 19:58:07, step: 502\nloss: 1.9862077236175537, time: 19:58:13, step: 503\nloss: 2.1372218132019043, time: 19:58:18, step: 504\nloss: 1.923275351524353, time: 19:58:23, step: 505\nloss: 1.644440770149231, time: 19:58:29, step: 506\nloss: 2.076727867126465, time: 19:58:34, step: 507\nloss: 2.2413928508758545, time: 19:58:40, step: 508\nloss: 1.8324936628341675, time: 19:58:45, step: 509\nloss: 1.8523789644241333, time: 19:58:51, step: 510\nloss: 2.0638201236724854, time: 19:58:56, step: 511\nloss: 1.8533194065093994, time: 19:59:02, step: 512\nloss: 1.8352563381195068, time: 19:59:07, step: 513\nloss: 1.7947546243667603, time: 19:59:12, step: 514\nloss: 1.822877049446106, time: 19:59:18, step: 515\nloss: 1.787575364112854, time: 19:59:23, step: 516\nloss: 2.028338670730591, time: 19:59:29, step: 517\nloss: 1.7219160795211792, time: 19:59:34, step: 518\nloss: 2.155977249145508, time: 19:59:40, step: 519\nloss: 1.786429762840271, time: 19:59:45, step: 520\n----- Time -> 19:59:50 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1433\n----- Time -> 19:59:50 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0061\n----- Time -> 19:59:51 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6785\n----- Time -> 19:59:51 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8354\n----- Time -> 19:59:52 ----- Validation Batch -> 5 ----  Validation Loss -> 2.0046\n----- Time -> 19:59:52 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9335923\nloss: 2.086771011352539, time: 19:59:53, step: 521\nloss: 1.8111021518707275, time: 19:59:59, step: 522\nloss: 1.6463572978973389, time: 20:00:04, step: 523\nloss: 1.9023346900939941, time: 20:00:10, step: 524\nloss: 1.7921236753463745, time: 20:00:15, step: 525\nloss: 1.9657148122787476, time: 20:00:20, step: 526\nloss: 1.8785364627838135, time: 20:00:26, step: 527\nloss: 1.8865193128585815, time: 20:00:31, step: 528\nloss: 1.8943897485733032, time: 20:00:37, step: 529\nloss: 1.8720840215682983, time: 20:00:42, step: 530\nloss: 1.8859490156173706, time: 20:00:48, step: 531\nloss: 1.8769766092300415, time: 20:00:53, step: 532\nloss: 1.9091980457305908, time: 20:00:59, step: 533\nloss: 1.9305386543273926, time: 20:01:04, step: 534\nloss: 1.8076072931289673, time: 20:01:10, step: 535\nloss: 1.8713854551315308, time: 20:01:15, step: 536\nloss: 1.9078155755996704, time: 20:01:21, step: 537\nloss: 2.0470497608184814, time: 20:01:26, step: 538\nloss: 2.0255401134490967, time: 20:01:32, step: 539\nloss: 1.8959673643112183, time: 20:01:37, step: 540\n----- Time -> 20:01:41 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1389\n----- Time -> 20:01:42 ----- Validation Batch -> 2 ----  Validation Loss -> 2.0010\n----- Time -> 20:01:43 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6754\n----- Time -> 20:01:43 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8301\n----- Time -> 20:01:44 ----- Validation Batch -> 5 ----  Validation Loss -> 1.9977\n----- Time -> 20:01:44 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9286215\nloss: 1.881593108177185, time: 20:01:45, step: 541\nloss: 1.977278470993042, time: 20:01:51, step: 542\nloss: 1.9319353103637695, time: 20:01:56, step: 543\nloss: 1.6546165943145752, time: 20:02:02, step: 544\nloss: 1.9269559383392334, time: 20:02:07, step: 545\nloss: 1.815502643585205, time: 20:02:13, step: 546\nloss: 2.087630271911621, time: 20:02:18, step: 547\nloss: 2.0494160652160645, time: 20:02:23, step: 548\nloss: 1.9411559104919434, time: 20:02:29, step: 549\nloss: 1.8431802988052368, time: 20:02:34, step: 550\nloss: 2.1403934955596924, time: 20:02:40, step: 551\nloss: 1.9991025924682617, time: 20:02:45, step: 552\nloss: 1.8995288610458374, time: 20:02:51, step: 553\nloss: 1.8398469686508179, time: 20:02:56, step: 554\nloss: 2.134371280670166, time: 20:03:02, step: 555\nloss: 2.01521372795105, time: 20:03:07, step: 556\nloss: 1.5051474571228027, time: 20:03:13, step: 557\nloss: 2.1220250129699707, time: 20:03:18, step: 558\nloss: 1.9438962936401367, time: 20:03:24, step: 559\nloss: 1.8024952411651611, time: 20:03:29, step: 560\n----- Time -> 20:03:33 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1325\n----- Time -> 20:03:34 ----- Validation Batch -> 2 ----  Validation Loss -> 1.9962\n----- Time -> 20:03:34 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6727\n----- Time -> 20:03:35 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8259\n----- Time -> 20:03:36 ----- Validation Batch -> 5 ----  Validation Loss -> 1.9953\n----- Time -> 20:03:36 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9245024\nloss: 1.936787486076355, time: 20:03:37, step: 561\nloss: 2.058298110961914, time: 20:03:43, step: 562\nloss: 1.7323403358459473, time: 20:03:48, step: 563\nloss: 2.140157461166382, time: 20:03:53, step: 564\nloss: 2.0206308364868164, time: 20:03:59, step: 565\nloss: 1.8153687715530396, time: 20:04:04, step: 566\nloss: 1.9691487550735474, time: 20:04:10, step: 567\nloss: 2.1292145252227783, time: 20:04:15, step: 568\nloss: 1.9857791662216187, time: 20:04:21, step: 569\nloss: 1.8586325645446777, time: 20:04:26, step: 570\nloss: 1.9806509017944336, time: 20:04:32, step: 571\nloss: 1.9424726963043213, time: 20:04:37, step: 572\nloss: 1.969051480293274, time: 20:04:42, step: 573\nloss: 2.1345784664154053, time: 20:04:48, step: 574\nloss: 1.9258793592453003, time: 20:04:53, step: 575\nloss: 1.9867024421691895, time: 20:04:59, step: 576\nloss: 1.9085835218429565, time: 20:05:04, step: 577\nloss: 1.803309679031372, time: 20:05:10, step: 578\nloss: 2.0285394191741943, time: 20:05:15, step: 579\nloss: 1.7515227794647217, time: 20:05:21, step: 580\n----- Time -> 20:05:25 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1283\n----- Time -> 20:05:26 ----- Validation Batch -> 2 ----  Validation Loss -> 1.9888\n----- Time -> 20:05:26 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6677\n----- Time -> 20:05:27 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8194\n----- Time -> 20:05:27 ----- Validation Batch -> 5 ----  Validation Loss -> 1.9880\n----- Time -> 20:05:27 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9184366\nloss: 1.9967515468597412, time: 20:05:29, step: 581\nloss: 1.8897095918655396, time: 20:05:34, step: 582\nloss: 1.6574475765228271, time: 20:05:40, step: 583\nloss: 1.9697803258895874, time: 20:05:45, step: 584\nloss: 2.133180856704712, time: 20:05:51, step: 585\nloss: 1.9934238195419312, time: 20:05:56, step: 586\nloss: 2.0278170108795166, time: 20:06:01, step: 587\nloss: 1.99027681350708, time: 20:06:07, step: 588\nloss: 1.9882797002792358, time: 20:06:12, step: 589\nloss: 1.8470573425292969, time: 20:06:18, step: 590\nloss: 1.9368202686309814, time: 20:06:23, step: 591\nloss: 2.0606565475463867, time: 20:06:29, step: 592\nloss: 1.9558091163635254, time: 20:06:34, step: 593\nloss: 2.1035571098327637, time: 20:06:40, step: 594\nloss: 1.9546335935592651, time: 20:06:45, step: 595\nloss: 1.9081785678863525, time: 20:06:51, step: 596\nloss: 2.0395045280456543, time: 20:06:56, step: 597\nloss: 1.835493564605713, time: 20:07:01, step: 598\nloss: 1.8753527402877808, time: 20:07:07, step: 599\nloss: 2.0294787883758545, time: 20:07:12, step: 600\n----- Time -> 20:07:17 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1211\n----- Time -> 20:07:17 ----- Validation Batch -> 2 ----  Validation Loss -> 1.9849\n----- Time -> 20:07:18 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6625\n----- Time -> 20:07:18 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8137\n----- Time -> 20:07:19 ----- Validation Batch -> 5 ----  Validation Loss -> 1.9826\n----- Time -> 20:07:19 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9129685\nloss: 1.9349743127822876, time: 20:07:21, step: 601\nloss: 1.9913488626480103, time: 20:07:26, step: 602\nloss: 2.006466865539551, time: 20:07:31, step: 603\nloss: 2.016087532043457, time: 20:07:37, step: 604\nloss: 2.0476126670837402, time: 20:07:42, step: 605\nloss: 1.9440195560455322, time: 20:07:48, step: 606\nloss: 2.085439443588257, time: 20:07:53, step: 607\nloss: 1.9150490760803223, time: 20:07:59, step: 608\nloss: 1.8658933639526367, time: 20:08:04, step: 609\nloss: 2.198190450668335, time: 20:08:10, step: 610\nloss: 2.050684690475464, time: 20:08:15, step: 611\nloss: 1.813657283782959, time: 20:08:21, step: 612\nloss: 1.8776689767837524, time: 20:08:26, step: 613\nloss: 1.9352121353149414, time: 20:08:32, step: 614\nloss: 1.9059845209121704, time: 20:08:37, step: 615\nloss: 1.8601808547973633, time: 20:08:43, step: 616\nloss: 1.8607125282287598, time: 20:08:48, step: 617\nloss: 1.900142788887024, time: 20:08:54, step: 618\nloss: 1.882836103439331, time: 20:08:59, step: 619\nloss: 2.067216634750366, time: 20:09:05, step: 620\n----- Time -> 20:09:09 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1136\n----- Time -> 20:09:10 ----- Validation Batch -> 2 ----  Validation Loss -> 1.9776\n----- Time -> 20:09:10 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6602\n----- Time -> 20:09:11 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8091\n----- Time -> 20:09:11 ----- Validation Batch -> 5 ----  Validation Loss -> 1.9731\n----- Time -> 20:09:11 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.9067066\nloss: 1.8821907043457031, time: 20:09:13, step: 621\nloss: 1.8582079410552979, time: 20:09:18, step: 622\nloss: 1.9940844774246216, time: 20:09:24, step: 623\nloss: 1.8781249523162842, time: 20:09:29, step: 624\nloss: 2.0352377891540527, time: 20:09:35, step: 625\nloss: 2.0235581398010254, time: 20:09:40, step: 626\nloss: 1.6216145753860474, time: 20:09:46, step: 627\nloss: 1.8175393342971802, time: 20:09:51, step: 628\nloss: 1.966212272644043, time: 20:09:56, step: 629\nloss: 1.9190683364868164, time: 20:10:02, step: 630\nloss: 1.913084626197815, time: 20:10:07, step: 631\nloss: 2.024658441543579, time: 20:10:13, step: 632\nloss: 2.0310840606689453, time: 20:10:18, step: 633\nloss: 1.815490961074829, time: 20:10:24, step: 634\nloss: 2.0487375259399414, time: 20:10:29, step: 635\nloss: 1.9668787717819214, time: 20:10:35, step: 636\nloss: 1.8893043994903564, time: 20:10:40, step: 637\nloss: 2.008134126663208, time: 20:10:46, step: 638\nloss: 1.7650871276855469, time: 20:10:51, step: 639\nloss: 2.0333173274993896, time: 20:10:57, step: 640\n----- Time -> 20:11:01 ----- Validation Batch -> 1 ----  Validation Loss -> 2.1069\n----- Time -> 20:11:02 ----- Validation Batch -> 2 ----  Validation Loss -> 1.9697\n----- Time -> 20:11:02 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6544\n----- Time -> 20:11:03 ----- Validation Batch -> 4 ----  Validation Loss -> 1.8015\n----- Time -> 20:11:03 ----- Validation Batch -> 5 ----  Validation Loss -> 1.9665\n----- Time -> 20:11:03 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.8998020\nloss: 2.137352228164673, time: 20:11:05, step: 641\nloss: 1.9668617248535156, time: 20:11:10, step: 642\nloss: 1.5402963161468506, time: 20:11:16, step: 643\nloss: 1.989637017250061, time: 20:11:21, step: 644\nloss: 1.941728949546814, time: 20:11:27, step: 645\nloss: 2.0645525455474854, time: 20:11:32, step: 646\nloss: 1.9508124589920044, time: 20:11:38, step: 647\nloss: 2.135822296142578, time: 20:11:43, step: 648\nloss: 1.8384315967559814, time: 20:11:48, step: 649\nloss: 1.911439061164856, time: 20:11:54, step: 650\nloss: 1.8848094940185547, time: 20:11:59, step: 651\nloss: 1.9024320840835571, time: 20:12:05, step: 652\nloss: 1.7338515520095825, time: 20:12:10, step: 653\nloss: 1.8825448751449585, time: 20:12:16, step: 654\nloss: 1.9530152082443237, time: 20:12:21, step: 655\nloss: 1.9822157621383667, time: 20:12:27, step: 656\nloss: 1.988777756690979, time: 20:12:32, step: 657\nloss: 1.9275285005569458, time: 20:12:38, step: 658\nloss: 1.8914074897766113, time: 20:12:43, step: 659\nloss: 1.7602097988128662, time: 20:12:49, step: 660\n----- Time -> 20:12:53 ----- Validation Batch -> 1 ----  Validation Loss -> 2.0970\n----- Time -> 20:12:54 ----- Validation Batch -> 2 ----  Validation Loss -> 1.9654\n----- Time -> 20:12:54 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6471\n----- Time -> 20:12:55 ----- Validation Batch -> 4 ----  Validation Loss -> 1.7965\n----- Time -> 20:12:55 ----- Validation Batch -> 5 ----  Validation Loss -> 1.9595\n----- Time -> 20:12:55 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.8930946\nloss: 1.8278535604476929, time: 20:12:57, step: 661\nloss: 2.123042583465576, time: 20:13:02, step: 662\nloss: 1.7982813119888306, time: 20:13:08, step: 663\nloss: 1.8632601499557495, time: 20:13:13, step: 664\nloss: 1.979128360748291, time: 20:13:19, step: 665\nloss: 2.0174784660339355, time: 20:13:24, step: 666\nloss: 1.9198544025421143, time: 20:13:30, step: 667\nloss: 1.6247328519821167, time: 20:13:35, step: 668\nloss: 2.041931629180908, time: 20:13:41, step: 669\nloss: 1.9708553552627563, time: 20:13:46, step: 670\nloss: 1.9168550968170166, time: 20:13:51, step: 671\nloss: 1.8087960481643677, time: 20:13:57, step: 672\nloss: 2.004940986633301, time: 20:14:02, step: 673\nloss: 2.004689931869507, time: 20:14:08, step: 674\nloss: 1.9282160997390747, time: 20:14:13, step: 675\nloss: 2.101576805114746, time: 20:14:19, step: 676\nloss: 1.8120568990707397, time: 20:14:24, step: 677\nloss: 1.9717538356781006, time: 20:14:30, step: 678\nloss: 1.9618809223175049, time: 20:14:35, step: 679\nloss: 1.7771328687667847, time: 20:14:41, step: 680\n----- Time -> 20:14:45 ----- Validation Batch -> 1 ----  Validation Loss -> 2.0909\n----- Time -> 20:14:46 ----- Validation Batch -> 2 ----  Validation Loss -> 1.9561\n----- Time -> 20:14:46 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6420\n----- Time -> 20:14:47 ----- Validation Batch -> 4 ----  Validation Loss -> 1.7899\n----- Time -> 20:14:47 ----- Validation Batch -> 5 ----  Validation Loss -> 1.9501\n----- Time -> 20:14:47 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.8857902\nloss: 2.081728458404541, time: 20:14:49, step: 681\nloss: 1.861548662185669, time: 20:14:54, step: 682\nloss: 1.8320600986480713, time: 20:15:00, step: 683\nloss: 1.866836428642273, time: 20:15:05, step: 684\nloss: 2.0859084129333496, time: 20:15:11, step: 685\nloss: 1.7782820463180542, time: 20:15:16, step: 686\nloss: 1.924906849861145, time: 20:15:22, step: 687\nloss: 1.8956729173660278, time: 20:15:27, step: 688\nloss: 1.983066439628601, time: 20:15:33, step: 689\nloss: 1.9236974716186523, time: 20:15:38, step: 690\nloss: 1.9042725563049316, time: 20:15:43, step: 691\nloss: 2.100759983062744, time: 20:15:49, step: 692\nloss: 1.9085681438446045, time: 20:15:54, step: 693\nloss: 1.9324685335159302, time: 20:16:00, step: 694\nloss: 1.9385722875595093, time: 20:16:05, step: 695\nloss: 1.8550801277160645, time: 20:16:11, step: 696\nloss: 1.7968982458114624, time: 20:16:16, step: 697\nloss: 1.9612367153167725, time: 20:16:22, step: 698\nloss: 1.8501019477844238, time: 20:16:27, step: 699\nloss: 1.7260326147079468, time: 20:16:33, step: 700\n----- Time -> 20:16:37 ----- Validation Batch -> 1 ----  Validation Loss -> 2.0808\n----- Time -> 20:16:38 ----- Validation Batch -> 2 ----  Validation Loss -> 1.9462\n----- Time -> 20:16:38 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6357\n----- Time -> 20:16:39 ----- Validation Batch -> 4 ----  Validation Loss -> 1.7814\n----- Time -> 20:16:39 ----- Validation Batch -> 5 ----  Validation Loss -> 1.9431\n----- Time -> 20:16:39 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.8774261\nloss: 1.8012311458587646, time: 20:16:41, step: 701\nloss: 1.7680656909942627, time: 20:16:46, step: 702\nloss: 1.9636975526809692, time: 20:16:52, step: 703\nloss: 1.801601529121399, time: 20:16:57, step: 704\nloss: 2.0213096141815186, time: 20:17:03, step: 705\nloss: 1.8341208696365356, time: 20:17:08, step: 706\nloss: 1.77059006690979, time: 20:17:14, step: 707\nloss: 1.837494969367981, time: 20:17:19, step: 708\nloss: 1.8461315631866455, time: 20:17:25, step: 709\nloss: 1.8546574115753174, time: 20:17:30, step: 710\nloss: 1.8816263675689697, time: 20:17:36, step: 711\nloss: 1.7004427909851074, time: 20:17:41, step: 712\nloss: 1.9090427160263062, time: 20:17:47, step: 713\nloss: 1.8580316305160522, time: 20:17:52, step: 714\nloss: 1.8706047534942627, time: 20:17:58, step: 715\nloss: 1.7696232795715332, time: 20:18:03, step: 716\nloss: 1.6800577640533447, time: 20:18:08, step: 717\nloss: 2.037580728530884, time: 20:18:14, step: 718\nloss: 1.8293981552124023, time: 20:18:19, step: 719\nloss: 1.8709086179733276, time: 20:18:25, step: 720\n----- Time -> 20:18:29 ----- Validation Batch -> 1 ----  Validation Loss -> 2.0694\n----- Time -> 20:18:30 ----- Validation Batch -> 2 ----  Validation Loss -> 1.9352\n----- Time -> 20:18:30 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6314\n----- Time -> 20:18:31 ----- Validation Batch -> 4 ----  Validation Loss -> 1.7753\n----- Time -> 20:18:31 ----- Validation Batch -> 5 ----  Validation Loss -> 1.9310\n----- Time -> 20:18:31 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.8684798\nloss: 1.829626202583313, time: 20:18:33, step: 721\nloss: 1.8923847675323486, time: 20:18:38, step: 722\nloss: 2.1360559463500977, time: 20:18:44, step: 723\nloss: 1.9872981309890747, time: 20:18:49, step: 724\nloss: 1.7695146799087524, time: 20:18:55, step: 725\nloss: 1.765014886856079, time: 20:19:00, step: 726\nloss: 1.9198377132415771, time: 20:19:06, step: 727\nloss: 1.861449956893921, time: 20:19:11, step: 728\nloss: 1.7657253742218018, time: 20:19:17, step: 729\nloss: 1.6174734830856323, time: 20:19:22, step: 730\nloss: 1.8230962753295898, time: 20:19:28, step: 731\nloss: 1.9776463508605957, time: 20:19:33, step: 732\nloss: 1.9104762077331543, time: 20:19:39, step: 733\nloss: 1.9013781547546387, time: 20:19:44, step: 734\nloss: 1.9773703813552856, time: 20:19:50, step: 735\nloss: 1.9810864925384521, time: 20:19:55, step: 736\nloss: 1.8321449756622314, time: 20:20:01, step: 737\nloss: 1.9911478757858276, time: 20:20:06, step: 738\nloss: 1.8161364793777466, time: 20:20:12, step: 739\nloss: 1.8475403785705566, time: 20:20:17, step: 740\n----- Time -> 20:20:21 ----- Validation Batch -> 1 ----  Validation Loss -> 2.0634\n----- Time -> 20:20:22 ----- Validation Batch -> 2 ----  Validation Loss -> 1.9292\n----- Time -> 20:20:22 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6242\n----- Time -> 20:20:23 ----- Validation Batch -> 4 ----  Validation Loss -> 1.7677\n----- Time -> 20:20:24 ----- Validation Batch -> 5 ----  Validation Loss -> 1.9241\n----- Time -> 20:20:24 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.8617451\nloss: 1.9606776237487793, time: 20:20:25, step: 741\nloss: 1.910841464996338, time: 20:20:31, step: 742\nloss: 1.939728021621704, time: 20:20:36, step: 743\nloss: 1.984718918800354, time: 20:20:42, step: 744\nloss: 1.5854508876800537, time: 20:20:47, step: 745\nloss: 1.7627006769180298, time: 20:20:53, step: 746\nloss: 1.7486774921417236, time: 20:20:58, step: 747\nloss: 1.7934744358062744, time: 20:21:03, step: 748\nloss: 1.920462727546692, time: 20:21:09, step: 749\nloss: 1.7973967790603638, time: 20:21:14, step: 750\nloss: 1.755996823310852, time: 20:21:20, step: 751\nloss: 1.899497628211975, time: 20:21:25, step: 752\nloss: 1.6340559720993042, time: 20:21:31, step: 753\nloss: 1.7589335441589355, time: 20:21:36, step: 754\nloss: 2.1783392429351807, time: 20:21:42, step: 755\nloss: 1.8721818923950195, time: 20:21:47, step: 756\nloss: 1.6279549598693848, time: 20:21:53, step: 757\nloss: 1.7974512577056885, time: 20:21:58, step: 758\nloss: 2.0861449241638184, time: 20:22:04, step: 759\nloss: 2.1491148471832275, time: 20:22:09, step: 760\n----- Time -> 20:22:13 ----- Validation Batch -> 1 ----  Validation Loss -> 2.0484\n----- Time -> 20:22:14 ----- Validation Batch -> 2 ----  Validation Loss -> 1.9173\n----- Time -> 20:22:15 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6179\n----- Time -> 20:22:15 ----- Validation Batch -> 4 ----  Validation Loss -> 1.7611\n----- Time -> 20:22:16 ----- Validation Batch -> 5 ----  Validation Loss -> 1.9124\n----- Time -> 20:22:16 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.8514002\nloss: 1.971770167350769, time: 20:22:17, step: 761\nloss: 1.7994537353515625, time: 20:22:23, step: 762\nloss: 1.8381396532058716, time: 20:22:28, step: 763\nloss: 1.883996605873108, time: 20:22:34, step: 764\nloss: 1.7359150648117065, time: 20:22:39, step: 765\nloss: 1.8085436820983887, time: 20:22:45, step: 766\nloss: 1.7596468925476074, time: 20:22:50, step: 767\nloss: 1.9236493110656738, time: 20:22:56, step: 768\nloss: 1.7963660955429077, time: 20:23:01, step: 769\nloss: 1.981408715248108, time: 20:23:06, step: 770\nloss: 1.858864665031433, time: 20:23:12, step: 771\nloss: 1.8719520568847656, time: 20:23:17, step: 772\nloss: 1.8480167388916016, time: 20:23:23, step: 773\nloss: 1.805443525314331, time: 20:23:28, step: 774\nloss: 1.8488250970840454, time: 20:23:34, step: 775\nloss: 2.1243460178375244, time: 20:23:39, step: 776\nloss: 1.795568823814392, time: 20:23:45, step: 777\nloss: 1.9613109827041626, time: 20:23:50, step: 778\nloss: 2.0321128368377686, time: 20:23:56, step: 779\nloss: 1.9594476222991943, time: 20:24:01, step: 780\n----- Time -> 20:24:06 ----- Validation Batch -> 1 ----  Validation Loss -> 2.0399\n----- Time -> 20:24:06 ----- Validation Batch -> 2 ----  Validation Loss -> 1.9103\n----- Time -> 20:24:07 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6109\n----- Time -> 20:24:07 ----- Validation Batch -> 4 ----  Validation Loss -> 1.7512\n----- Time -> 20:24:08 ----- Validation Batch -> 5 ----  Validation Loss -> 1.9030\n----- Time -> 20:24:08 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.8430833\nloss: 1.8144112825393677, time: 20:24:09, step: 781\nloss: 1.7888505458831787, time: 20:24:15, step: 782\nloss: 1.8056811094284058, time: 20:24:20, step: 783\nloss: 1.822164535522461, time: 20:24:26, step: 784\nloss: 2.0159912109375, time: 20:24:31, step: 785\nloss: 1.8733787536621094, time: 20:24:37, step: 786\nloss: 1.859449863433838, time: 20:24:42, step: 787\nloss: 1.801393985748291, time: 20:24:48, step: 788\nloss: 2.0693256855010986, time: 20:24:53, step: 789\nloss: 1.7730706930160522, time: 20:24:58, step: 790\nloss: 1.8324393033981323, time: 20:25:04, step: 791\nloss: 1.714787483215332, time: 20:25:09, step: 792\nloss: 1.9417457580566406, time: 20:25:15, step: 793\nloss: 1.8713945150375366, time: 20:25:20, step: 794\nloss: 1.9149589538574219, time: 20:25:26, step: 795\nloss: 1.7241761684417725, time: 20:25:31, step: 796\nloss: 2.085289716720581, time: 20:25:37, step: 797\nloss: 1.8038572072982788, time: 20:25:42, step: 798\nloss: 1.7965621948242188, time: 20:25:47, step: 799\nloss: 1.785935401916504, time: 20:25:53, step: 800\n----- Time -> 20:25:57 ----- Validation Batch -> 1 ----  Validation Loss -> 2.0280\n----- Time -> 20:25:58 ----- Validation Batch -> 2 ----  Validation Loss -> 1.9000\n----- Time -> 20:25:58 ----- Validation Batch -> 3 ----  Validation Loss -> 1.6042\n----- Time -> 20:25:59 ----- Validation Batch -> 4 ----  Validation Loss -> 1.7461\n----- Time -> 20:25:59 ----- Validation Batch -> 5 ----  Validation Loss -> 1.8944\n----- Time -> 20:26:00 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.8345477\nloss: 1.8876876831054688, time: 20:26:01, step: 801\nloss: 1.8142977952957153, time: 20:26:07, step: 802\nloss: 1.8632558584213257, time: 20:26:12, step: 803\nloss: 1.9122892618179321, time: 20:26:18, step: 804\nloss: 1.8672882318496704, time: 20:26:23, step: 805\nloss: 1.8484309911727905, time: 20:26:28, step: 806\nloss: 1.7714664936065674, time: 20:26:34, step: 807\nloss: 1.752076506614685, time: 20:26:39, step: 808\nloss: 1.941900372505188, time: 20:26:45, step: 809\nloss: 1.8845499753952026, time: 20:26:50, step: 810\nloss: 1.8425986766815186, time: 20:26:56, step: 811\nloss: 1.9079627990722656, time: 20:27:01, step: 812\nloss: 1.9310823678970337, time: 20:27:07, step: 813\nloss: 1.7583953142166138, time: 20:27:12, step: 814\nloss: 1.962785243988037, time: 20:27:17, step: 815\nloss: 1.7422375679016113, time: 20:27:23, step: 816\nloss: 1.781816840171814, time: 20:27:28, step: 817\nloss: 1.9490492343902588, time: 20:27:34, step: 818\nloss: 1.8495452404022217, time: 20:27:39, step: 819\nloss: 1.9105150699615479, time: 20:27:45, step: 820\n----- Time -> 20:27:49 ----- Validation Batch -> 1 ----  Validation Loss -> 2.0209\n----- Time -> 20:27:50 ----- Validation Batch -> 2 ----  Validation Loss -> 1.8877\n----- Time -> 20:27:50 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5976\n----- Time -> 20:27:51 ----- Validation Batch -> 4 ----  Validation Loss -> 1.7349\n----- Time -> 20:27:51 ----- Validation Batch -> 5 ----  Validation Loss -> 1.8825\n----- Time -> 20:27:51 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.8246976\nloss: 1.8246887922286987, time: 20:27:53, step: 821\nloss: 1.8755015134811401, time: 20:27:58, step: 822\nloss: 1.8741294145584106, time: 20:28:04, step: 823\nloss: 1.8375390768051147, time: 20:28:09, step: 824\nloss: 1.8576611280441284, time: 20:28:15, step: 825\nloss: 1.8570636510849, time: 20:28:20, step: 826\nloss: 1.7835265398025513, time: 20:28:26, step: 827\nloss: 2.010333776473999, time: 20:28:31, step: 828\nloss: 1.6329076290130615, time: 20:28:36, step: 829\nloss: 1.7214441299438477, time: 20:28:42, step: 830\nloss: 1.6905725002288818, time: 20:28:47, step: 831\nloss: 1.7849798202514648, time: 20:28:53, step: 832\nloss: 1.856756329536438, time: 20:28:58, step: 833\nloss: 1.937564730644226, time: 20:29:04, step: 834\nloss: 1.8740202188491821, time: 20:29:10, step: 835\nloss: 1.8493825197219849, time: 20:29:15, step: 836\nloss: 1.9795712232589722, time: 20:29:21, step: 837\nloss: 1.7574594020843506, time: 20:29:26, step: 838\nloss: 1.7453747987747192, time: 20:29:32, step: 839\nloss: 1.7288784980773926, time: 20:29:37, step: 840\n----- Time -> 20:29:41 ----- Validation Batch -> 1 ----  Validation Loss -> 2.0067\n----- Time -> 20:29:42 ----- Validation Batch -> 2 ----  Validation Loss -> 1.8791\n----- Time -> 20:29:42 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5893\n----- Time -> 20:29:43 ----- Validation Batch -> 4 ----  Validation Loss -> 1.7268\n----- Time -> 20:29:44 ----- Validation Batch -> 5 ----  Validation Loss -> 1.8686\n----- Time -> 20:29:44 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.8141122\nloss: 1.757877230644226, time: 20:29:45, step: 841\nloss: 1.9093371629714966, time: 20:29:51, step: 842\nloss: 1.8367902040481567, time: 20:29:56, step: 843\nloss: 2.0488710403442383, time: 20:30:02, step: 844\nloss: 1.8949570655822754, time: 20:30:07, step: 845\nloss: 1.7765535116195679, time: 20:30:12, step: 846\nloss: 1.8128688335418701, time: 20:30:18, step: 847\nloss: 1.9307266473770142, time: 20:30:23, step: 848\nloss: 1.837600588798523, time: 20:30:29, step: 849\nloss: 1.7039090394973755, time: 20:30:34, step: 850\nloss: 1.6730645895004272, time: 20:30:40, step: 851\nloss: 2.1950109004974365, time: 20:30:45, step: 852\nloss: 1.818692684173584, time: 20:30:50, step: 853\nloss: 1.8152236938476562, time: 20:30:56, step: 854\nloss: 1.9718878269195557, time: 20:31:01, step: 855\nloss: 1.8414596319198608, time: 20:31:07, step: 856\nloss: 1.7608402967453003, time: 20:31:12, step: 857\nloss: 1.8223721981048584, time: 20:31:18, step: 858\nloss: 1.8670873641967773, time: 20:31:23, step: 859\nloss: 1.9467169046401978, time: 20:31:29, step: 860\n----- Time -> 20:31:33 ----- Validation Batch -> 1 ----  Validation Loss -> 1.9962\n----- Time -> 20:31:34 ----- Validation Batch -> 2 ----  Validation Loss -> 1.8665\n----- Time -> 20:31:34 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5829\n----- Time -> 20:31:35 ----- Validation Batch -> 4 ----  Validation Loss -> 1.7164\n----- Time -> 20:31:35 ----- Validation Batch -> 5 ----  Validation Loss -> 1.8564\n----- Time -> 20:31:35 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.8036731\nloss: 1.8680717945098877, time: 20:31:37, step: 861\nloss: 1.908833622932434, time: 20:31:42, step: 862\nloss: 1.6742172241210938, time: 20:31:48, step: 863\nloss: 1.8606973886489868, time: 20:31:53, step: 864\nloss: 1.8433340787887573, time: 20:31:59, step: 865\nloss: 1.8345407247543335, time: 20:32:04, step: 866\nloss: 1.7260652780532837, time: 20:32:09, step: 867\nloss: 1.5651177167892456, time: 20:32:15, step: 868\nloss: 1.8978790044784546, time: 20:32:20, step: 869\nloss: 1.8191332817077637, time: 20:32:26, step: 870\nloss: 1.8481427431106567, time: 20:32:31, step: 871\nloss: 1.9313600063323975, time: 20:32:37, step: 872\nloss: 1.9233105182647705, time: 20:32:42, step: 873\nloss: 1.684441089630127, time: 20:32:48, step: 874\nloss: 1.9550074338912964, time: 20:32:53, step: 875\nloss: 1.7968100309371948, time: 20:32:59, step: 876\nloss: 1.8436630964279175, time: 20:33:04, step: 877\nloss: 1.8757679462432861, time: 20:33:09, step: 878\nloss: 1.7288812398910522, time: 20:33:15, step: 879\nloss: 1.8056679964065552, time: 20:33:20, step: 880\n----- Time -> 20:33:25 ----- Validation Batch -> 1 ----  Validation Loss -> 1.9852\n----- Time -> 20:33:25 ----- Validation Batch -> 2 ----  Validation Loss -> 1.8564\n----- Time -> 20:33:26 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5771\n----- Time -> 20:33:26 ----- Validation Batch -> 4 ----  Validation Loss -> 1.7061\n----- Time -> 20:33:27 ----- Validation Batch -> 5 ----  Validation Loss -> 1.8464\n----- Time -> 20:33:27 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.7942621\nloss: 1.9317559003829956, time: 20:33:28, step: 881\nloss: 1.7294176816940308, time: 20:33:34, step: 882\nloss: 1.96364164352417, time: 20:33:39, step: 883\nloss: 1.8140580654144287, time: 20:33:45, step: 884\nloss: 1.7579631805419922, time: 20:33:50, step: 885\nloss: 1.8215720653533936, time: 20:33:56, step: 886\nloss: 1.9683092832565308, time: 20:34:01, step: 887\nloss: 1.924208164215088, time: 20:34:07, step: 888\nloss: 1.7038094997406006, time: 20:34:12, step: 889\nloss: 1.8767212629318237, time: 20:34:17, step: 890\nloss: 1.5628331899642944, time: 20:34:23, step: 891\nloss: 1.682472825050354, time: 20:34:28, step: 892\nloss: 1.846699833869934, time: 20:34:34, step: 893\nloss: 1.7950164079666138, time: 20:34:39, step: 894\nloss: 1.6849143505096436, time: 20:34:45, step: 895\nloss: 1.8115190267562866, time: 20:34:50, step: 896\nloss: 1.745323657989502, time: 20:34:55, step: 897\nloss: 1.9993730783462524, time: 20:35:01, step: 898\nloss: 1.8949471712112427, time: 20:35:06, step: 899\nloss: 1.7542178630828857, time: 20:35:12, step: 900\n----- Time -> 20:35:16 ----- Validation Batch -> 1 ----  Validation Loss -> 1.9748\n----- Time -> 20:35:17 ----- Validation Batch -> 2 ----  Validation Loss -> 1.8458\n----- Time -> 20:35:17 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5700\n----- Time -> 20:35:18 ----- Validation Batch -> 4 ----  Validation Loss -> 1.6967\n----- Time -> 20:35:18 ----- Validation Batch -> 5 ----  Validation Loss -> 1.8349\n----- Time -> 20:35:18 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.7844315\nloss: 1.8497148752212524, time: 20:35:20, step: 901\nloss: 1.7976326942443848, time: 20:35:25, step: 902\nloss: 1.71255624294281, time: 20:35:31, step: 903\nloss: 1.715293526649475, time: 20:35:36, step: 904\nloss: 1.8852934837341309, time: 20:35:42, step: 905\nloss: 1.7967385053634644, time: 20:35:47, step: 906\nloss: 1.7504198551177979, time: 20:35:53, step: 907\nloss: 1.629832148551941, time: 20:35:58, step: 908\nloss: 1.6435315608978271, time: 20:36:04, step: 909\nloss: 1.9323934316635132, time: 20:36:09, step: 910\nloss: 1.8386635780334473, time: 20:36:14, step: 911\nloss: 1.8295910358428955, time: 20:36:20, step: 912\nloss: 1.8872264623641968, time: 20:36:25, step: 913\nloss: 1.8258248567581177, time: 20:36:31, step: 914\nloss: 1.763892650604248, time: 20:36:36, step: 915\nloss: 1.6889395713806152, time: 20:36:42, step: 916\nloss: 1.7136017084121704, time: 20:36:47, step: 917\nloss: 1.9310657978057861, time: 20:36:53, step: 918\nloss: 1.834169864654541, time: 20:36:58, step: 919\nloss: 1.689382553100586, time: 20:37:03, step: 920\n----- Time -> 20:37:08 ----- Validation Batch -> 1 ----  Validation Loss -> 1.9640\n----- Time -> 20:37:08 ----- Validation Batch -> 2 ----  Validation Loss -> 1.8330\n----- Time -> 20:37:09 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5616\n----- Time -> 20:37:09 ----- Validation Batch -> 4 ----  Validation Loss -> 1.6869\n----- Time -> 20:37:10 ----- Validation Batch -> 5 ----  Validation Loss -> 1.8212\n----- Time -> 20:37:10 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.7733372\nloss: 1.8670612573623657, time: 20:37:12, step: 921\nloss: 1.6756342649459839, time: 20:37:17, step: 922\nloss: 1.5608665943145752, time: 20:37:22, step: 923\nloss: 1.9082671403884888, time: 20:37:28, step: 924\nloss: 1.7923126220703125, time: 20:37:33, step: 925\nloss: 1.732412338256836, time: 20:37:39, step: 926\nloss: 1.9422467947006226, time: 20:37:44, step: 927\nloss: 1.8755850791931152, time: 20:37:50, step: 928\nloss: 1.7912442684173584, time: 20:37:55, step: 929\nloss: 1.5818976163864136, time: 20:38:01, step: 930\nloss: 2.193622589111328, time: 20:38:06, step: 931\nloss: 1.8484941720962524, time: 20:38:12, step: 932\nloss: 1.6416833400726318, time: 20:38:17, step: 933\nloss: 1.6407257318496704, time: 20:38:22, step: 934\nloss: 1.7343143224716187, time: 20:38:28, step: 935\nloss: 1.7865829467773438, time: 20:38:33, step: 936\nloss: 1.7526129484176636, time: 20:38:39, step: 937\nloss: 1.7829315662384033, time: 20:38:44, step: 938\nloss: 1.7082022428512573, time: 20:38:50, step: 939\nloss: 1.7284170389175415, time: 20:38:55, step: 940\n----- Time -> 20:39:00 ----- Validation Batch -> 1 ----  Validation Loss -> 1.9496\n----- Time -> 20:39:00 ----- Validation Batch -> 2 ----  Validation Loss -> 1.8228\n----- Time -> 20:39:01 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5515\n----- Time -> 20:39:01 ----- Validation Batch -> 4 ----  Validation Loss -> 1.6738\n----- Time -> 20:39:02 ----- Validation Batch -> 5 ----  Validation Loss -> 1.8054\n----- Time -> 20:39:02 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.7606256\nloss: 1.6995341777801514, time: 20:39:03, step: 941\nloss: 1.6532069444656372, time: 20:39:09, step: 942\nloss: 1.7437407970428467, time: 20:39:14, step: 943\nloss: 1.931900978088379, time: 20:39:20, step: 944\nloss: 1.7075258493423462, time: 20:39:25, step: 945\nloss: 1.512198805809021, time: 20:39:31, step: 946\nloss: 1.6709063053131104, time: 20:39:36, step: 947\nloss: 1.6850491762161255, time: 20:39:41, step: 948\nloss: 2.0434744358062744, time: 20:39:47, step: 949\nloss: 1.8328869342803955, time: 20:39:52, step: 950\nloss: 1.9613451957702637, time: 20:39:58, step: 951\nloss: 1.811686635017395, time: 20:40:03, step: 952\nloss: 1.8241145610809326, time: 20:40:09, step: 953\nloss: 1.5491636991500854, time: 20:40:14, step: 954\nloss: 1.6843961477279663, time: 20:40:20, step: 955\nloss: 1.6358914375305176, time: 20:40:25, step: 956\nloss: 1.6053004264831543, time: 20:40:30, step: 957\nloss: 1.85886549949646, time: 20:40:36, step: 958\nloss: 1.6613729000091553, time: 20:40:41, step: 959\nloss: 1.7749381065368652, time: 20:40:47, step: 960\n----- Time -> 20:40:51 ----- Validation Batch -> 1 ----  Validation Loss -> 1.9402\n----- Time -> 20:40:52 ----- Validation Batch -> 2 ----  Validation Loss -> 1.8114\n----- Time -> 20:40:52 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5440\n----- Time -> 20:40:53 ----- Validation Batch -> 4 ----  Validation Loss -> 1.6629\n----- Time -> 20:40:53 ----- Validation Batch -> 5 ----  Validation Loss -> 1.7955\n----- Time -> 20:40:53 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.7508114\nloss: 1.7630497217178345, time: 20:40:55, step: 961\nloss: 2.00400972366333, time: 20:41:00, step: 962\nloss: 1.934076189994812, time: 20:41:06, step: 963\nloss: 1.6805589199066162, time: 20:41:11, step: 964\nloss: 1.5765089988708496, time: 20:41:17, step: 965\nloss: 1.711810827255249, time: 20:41:22, step: 966\nloss: 1.5830074548721313, time: 20:41:28, step: 967\nloss: 1.8745535612106323, time: 20:41:33, step: 968\nloss: 1.881819486618042, time: 20:41:39, step: 969\nloss: 1.6882070302963257, time: 20:41:44, step: 970\nloss: 1.6590884923934937, time: 20:41:49, step: 971\nloss: 1.8877190351486206, time: 20:41:55, step: 972\nloss: 1.6936808824539185, time: 20:42:00, step: 973\nloss: 1.7361583709716797, time: 20:42:06, step: 974\nloss: 1.3905845880508423, time: 20:42:11, step: 975\nloss: 1.7040890455245972, time: 20:42:17, step: 976\nloss: 1.650222897529602, time: 20:42:22, step: 977\nloss: 1.7687691450119019, time: 20:42:28, step: 978\nloss: 1.6574971675872803, time: 20:42:33, step: 979\nloss: 1.5360724925994873, time: 20:42:38, step: 980\n----- Time -> 20:42:43 ----- Validation Batch -> 1 ----  Validation Loss -> 1.9297\n----- Time -> 20:42:43 ----- Validation Batch -> 2 ----  Validation Loss -> 1.7989\n----- Time -> 20:42:44 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5366\n----- Time -> 20:42:45 ----- Validation Batch -> 4 ----  Validation Loss -> 1.6503\n----- Time -> 20:42:45 ----- Validation Batch -> 5 ----  Validation Loss -> 1.7818\n----- Time -> 20:42:45 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.7394583\nloss: 1.7809090614318848, time: 20:42:47, step: 981\nloss: 1.9139854907989502, time: 20:42:52, step: 982\nloss: 2.0234014987945557, time: 20:42:58, step: 983\nloss: 1.4275885820388794, time: 20:43:03, step: 984\nloss: 1.4906717538833618, time: 20:43:09, step: 985\nloss: 1.674063801765442, time: 20:43:14, step: 986\nloss: 1.8125724792480469, time: 20:43:19, step: 987\nloss: 1.8100097179412842, time: 20:43:25, step: 988\nloss: 1.7055284976959229, time: 20:43:30, step: 989\nloss: 1.684460163116455, time: 20:43:36, step: 990\nloss: 1.7755006551742554, time: 20:43:41, step: 991\nloss: 1.7622957229614258, time: 20:43:47, step: 992\nloss: 1.873233675956726, time: 20:43:52, step: 993\nloss: 1.727308750152588, time: 20:43:58, step: 994\nloss: 1.7633132934570312, time: 20:44:03, step: 995\nloss: 1.6920160055160522, time: 20:44:08, step: 996\nloss: 1.591776728630066, time: 20:44:14, step: 997\nloss: 1.6288166046142578, time: 20:44:19, step: 998\nloss: 1.8093229532241821, time: 20:44:25, step: 999\nloss: 1.5481079816818237, time: 20:44:30, step: 1000\n----- Time -> 20:44:35 ----- Validation Batch -> 1 ----  Validation Loss -> 1.9192\n----- Time -> 20:44:35 ----- Validation Batch -> 2 ----  Validation Loss -> 1.7872\n----- Time -> 20:44:36 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5261\n----- Time -> 20:44:36 ----- Validation Batch -> 4 ----  Validation Loss -> 1.6397\n----- Time -> 20:44:37 ----- Validation Batch -> 5 ----  Validation Loss -> 1.7715\n----- Time -> 20:44:37 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.7287243\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "want to continue training after 1000 steps \n"
        },
        {
          "name": "stdout",
          "text": "loss: 1.7331489324569702, time: 20:46:39, step: 1001\nloss: 1.7081866264343262, time: 20:46:45, step: 1002\nloss: 1.8107364177703857, time: 20:46:50, step: 1003\nloss: 1.8283324241638184, time: 20:46:55, step: 1004\nloss: 1.685908555984497, time: 20:47:01, step: 1005\nloss: 1.8649673461914062, time: 20:47:06, step: 1006\nloss: 1.5657192468643188, time: 20:47:12, step: 1007\nloss: 1.7067697048187256, time: 20:47:17, step: 1008\nloss: 1.9105116128921509, time: 20:47:23, step: 1009\nloss: 1.7716692686080933, time: 20:47:28, step: 1010\nloss: 1.6767313480377197, time: 20:47:34, step: 1011\nloss: 1.8783650398254395, time: 20:47:39, step: 1012\nloss: 1.8751152753829956, time: 20:47:44, step: 1013\nloss: 1.723248839378357, time: 20:47:50, step: 1014\nloss: 1.8573319911956787, time: 20:47:55, step: 1015\nloss: 1.636582374572754, time: 20:48:01, step: 1016\nloss: 1.7426223754882812, time: 20:48:06, step: 1017\nloss: 1.5691308975219727, time: 20:48:12, step: 1018\nloss: 1.9785736799240112, time: 20:48:17, step: 1019\nloss: 1.7008910179138184, time: 20:48:23, step: 1020\n----- Time -> 20:48:27 ----- Validation Batch -> 1 ----  Validation Loss -> 1.9047\n----- Time -> 20:48:28 ----- Validation Batch -> 2 ----  Validation Loss -> 1.7733\n----- Time -> 20:48:28 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5181\n----- Time -> 20:48:29 ----- Validation Batch -> 4 ----  Validation Loss -> 1.6303\n----- Time -> 20:48:29 ----- Validation Batch -> 5 ----  Validation Loss -> 1.7570\n----- Time -> 20:48:29 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.7166818\nloss: 1.4196832180023193, time: 20:48:31, step: 1021\nloss: 1.8393751382827759, time: 20:48:36, step: 1022\nloss: 1.8170300722122192, time: 20:48:42, step: 1023\nloss: 1.6860922574996948, time: 20:48:47, step: 1024\nloss: 1.8843306303024292, time: 20:48:53, step: 1025\nloss: 1.9591927528381348, time: 20:48:58, step: 1026\nloss: 1.8151435852050781, time: 20:49:04, step: 1027\nloss: 1.7859110832214355, time: 20:49:09, step: 1028\nloss: 1.7286827564239502, time: 20:49:15, step: 1029\nloss: 1.5902456045150757, time: 20:49:20, step: 1030\nloss: 1.7351067066192627, time: 20:49:26, step: 1031\nloss: 1.7102737426757812, time: 20:49:31, step: 1032\nloss: 1.7912871837615967, time: 20:49:36, step: 1033\nloss: 1.7790820598602295, time: 20:49:42, step: 1034\nloss: 1.7187304496765137, time: 20:49:47, step: 1035\nloss: 1.7650940418243408, time: 20:49:53, step: 1036\nloss: 1.8794257640838623, time: 20:49:58, step: 1037\nloss: 1.5680615901947021, time: 20:50:04, step: 1038\nloss: 1.718077301979065, time: 20:50:09, step: 1039\nloss: 1.67611825466156, time: 20:50:15, step: 1040\n----- Time -> 20:50:19 ----- Validation Batch -> 1 ----  Validation Loss -> 1.8936\n----- Time -> 20:50:20 ----- Validation Batch -> 2 ----  Validation Loss -> 1.7638\n----- Time -> 20:50:20 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5090\n----- Time -> 20:50:21 ----- Validation Batch -> 4 ----  Validation Loss -> 1.6168\n----- Time -> 20:50:21 ----- Validation Batch -> 5 ----  Validation Loss -> 1.7435\n----- Time -> 20:50:21 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.7053418\nloss: 1.6945897340774536, time: 20:50:23, step: 1041\nloss: 1.6694055795669556, time: 20:50:28, step: 1042\nloss: 1.6901659965515137, time: 20:50:34, step: 1043\nloss: 1.6016193628311157, time: 20:50:39, step: 1044\nloss: 1.8442498445510864, time: 20:50:45, step: 1045\nloss: 1.543344497680664, time: 20:50:50, step: 1046\nloss: 1.6020267009735107, time: 20:50:56, step: 1047\nloss: 1.5634582042694092, time: 20:51:01, step: 1048\nloss: 1.7192001342773438, time: 20:51:07, step: 1049\nloss: 1.753664493560791, time: 20:51:12, step: 1050\nloss: 1.568689227104187, time: 20:51:18, step: 1051\nloss: 1.6727979183197021, time: 20:51:23, step: 1052\nloss: 1.7116838693618774, time: 20:51:29, step: 1053\nloss: 1.8510503768920898, time: 20:51:34, step: 1054\nloss: 1.7077480554580688, time: 20:51:39, step: 1055\nloss: 1.6258723735809326, time: 20:51:45, step: 1056\nloss: 1.7003891468048096, time: 20:51:50, step: 1057\nloss: 1.7275229692459106, time: 20:51:56, step: 1058\nloss: 1.7025014162063599, time: 20:52:01, step: 1059\nloss: 1.5158870220184326, time: 20:52:07, step: 1060\n----- Time -> 20:52:11 ----- Validation Batch -> 1 ----  Validation Loss -> 1.8832\n----- Time -> 20:52:12 ----- Validation Batch -> 2 ----  Validation Loss -> 1.7511\n----- Time -> 20:52:12 ----- Validation Batch -> 3 ----  Validation Loss -> 1.5027\n----- Time -> 20:52:13 ----- Validation Batch -> 4 ----  Validation Loss -> 1.6061\n----- Time -> 20:52:13 ----- Validation Batch -> 5 ----  Validation Loss -> 1.7336\n----- Time -> 20:52:13 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.6953409\nloss: 1.6865074634552002, time: 20:52:15, step: 1061\nloss: 1.6772748231887817, time: 20:52:20, step: 1062\nloss: 1.8680559396743774, time: 20:52:26, step: 1063\nloss: 1.7550016641616821, time: 20:52:31, step: 1064\nloss: 1.819929599761963, time: 20:52:37, step: 1065\nloss: 1.7429887056350708, time: 20:52:42, step: 1066\nloss: 1.7368279695510864, time: 20:52:48, step: 1067\nloss: 1.5978690385818481, time: 20:52:53, step: 1068\nloss: 1.4882283210754395, time: 20:52:59, step: 1069\nloss: 1.66807222366333, time: 20:53:04, step: 1070\nloss: 1.6212127208709717, time: 20:53:10, step: 1071\nloss: 1.6944754123687744, time: 20:53:20, step: 1073\nloss: 1.6690422296524048, time: 20:53:26, step: 1074\nloss: 1.5995057821273804, time: 20:53:31, step: 1075\nloss: 1.7424291372299194, time: 20:53:37, step: 1076\nloss: 1.7937761545181274, time: 20:53:42, step: 1077\nloss: 1.677086353302002, time: 20:53:48, step: 1078\nloss: 1.5908914804458618, time: 20:53:53, step: 1079\nloss: 1.7018133401870728, time: 20:53:59, step: 1080\n----- Time -> 20:54:03 ----- Validation Batch -> 1 ----  Validation Loss -> 1.8699\n----- Time -> 20:54:04 ----- Validation Batch -> 2 ----  Validation Loss -> 1.7399\n----- Time -> 20:54:04 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4910\n----- Time -> 20:54:05 ----- Validation Batch -> 4 ----  Validation Loss -> 1.5950\n----- Time -> 20:54:05 ----- Validation Batch -> 5 ----  Validation Loss -> 1.7190\n----- Time -> 20:54:05 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.6829505\nloss: 1.5162080526351929, time: 20:54:07, step: 1081\nloss: 1.778709888458252, time: 20:54:12, step: 1082\nloss: 1.6422240734100342, time: 20:54:18, step: 1083\nloss: 1.6778156757354736, time: 20:54:23, step: 1084\nloss: 1.6429548263549805, time: 20:54:29, step: 1085\nloss: 1.6797187328338623, time: 20:54:34, step: 1086\nloss: 1.5863969326019287, time: 20:54:40, step: 1087\nloss: 1.5454761981964111, time: 20:54:45, step: 1088\nloss: 1.660377860069275, time: 20:54:51, step: 1089\nloss: 1.6717158555984497, time: 20:54:56, step: 1090\nloss: 1.7855499982833862, time: 20:55:02, step: 1091\nloss: 1.8304473161697388, time: 20:55:07, step: 1092\nloss: 1.4091421365737915, time: 20:55:13, step: 1093\nloss: 1.560300588607788, time: 20:55:18, step: 1094\nloss: 1.4904680252075195, time: 20:55:23, step: 1095\nloss: 1.734959363937378, time: 20:55:29, step: 1096\nloss: 1.7848018407821655, time: 20:55:34, step: 1097\nloss: 1.67214035987854, time: 20:55:40, step: 1098\nloss: 1.7296717166900635, time: 20:55:45, step: 1099\nloss: 1.741230845451355, time: 20:55:51, step: 1100\n----- Time -> 20:55:55 ----- Validation Batch -> 1 ----  Validation Loss -> 1.8586\n----- Time -> 20:55:56 ----- Validation Batch -> 2 ----  Validation Loss -> 1.7308\n----- Time -> 20:55:56 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4844\n----- Time -> 20:55:57 ----- Validation Batch -> 4 ----  Validation Loss -> 1.5860\n----- Time -> 20:55:57 ----- Validation Batch -> 5 ----  Validation Loss -> 1.7074\n----- Time -> 20:55:57 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.6734499\nloss: 1.4482313394546509, time: 20:55:59, step: 1101\nloss: 1.690754771232605, time: 20:56:04, step: 1102\nloss: 1.7294692993164062, time: 20:56:10, step: 1103\nloss: 1.6522496938705444, time: 20:56:15, step: 1104\nloss: 1.874320387840271, time: 20:56:21, step: 1105\nloss: 1.6242225170135498, time: 20:56:26, step: 1106\nloss: 1.7281429767608643, time: 20:56:32, step: 1107\nloss: 1.6134281158447266, time: 20:56:37, step: 1108\nloss: 1.6668587923049927, time: 20:56:43, step: 1109\nloss: 1.6072380542755127, time: 20:56:48, step: 1110\nloss: 1.6857870817184448, time: 20:56:54, step: 1111\nloss: 1.8455849885940552, time: 20:56:59, step: 1112\nloss: 1.6965677738189697, time: 20:57:04, step: 1113\nloss: 1.8175886869430542, time: 20:57:10, step: 1114\nloss: 1.5357002019882202, time: 20:57:15, step: 1115\nloss: 1.486336588859558, time: 20:57:21, step: 1116\nloss: 1.6077501773834229, time: 20:57:26, step: 1117\nloss: 1.5957789421081543, time: 20:57:32, step: 1118\nloss: 1.66727614402771, time: 20:57:37, step: 1119\nloss: 1.6869919300079346, time: 20:57:43, step: 1120\n----- Time -> 20:57:47 ----- Validation Batch -> 1 ----  Validation Loss -> 1.8490\n----- Time -> 20:57:48 ----- Validation Batch -> 2 ----  Validation Loss -> 1.7187\n----- Time -> 20:57:48 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4756\n----- Time -> 20:57:49 ----- Validation Batch -> 4 ----  Validation Loss -> 1.5748\n----- Time -> 20:57:49 ----- Validation Batch -> 5 ----  Validation Loss -> 1.6963\n----- Time -> 20:57:49 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.6628608\nloss: 1.5777121782302856, time: 20:57:51, step: 1121\nloss: 1.6353940963745117, time: 20:57:56, step: 1122\nloss: 1.64284086227417, time: 20:58:02, step: 1123\nloss: 1.660250186920166, time: 20:58:07, step: 1124\nloss: 1.767556071281433, time: 20:58:13, step: 1125\nloss: 1.6728543043136597, time: 20:58:18, step: 1126\nloss: 1.5834676027297974, time: 20:58:23, step: 1127\nloss: 1.62661612033844, time: 20:58:29, step: 1128\nloss: 1.4950549602508545, time: 20:58:34, step: 1129\nloss: 1.6517552137374878, time: 20:58:40, step: 1130\nloss: 1.7682087421417236, time: 20:58:45, step: 1131\nloss: 1.5623291730880737, time: 20:58:51, step: 1132\nloss: 1.7922786474227905, time: 20:58:56, step: 1133\nloss: 1.665087342262268, time: 20:59:02, step: 1134\nloss: 1.5728768110275269, time: 20:59:07, step: 1135\nloss: 1.6207438707351685, time: 20:59:13, step: 1136\nloss: 1.61531662940979, time: 20:59:18, step: 1137\nloss: 1.631471037864685, time: 20:59:23, step: 1138\nloss: 1.5766096115112305, time: 20:59:29, step: 1139\nloss: 1.6751525402069092, time: 20:59:34, step: 1140\n----- Time -> 20:59:39 ----- Validation Batch -> 1 ----  Validation Loss -> 1.8385\n----- Time -> 20:59:39 ----- Validation Batch -> 2 ----  Validation Loss -> 1.7097\n----- Time -> 20:59:40 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4662\n----- Time -> 20:59:40 ----- Validation Batch -> 4 ----  Validation Loss -> 1.5643\n----- Time -> 20:59:41 ----- Validation Batch -> 5 ----  Validation Loss -> 1.6853\n----- Time -> 20:59:41 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.6528004\nloss: 1.7612700462341309, time: 20:59:43, step: 1141\nloss: 1.5936143398284912, time: 20:59:48, step: 1142\nloss: 1.6734634637832642, time: 20:59:53, step: 1143\nloss: 1.8365718126296997, time: 20:59:59, step: 1144\nloss: 1.5725642442703247, time: 21:00:04, step: 1145\nloss: 1.7424079179763794, time: 21:00:10, step: 1146\nloss: 1.5887413024902344, time: 21:00:15, step: 1147\nloss: 1.5844089984893799, time: 21:00:21, step: 1148\nloss: 1.7304831743240356, time: 21:00:26, step: 1149\nloss: 1.5855709314346313, time: 21:00:31, step: 1150\nloss: 1.5831328630447388, time: 21:00:37, step: 1151\nloss: 1.8001775741577148, time: 21:00:42, step: 1152\nloss: 1.696498155593872, time: 21:00:48, step: 1153\nloss: 1.5569173097610474, time: 21:00:53, step: 1154\nloss: 1.6735687255859375, time: 21:00:59, step: 1155\nloss: 1.4476902484893799, time: 21:01:04, step: 1156\nloss: 1.5051206350326538, time: 21:01:10, step: 1157\nloss: 1.4969269037246704, time: 21:01:15, step: 1158\nloss: 1.598597526550293, time: 21:01:21, step: 1159\nloss: 1.5016870498657227, time: 21:01:26, step: 1160\n----- Time -> 21:01:31 ----- Validation Batch -> 1 ----  Validation Loss -> 1.8270\n----- Time -> 21:01:31 ----- Validation Batch -> 2 ----  Validation Loss -> 1.6993\n----- Time -> 21:01:32 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4568\n----- Time -> 21:01:32 ----- Validation Batch -> 4 ----  Validation Loss -> 1.5535\n----- Time -> 21:01:33 ----- Validation Batch -> 5 ----  Validation Loss -> 1.6730\n----- Time -> 21:01:33 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.6419193\nloss: 1.6336798667907715, time: 21:01:34, step: 1161\nloss: 1.6310827732086182, time: 21:01:40, step: 1162\nloss: 1.5924159288406372, time: 21:01:45, step: 1163\nloss: 1.5247321128845215, time: 21:01:51, step: 1164\nloss: 1.3101351261138916, time: 21:01:56, step: 1165\nloss: 1.6035730838775635, time: 21:02:02, step: 1166\nloss: 1.63382887840271, time: 21:02:07, step: 1167\nloss: 1.4858348369598389, time: 21:02:12, step: 1168\nloss: 1.6641649007797241, time: 21:02:18, step: 1169\nloss: 1.8583006858825684, time: 21:02:23, step: 1170\nloss: 1.661653757095337, time: 21:02:29, step: 1171\nloss: 1.8701850175857544, time: 21:02:34, step: 1172\nloss: 1.692292332649231, time: 21:02:40, step: 1173\nloss: 1.5657004117965698, time: 21:02:45, step: 1174\nloss: 1.7654324769973755, time: 21:02:51, step: 1175\nloss: 1.5231719017028809, time: 21:02:56, step: 1176\nloss: 1.62473726272583, time: 21:03:02, step: 1177\nloss: 1.8047481775283813, time: 21:03:07, step: 1178\nloss: 1.5167630910873413, time: 21:03:12, step: 1179\nloss: 1.655880331993103, time: 21:03:18, step: 1180\n----- Time -> 21:03:22 ----- Validation Batch -> 1 ----  Validation Loss -> 1.8205\n----- Time -> 21:03:23 ----- Validation Batch -> 2 ----  Validation Loss -> 1.6913\n----- Time -> 21:03:23 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4502\n----- Time -> 21:03:24 ----- Validation Batch -> 4 ----  Validation Loss -> 1.5470\n----- Time -> 21:03:25 ----- Validation Batch -> 5 ----  Validation Loss -> 1.6657\n----- Time -> 21:03:25 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.6349430\nloss: 1.6469552516937256, time: 21:03:26, step: 1181\nloss: 1.6700983047485352, time: 21:03:32, step: 1182\nloss: 1.493208885192871, time: 21:03:37, step: 1183\nloss: 1.6827480792999268, time: 21:03:43, step: 1184\nloss: 1.7697758674621582, time: 21:03:48, step: 1185\nloss: 1.6440176963806152, time: 21:03:53, step: 1186\nloss: 1.4564783573150635, time: 21:03:59, step: 1187\nloss: 1.459791660308838, time: 21:04:04, step: 1188\nloss: 1.578269600868225, time: 21:04:10, step: 1189\nloss: 1.5368136167526245, time: 21:04:15, step: 1190\nloss: 1.5985133647918701, time: 21:04:21, step: 1191\nloss: 1.7000606060028076, time: 21:04:26, step: 1192\nloss: 1.927560806274414, time: 21:04:32, step: 1193\nloss: 1.540178656578064, time: 21:04:37, step: 1194\nloss: 1.8080071210861206, time: 21:04:43, step: 1195\nloss: 1.63461434841156, time: 21:04:48, step: 1196\nloss: 1.7490936517715454, time: 21:04:54, step: 1197\nloss: 1.7262760400772095, time: 21:04:59, step: 1198\nloss: 1.562036156654358, time: 21:05:05, step: 1199\nloss: 1.6612224578857422, time: 21:05:10, step: 1200\n----- Time -> 21:05:15 ----- Validation Batch -> 1 ----  Validation Loss -> 1.8104\n----- Time -> 21:05:15 ----- Validation Batch -> 2 ----  Validation Loss -> 1.6791\n----- Time -> 21:05:16 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4429\n----- Time -> 21:05:16 ----- Validation Batch -> 4 ----  Validation Loss -> 1.5361\n----- Time -> 21:05:17 ----- Validation Batch -> 5 ----  Validation Loss -> 1.6573\n----- Time -> 21:05:17 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.6251499\nloss: 1.5362480878829956, time: 21:05:18, step: 1201\nloss: 1.566964030265808, time: 21:05:24, step: 1202\nloss: 1.5610569715499878, time: 21:05:29, step: 1203\nloss: 1.6874845027923584, time: 21:05:35, step: 1204\nloss: 1.5652579069137573, time: 21:05:40, step: 1205\nloss: 1.6044143438339233, time: 21:05:46, step: 1206\nloss: 1.6755106449127197, time: 21:05:51, step: 1207\nloss: 1.5501494407653809, time: 21:05:57, step: 1208\nloss: 1.5233755111694336, time: 21:06:02, step: 1209\nloss: 1.5210031270980835, time: 21:06:08, step: 1210\nloss: 1.7104637622833252, time: 21:06:13, step: 1211\nloss: 1.6450692415237427, time: 21:06:19, step: 1212\nloss: 1.534349799156189, time: 21:06:24, step: 1213\nloss: 1.8560866117477417, time: 21:06:30, step: 1214\nloss: 1.544121265411377, time: 21:06:35, step: 1215\nloss: 1.7250999212265015, time: 21:06:41, step: 1216\nloss: 1.5714664459228516, time: 21:06:46, step: 1217\nloss: 1.5651980638504028, time: 21:06:52, step: 1218\nloss: 1.4390439987182617, time: 21:06:57, step: 1219\nloss: 1.6078966856002808, time: 21:07:03, step: 1220\n----- Time -> 21:07:07 ----- Validation Batch -> 1 ----  Validation Loss -> 1.8026\n----- Time -> 21:07:08 ----- Validation Batch -> 2 ----  Validation Loss -> 1.6731\n----- Time -> 21:07:08 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4349\n----- Time -> 21:07:09 ----- Validation Batch -> 4 ----  Validation Loss -> 1.5274\n----- Time -> 21:07:09 ----- Validation Batch -> 5 ----  Validation Loss -> 1.6435\n----- Time -> 21:07:09 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.6163006\nloss: 1.6488523483276367, time: 21:07:11, step: 1221\nloss: 1.633126974105835, time: 21:07:16, step: 1222\nloss: 1.6585999727249146, time: 21:07:22, step: 1223\nloss: 1.620556354522705, time: 21:07:27, step: 1224\nloss: 1.60212242603302, time: 21:07:33, step: 1225\nloss: 1.2954689264297485, time: 21:07:38, step: 1226\nloss: 1.5165987014770508, time: 21:07:44, step: 1227\nloss: 1.489396572113037, time: 21:07:49, step: 1228\nloss: 1.7143224477767944, time: 21:07:55, step: 1229\nloss: 1.6970627307891846, time: 21:08:01, step: 1230\nloss: 1.559611439704895, time: 21:08:06, step: 1231\nloss: 1.5758709907531738, time: 21:08:12, step: 1232\nloss: 1.4582608938217163, time: 21:08:17, step: 1233\nloss: 1.7087825536727905, time: 21:08:23, step: 1234\nloss: 1.5076824426651, time: 21:08:28, step: 1235\nloss: 1.6374770402908325, time: 21:08:33, step: 1236\nloss: 1.5520281791687012, time: 21:08:39, step: 1237\nloss: 1.6743284463882446, time: 21:08:44, step: 1238\nloss: 1.5724830627441406, time: 21:08:50, step: 1239\nloss: 1.529542326927185, time: 21:08:55, step: 1240\n----- Time -> 21:09:00 ----- Validation Batch -> 1 ----  Validation Loss -> 1.7926\n----- Time -> 21:09:00 ----- Validation Batch -> 2 ----  Validation Loss -> 1.6633\n----- Time -> 21:09:01 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4289\n----- Time -> 21:09:02 ----- Validation Batch -> 4 ----  Validation Loss -> 1.5176\n----- Time -> 21:09:02 ----- Validation Batch -> 5 ----  Validation Loss -> 1.6366\n----- Time -> 21:09:02 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.6077874\nloss: 1.6938573122024536, time: 21:09:04, step: 1241\nloss: 1.561505913734436, time: 21:09:09, step: 1242\nloss: 1.659442663192749, time: 21:09:15, step: 1243\nloss: 1.4998164176940918, time: 21:09:20, step: 1244\nloss: 1.7298963069915771, time: 21:09:26, step: 1245\nloss: 1.5419188737869263, time: 21:09:31, step: 1246\nloss: 1.6282488107681274, time: 21:09:36, step: 1247\nloss: 1.5398226976394653, time: 21:09:42, step: 1248\nloss: 1.6525166034698486, time: 21:09:48, step: 1249\nloss: 1.6054679155349731, time: 21:09:53, step: 1250\nloss: 1.4723087549209595, time: 21:09:59, step: 1251\nloss: 1.4419372081756592, time: 21:10:04, step: 1252\nloss: 1.586451768875122, time: 21:10:10, step: 1253\nloss: 1.571089267730713, time: 21:10:15, step: 1254\nloss: 1.4751230478286743, time: 21:10:20, step: 1255\nloss: 1.5499309301376343, time: 21:10:26, step: 1256\nloss: 1.614043116569519, time: 21:10:31, step: 1257\nloss: 1.6564407348632812, time: 21:10:37, step: 1258\nloss: 1.6776448488235474, time: 21:10:42, step: 1259\nloss: 1.501063585281372, time: 21:10:48, step: 1260\n----- Time -> 21:10:52 ----- Validation Batch -> 1 ----  Validation Loss -> 1.7830\n----- Time -> 21:10:53 ----- Validation Batch -> 2 ----  Validation Loss -> 1.6523\n----- Time -> 21:10:53 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4195\n----- Time -> 21:10:54 ----- Validation Batch -> 4 ----  Validation Loss -> 1.5096\n----- Time -> 21:10:55 ----- Validation Batch -> 5 ----  Validation Loss -> 1.6233\n----- Time -> 21:10:55 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.5975368\nloss: 1.524161696434021, time: 21:10:56, step: 1261\nloss: 1.5527193546295166, time: 21:11:02, step: 1262\nloss: 1.6449592113494873, time: 21:11:07, step: 1263\nloss: 1.6217347383499146, time: 21:11:13, step: 1264\nloss: 1.6235096454620361, time: 21:11:18, step: 1265\nloss: 1.4436006546020508, time: 21:11:24, step: 1266\nloss: 1.5715073347091675, time: 21:11:29, step: 1267\nloss: 1.6226317882537842, time: 21:11:35, step: 1268\nloss: 1.2853032350540161, time: 21:11:40, step: 1269\nloss: 1.4649461507797241, time: 21:11:45, step: 1270\nloss: 1.6359508037567139, time: 21:11:51, step: 1271\nloss: 1.7366647720336914, time: 21:11:56, step: 1272\nloss: 1.521712303161621, time: 21:12:02, step: 1273\nloss: 1.5656137466430664, time: 21:12:07, step: 1274\nloss: 1.3891289234161377, time: 21:12:13, step: 1275\nloss: 1.6794542074203491, time: 21:12:18, step: 1276\nloss: 1.9482882022857666, time: 21:12:24, step: 1277\nloss: 1.6224806308746338, time: 21:12:29, step: 1278\nloss: 1.5929001569747925, time: 21:12:35, step: 1279\nloss: 1.632602334022522, time: 21:12:40, step: 1280\n----- Time -> 21:12:45 ----- Validation Batch -> 1 ----  Validation Loss -> 1.7729\n----- Time -> 21:12:45 ----- Validation Batch -> 2 ----  Validation Loss -> 1.6437\n----- Time -> 21:12:46 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4120\n----- Time -> 21:12:47 ----- Validation Batch -> 4 ----  Validation Loss -> 1.5004\n----- Time -> 21:12:47 ----- Validation Batch -> 5 ----  Validation Loss -> 1.6150\n----- Time -> 21:12:47 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.5887898\nloss: 1.3829102516174316, time: 21:12:49, step: 1281\nloss: 1.7684217691421509, time: 21:12:54, step: 1282\nloss: 1.576264500617981, time: 21:13:00, step: 1283\nloss: 1.7595322132110596, time: 21:13:05, step: 1284\nloss: 1.5695322751998901, time: 21:13:11, step: 1285\nloss: 1.6422152519226074, time: 21:13:16, step: 1286\nloss: 1.24395751953125, time: 21:13:22, step: 1287\nloss: 1.4819055795669556, time: 21:13:27, step: 1288\nloss: 1.5395244359970093, time: 21:13:33, step: 1289\nloss: 1.4882148504257202, time: 21:13:38, step: 1290\nloss: 1.6071903705596924, time: 21:13:44, step: 1291\nloss: 1.6768772602081299, time: 21:13:49, step: 1292\nloss: 1.5439444780349731, time: 21:13:55, step: 1293\nloss: 1.4774260520935059, time: 21:14:00, step: 1294\nloss: 1.5667574405670166, time: 21:14:06, step: 1295\nloss: 1.5506494045257568, time: 21:14:11, step: 1296\nloss: 1.5776338577270508, time: 21:14:16, step: 1297\nloss: 1.370402455329895, time: 21:14:22, step: 1298\nloss: 1.5307990312576294, time: 21:14:27, step: 1299\nloss: 1.9206275939941406, time: 21:14:33, step: 1300\n----- Time -> 21:14:37 ----- Validation Batch -> 1 ----  Validation Loss -> 1.7636\n----- Time -> 21:14:38 ----- Validation Batch -> 2 ----  Validation Loss -> 1.6335\n----- Time -> 21:14:39 ----- Validation Batch -> 3 ----  Validation Loss -> 1.4029\n----- Time -> 21:14:39 ----- Validation Batch -> 4 ----  Validation Loss -> 1.4915\n----- Time -> 21:14:40 ----- Validation Batch -> 5 ----  Validation Loss -> 1.6021\n----- Time -> 21:14:40 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.5787085\nloss: 1.6881271600723267, time: 21:14:41, step: 1301\nloss: 1.7150832414627075, time: 21:14:47, step: 1302\nloss: 1.6683077812194824, time: 21:14:52, step: 1303\nloss: 1.4949908256530762, time: 21:14:58, step: 1304\nloss: 1.562109112739563, time: 21:15:03, step: 1305\nloss: 1.3383225202560425, time: 21:15:09, step: 1306\nloss: 1.4930744171142578, time: 21:15:14, step: 1307\nloss: 1.4410985708236694, time: 21:15:20, step: 1308\nloss: 1.5734045505523682, time: 21:15:25, step: 1309\nloss: 1.4412922859191895, time: 21:15:31, step: 1310\nloss: 1.5251352787017822, time: 21:15:36, step: 1311\nloss: 1.648491382598877, time: 21:15:42, step: 1312\nloss: 1.5835130214691162, time: 21:15:47, step: 1313\nloss: 1.558107852935791, time: 21:15:53, step: 1314\nloss: 1.5917129516601562, time: 21:15:58, step: 1315\nloss: 1.5990492105484009, time: 21:16:04, step: 1316\nloss: 1.4702025651931763, time: 21:16:09, step: 1317\nloss: 1.4444303512573242, time: 21:16:15, step: 1318\nloss: 1.445900321006775, time: 21:16:20, step: 1319\nloss: 1.4930304288864136, time: 21:16:26, step: 1320\n----- Time -> 21:16:30 ----- Validation Batch -> 1 ----  Validation Loss -> 1.7527\n----- Time -> 21:16:31 ----- Validation Batch -> 2 ----  Validation Loss -> 1.6219\n----- Time -> 21:16:31 ----- Validation Batch -> 3 ----  Validation Loss -> 1.3954\n----- Time -> 21:16:32 ----- Validation Batch -> 4 ----  Validation Loss -> 1.4803\n----- Time -> 21:16:32 ----- Validation Batch -> 5 ----  Validation Loss -> 1.5929\n----- Time -> 21:16:32 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.5686493\nloss: 1.628392219543457, time: 21:16:34, step: 1321\nloss: 1.4594106674194336, time: 21:16:39, step: 1322\nloss: 1.6278555393218994, time: 21:16:45, step: 1323\nloss: 1.4860907793045044, time: 21:16:50, step: 1324\nloss: 1.5615745782852173, time: 21:16:56, step: 1325\nloss: 1.4728128910064697, time: 21:17:01, step: 1326\nloss: 1.5634369850158691, time: 21:17:07, step: 1327\nloss: 1.6693981885910034, time: 21:17:12, step: 1328\nloss: 1.4957793951034546, time: 21:17:18, step: 1329\nloss: 1.6984307765960693, time: 21:17:23, step: 1330\nloss: 1.4730793237686157, time: 21:17:29, step: 1331\nloss: 1.546912431716919, time: 21:17:34, step: 1332\nloss: 1.5614445209503174, time: 21:17:40, step: 1333\nloss: 1.7672075033187866, time: 21:17:45, step: 1334\nloss: 1.5448625087738037, time: 21:17:51, step: 1335\nloss: 1.434762954711914, time: 21:17:56, step: 1336\nloss: 1.5719573497772217, time: 21:18:02, step: 1337\nloss: 1.6753050088882446, time: 21:18:07, step: 1338\nloss: 1.5609827041625977, time: 21:18:13, step: 1339\nloss: 1.54837965965271, time: 21:18:18, step: 1340\n----- Time -> 21:18:23 ----- Validation Batch -> 1 ----  Validation Loss -> 1.7400\n----- Time -> 21:18:23 ----- Validation Batch -> 2 ----  Validation Loss -> 1.6126\n----- Time -> 21:18:24 ----- Validation Batch -> 3 ----  Validation Loss -> 1.3869\n----- Time -> 21:18:24 ----- Validation Batch -> 4 ----  Validation Loss -> 1.4687\n----- Time -> 21:18:25 ----- Validation Batch -> 5 ----  Validation Loss -> 1.5800\n----- Time -> 21:18:25 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.5576255\nloss: 1.731169581413269, time: 21:18:26, step: 1341\nloss: 1.5128687620162964, time: 21:18:32, step: 1342\nloss: 1.4827591180801392, time: 21:18:38, step: 1343\nloss: 1.5449469089508057, time: 21:18:43, step: 1344\nloss: 1.529306411743164, time: 21:18:48, step: 1345\nloss: 1.6591991186141968, time: 21:18:54, step: 1346\nloss: 1.6076637506484985, time: 21:18:59, step: 1347\nloss: 1.6859240531921387, time: 21:19:05, step: 1348\nloss: 1.4873440265655518, time: 21:19:10, step: 1349\nloss: 1.3931732177734375, time: 21:19:16, step: 1350\nloss: 1.5537854433059692, time: 21:19:21, step: 1351\nloss: 1.5706267356872559, time: 21:19:27, step: 1352\nloss: 1.3906221389770508, time: 21:19:32, step: 1353\nloss: 1.63988196849823, time: 21:19:38, step: 1354\nloss: 1.5222828388214111, time: 21:19:43, step: 1355\nloss: 1.6856838464736938, time: 21:19:49, step: 1356\nloss: 1.5621612071990967, time: 21:19:54, step: 1357\nloss: 1.5050472021102905, time: 21:20:00, step: 1358\nloss: 1.6443712711334229, time: 21:20:05, step: 1359\nloss: 1.5501679182052612, time: 21:20:11, step: 1360\n----- Time -> 21:20:15 ----- Validation Batch -> 1 ----  Validation Loss -> 1.7314\n----- Time -> 21:20:16 ----- Validation Batch -> 2 ----  Validation Loss -> 1.6007\n----- Time -> 21:20:16 ----- Validation Batch -> 3 ----  Validation Loss -> 1.3788\n----- Time -> 21:20:17 ----- Validation Batch -> 4 ----  Validation Loss -> 1.4578\n----- Time -> 21:20:18 ----- Validation Batch -> 5 ----  Validation Loss -> 1.5667\n----- Time -> 21:20:18 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.5470910\nloss: 1.6365257501602173, time: 21:20:19, step: 1361\nloss: 1.4732134342193604, time: 21:20:25, step: 1362\nloss: 1.7793437242507935, time: 21:20:30, step: 1363\nloss: 1.442802906036377, time: 21:20:36, step: 1364\nloss: 1.850196123123169, time: 21:20:41, step: 1365\nloss: 1.6016780138015747, time: 21:20:47, step: 1366\nloss: 1.8876543045043945, time: 21:20:52, step: 1367\nloss: 1.5310039520263672, time: 21:20:58, step: 1368\nloss: 1.4580668210983276, time: 21:21:03, step: 1369\nloss: 1.6165764331817627, time: 21:21:09, step: 1370\nloss: 1.5823092460632324, time: 21:21:14, step: 1371\nloss: 1.625777244567871, time: 21:21:20, step: 1372\nloss: 1.5290130376815796, time: 21:21:25, step: 1373\nloss: 1.7099193334579468, time: 21:21:31, step: 1374\nloss: 1.4975703954696655, time: 21:21:36, step: 1375\nloss: 1.4642307758331299, time: 21:21:42, step: 1376\nloss: 1.4157155752182007, time: 21:21:47, step: 1377\nloss: 1.428107500076294, time: 21:21:53, step: 1378\nloss: 1.4658480882644653, time: 21:21:58, step: 1379\nloss: 1.5258713960647583, time: 21:22:04, step: 1380\n----- Time -> 21:22:08 ----- Validation Batch -> 1 ----  Validation Loss -> 1.7190\n----- Time -> 21:22:09 ----- Validation Batch -> 2 ----  Validation Loss -> 1.5869\n----- Time -> 21:22:09 ----- Validation Batch -> 3 ----  Validation Loss -> 1.3685\n----- Time -> 21:22:10 ----- Validation Batch -> 4 ----  Validation Loss -> 1.4479\n----- Time -> 21:22:10 ----- Validation Batch -> 5 ----  Validation Loss -> 1.5555\n----- Time -> 21:22:10 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.5355739\nloss: 1.566219687461853, time: 21:22:12, step: 1381\nloss: 1.633888840675354, time: 21:22:17, step: 1382\nloss: 1.549950122833252, time: 21:22:23, step: 1383\nloss: 1.3792238235473633, time: 21:22:28, step: 1384\nloss: 1.5058307647705078, time: 21:22:34, step: 1385\nloss: 1.7214847803115845, time: 21:22:39, step: 1386\nloss: 1.5000574588775635, time: 21:22:45, step: 1387\nloss: 1.5722694396972656, time: 21:22:50, step: 1388\nloss: 1.4612267017364502, time: 21:22:56, step: 1389\nloss: 1.5465519428253174, time: 21:23:01, step: 1390\nloss: 1.7999130487442017, time: 21:23:07, step: 1391\nloss: 1.6916956901550293, time: 21:23:12, step: 1392\nloss: 1.3805406093597412, time: 21:23:18, step: 1393\nloss: 1.745460033416748, time: 21:23:23, step: 1394\nloss: 1.6765271425247192, time: 21:23:29, step: 1395\nloss: 1.5617847442626953, time: 21:23:34, step: 1396\nloss: 1.3996491432189941, time: 21:23:40, step: 1397\nloss: 1.6220300197601318, time: 21:23:45, step: 1398\nloss: 1.7335747480392456, time: 21:23:51, step: 1399\nloss: 1.529785394668579, time: 21:23:56, step: 1400\n----- Time -> 21:24:01 ----- Validation Batch -> 1 ----  Validation Loss -> 1.7063\n----- Time -> 21:24:01 ----- Validation Batch -> 2 ----  Validation Loss -> 1.5736\n----- Time -> 21:24:02 ----- Validation Batch -> 3 ----  Validation Loss -> 1.3560\n----- Time -> 21:24:02 ----- Validation Batch -> 4 ----  Validation Loss -> 1.4349\n----- Time -> 21:24:03 ----- Validation Batch -> 5 ----  Validation Loss -> 1.5422\n----- Time -> 21:24:03 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.5226046\nloss: 1.466691255569458, time: 21:24:05, step: 1401\nloss: 1.3455170392990112, time: 21:24:10, step: 1402\nloss: 1.3427317142486572, time: 21:24:16, step: 1403\nloss: 1.472969651222229, time: 21:24:21, step: 1404\nloss: 1.498350977897644, time: 21:24:27, step: 1405\nloss: 1.5158278942108154, time: 21:24:32, step: 1406\nloss: 1.6275781393051147, time: 21:24:37, step: 1407\nloss: 1.5189393758773804, time: 21:24:43, step: 1408\nloss: 1.7410122156143188, time: 21:24:48, step: 1409\nloss: 1.3154288530349731, time: 21:24:54, step: 1410\nloss: 1.5021613836288452, time: 21:24:59, step: 1411\nloss: 1.4181265830993652, time: 21:25:05, step: 1412\nloss: 1.5169576406478882, time: 21:25:10, step: 1413\nloss: 1.430554747581482, time: 21:25:16, step: 1414\nloss: 1.4717971086502075, time: 21:25:21, step: 1415\nloss: 1.347946047782898, time: 21:25:27, step: 1416\nloss: 1.5966683626174927, time: 21:25:32, step: 1417\nloss: 1.658940315246582, time: 21:25:38, step: 1418\nloss: 1.7483662366867065, time: 21:25:43, step: 1419\nloss: 1.5045994520187378, time: 21:25:49, step: 1420\n----- Time -> 21:25:53 ----- Validation Batch -> 1 ----  Validation Loss -> 1.6935\n----- Time -> 21:25:54 ----- Validation Batch -> 2 ----  Validation Loss -> 1.5594\n----- Time -> 21:25:54 ----- Validation Batch -> 3 ----  Validation Loss -> 1.3474\n----- Time -> 21:25:55 ----- Validation Batch -> 4 ----  Validation Loss -> 1.4225\n----- Time -> 21:25:55 ----- Validation Batch -> 5 ----  Validation Loss -> 1.5261\n----- Time -> 21:25:55 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.5097889\nloss: 1.3828966617584229, time: 21:25:57, step: 1421\nloss: 1.417288899421692, time: 21:26:02, step: 1422\nloss: 1.6019208431243896, time: 21:26:08, step: 1423\nloss: 1.39784574508667, time: 21:26:13, step: 1424\nloss: 1.5047142505645752, time: 21:26:19, step: 1425\nloss: 1.5046099424362183, time: 21:26:24, step: 1426\nloss: 1.488593578338623, time: 21:26:30, step: 1427\nloss: 1.3832495212554932, time: 21:26:36, step: 1428\nloss: 1.528961181640625, time: 21:26:41, step: 1429\nloss: 1.5646432638168335, time: 21:26:46, step: 1430\nloss: 1.5269665718078613, time: 21:26:52, step: 1431\nloss: 1.4439506530761719, time: 21:26:57, step: 1432\nloss: 1.5740920305252075, time: 21:27:03, step: 1433\nloss: 1.411832571029663, time: 21:27:08, step: 1434\nloss: 1.6731704473495483, time: 21:27:14, step: 1435\nloss: 1.464358925819397, time: 21:27:19, step: 1436\nloss: 1.5060611963272095, time: 21:27:25, step: 1437\nloss: 1.5960556268692017, time: 21:27:30, step: 1438\nloss: 1.3245962858200073, time: 21:27:36, step: 1439\nloss: 1.689078688621521, time: 21:27:41, step: 1440\n----- Time -> 21:27:46 ----- Validation Batch -> 1 ----  Validation Loss -> 1.6798\n----- Time -> 21:27:46 ----- Validation Batch -> 2 ----  Validation Loss -> 1.5458\n----- Time -> 21:27:47 ----- Validation Batch -> 3 ----  Validation Loss -> 1.3355\n----- Time -> 21:27:47 ----- Validation Batch -> 4 ----  Validation Loss -> 1.4098\n----- Time -> 21:27:48 ----- Validation Batch -> 5 ----  Validation Loss -> 1.5121\n----- Time -> 21:27:48 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.4965935\nloss: 1.4822770357131958, time: 21:27:50, step: 1441\nloss: 1.4075167179107666, time: 21:27:55, step: 1442\nloss: 1.4129390716552734, time: 21:28:01, step: 1443\nloss: 1.6778305768966675, time: 21:28:07, step: 1444\nloss: 1.3444417715072632, time: 21:28:12, step: 1445\nloss: 1.5417147874832153, time: 21:28:18, step: 1446\nloss: 1.3841713666915894, time: 21:28:23, step: 1447\nloss: 1.6227192878723145, time: 21:28:29, step: 1448\nloss: 1.577426552772522, time: 21:28:34, step: 1449\nloss: 1.5286608934402466, time: 21:28:40, step: 1450\nloss: 1.642523169517517, time: 21:28:45, step: 1451\nloss: 1.3630177974700928, time: 21:28:51, step: 1452\nloss: 1.4021819829940796, time: 21:28:56, step: 1453\nloss: 1.5150195360183716, time: 21:29:02, step: 1454\nloss: 1.6804407835006714, time: 21:29:07, step: 1455\nloss: 1.47671377658844, time: 21:29:13, step: 1456\nloss: 1.5941168069839478, time: 21:29:18, step: 1457\nloss: 1.4749431610107422, time: 21:29:24, step: 1458\nloss: 1.6279194355010986, time: 21:29:29, step: 1459\nloss: 1.6086418628692627, time: 21:29:35, step: 1460\n----- Time -> 21:29:39 ----- Validation Batch -> 1 ----  Validation Loss -> 1.6617\n----- Time -> 21:29:40 ----- Validation Batch -> 2 ----  Validation Loss -> 1.5274\n----- Time -> 21:29:40 ----- Validation Batch -> 3 ----  Validation Loss -> 1.3219\n----- Time -> 21:29:41 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3941\n----- Time -> 21:29:41 ----- Validation Batch -> 5 ----  Validation Loss -> 1.4951\n----- Time -> 21:29:41 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.4800346\nloss: 1.3626012802124023, time: 21:29:43, step: 1461\nloss: 1.536521315574646, time: 21:29:48, step: 1462\nloss: 1.3229645490646362, time: 21:29:54, step: 1463\nloss: 1.4606057405471802, time: 21:29:59, step: 1464\nloss: 1.3321820497512817, time: 21:30:05, step: 1465\nloss: 1.595180869102478, time: 21:30:10, step: 1466\nloss: 1.5056015253067017, time: 21:30:16, step: 1467\nloss: 1.3806169033050537, time: 21:30:21, step: 1468\nloss: 1.3780624866485596, time: 21:30:27, step: 1469\nloss: 1.4692604541778564, time: 21:30:32, step: 1470\nloss: 1.4973015785217285, time: 21:30:38, step: 1471\nloss: 1.4926321506500244, time: 21:30:43, step: 1472\nloss: 1.4560433626174927, time: 21:30:49, step: 1473\nloss: 1.4153742790222168, time: 21:30:54, step: 1474\nloss: 1.3521904945373535, time: 21:31:00, step: 1475\nloss: 1.447106957435608, time: 21:31:05, step: 1476\nloss: 1.4104949235916138, time: 21:31:11, step: 1477\nloss: 1.6190776824951172, time: 21:31:16, step: 1478\nloss: 1.479799747467041, time: 21:31:22, step: 1479\nloss: 1.3845257759094238, time: 21:31:27, step: 1480\n----- Time -> 21:31:32 ----- Validation Batch -> 1 ----  Validation Loss -> 1.6462\n----- Time -> 21:31:32 ----- Validation Batch -> 2 ----  Validation Loss -> 1.5106\n----- Time -> 21:31:33 ----- Validation Batch -> 3 ----  Validation Loss -> 1.3050\n----- Time -> 21:31:33 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3761\n----- Time -> 21:31:34 ----- Validation Batch -> 5 ----  Validation Loss -> 1.4771\n----- Time -> 21:31:34 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.4630021\nloss: 1.327000617980957, time: 21:31:36, step: 1481\nloss: 1.4163072109222412, time: 21:31:41, step: 1482\nloss: 1.5103744268417358, time: 21:31:47, step: 1483\nloss: 1.6245110034942627, time: 21:31:52, step: 1484\nloss: 1.508378028869629, time: 21:31:58, step: 1485\nloss: 1.3876372575759888, time: 21:32:03, step: 1486\nloss: 1.4575917720794678, time: 21:32:09, step: 1487\nloss: 1.3675603866577148, time: 21:32:14, step: 1488\nloss: 1.5452821254730225, time: 21:32:20, step: 1489\nloss: 1.318621277809143, time: 21:32:25, step: 1490\nloss: 1.269981026649475, time: 21:32:30, step: 1491\nloss: 1.5598615407943726, time: 21:32:36, step: 1492\nloss: 1.4138529300689697, time: 21:32:42, step: 1493\nloss: 1.4455103874206543, time: 21:32:47, step: 1494\nloss: 1.5457173585891724, time: 21:32:53, step: 1495\nloss: 1.3877378702163696, time: 21:32:58, step: 1496\nloss: 1.3854491710662842, time: 21:33:03, step: 1497\nloss: 1.3872631788253784, time: 21:33:09, step: 1498\nloss: 1.484635353088379, time: 21:33:14, step: 1499\nloss: 1.3490391969680786, time: 21:33:20, step: 1500\n----- Time -> 21:33:24 ----- Validation Batch -> 1 ----  Validation Loss -> 1.6288\n----- Time -> 21:33:25 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4905\n----- Time -> 21:33:26 ----- Validation Batch -> 3 ----  Validation Loss -> 1.2885\n----- Time -> 21:33:26 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3577\n----- Time -> 21:33:27 ----- Validation Batch -> 5 ----  Validation Loss -> 1.4561\n----- Time -> 21:33:27 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.4443298\nloss: 1.4190022945404053, time: 21:33:28, step: 1501\nloss: 1.3731427192687988, time: 21:33:34, step: 1502\nloss: 1.3614290952682495, time: 21:33:39, step: 1503\nloss: 1.343424677848816, time: 21:33:45, step: 1504\nloss: 1.477192759513855, time: 21:33:50, step: 1505\nloss: 1.447716236114502, time: 21:33:56, step: 1506\nloss: 1.2822903394699097, time: 21:34:01, step: 1507\nloss: 1.3016515970230103, time: 21:34:07, step: 1508\nloss: 1.4531078338623047, time: 21:34:12, step: 1509\nloss: 1.5006722211837769, time: 21:34:18, step: 1510\nloss: 1.2869946956634521, time: 21:34:23, step: 1511\nloss: 1.520563006401062, time: 21:34:29, step: 1512\nloss: 1.253475308418274, time: 21:34:34, step: 1513\nloss: 1.4239743947982788, time: 21:34:40, step: 1514\nloss: 1.4593114852905273, time: 21:34:45, step: 1515\nloss: 1.413362741470337, time: 21:34:51, step: 1516\nloss: 1.482040286064148, time: 21:34:56, step: 1517\nloss: 1.5338431596755981, time: 21:35:02, step: 1518\nloss: 1.242508053779602, time: 21:35:07, step: 1519\nloss: 1.5018473863601685, time: 21:35:12, step: 1520\n----- Time -> 21:35:17 ----- Validation Batch -> 1 ----  Validation Loss -> 1.6061\n----- Time -> 21:35:17 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4675\n----- Time -> 21:35:18 ----- Validation Batch -> 3 ----  Validation Loss -> 1.2696\n----- Time -> 21:35:19 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3371\n----- Time -> 21:35:19 ----- Validation Batch -> 5 ----  Validation Loss -> 1.4308\n----- Time -> 21:35:19 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.4222184\nloss: 1.4449185132980347, time: 21:35:21, step: 1521\nloss: 1.3771452903747559, time: 21:35:26, step: 1522\nloss: 1.293843150138855, time: 21:35:32, step: 1523\nloss: 1.4618349075317383, time: 21:35:37, step: 1524\nloss: 1.2928320169448853, time: 21:35:43, step: 1525\nloss: 1.3748422861099243, time: 21:35:48, step: 1526\nloss: 1.4058758020401, time: 21:35:54, step: 1527\nloss: 1.4905791282653809, time: 21:35:59, step: 1528\nloss: 1.579344391822815, time: 21:36:05, step: 1529\nloss: 1.3057365417480469, time: 21:36:10, step: 1530\nloss: 1.406562089920044, time: 21:36:16, step: 1531\nloss: 1.6170921325683594, time: 21:36:21, step: 1532\nloss: 1.3881021738052368, time: 21:36:27, step: 1533\nloss: 1.3905720710754395, time: 21:36:32, step: 1534\nloss: 1.5530407428741455, time: 21:36:38, step: 1535\nloss: 1.356853723526001, time: 21:36:43, step: 1536\nloss: 1.463294267654419, time: 21:36:49, step: 1537\nloss: 1.3785088062286377, time: 21:36:54, step: 1538\nloss: 1.2993372678756714, time: 21:37:00, step: 1539\nloss: 1.3304214477539062, time: 21:37:05, step: 1540\n----- Time -> 21:37:09 ----- Validation Batch -> 1 ----  Validation Loss -> 1.5820\n----- Time -> 21:37:10 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4434\n----- Time -> 21:37:11 ----- Validation Batch -> 3 ----  Validation Loss -> 1.2501\n----- Time -> 21:37:11 ----- Validation Batch -> 4 ----  Validation Loss -> 1.3152\n----- Time -> 21:37:12 ----- Validation Batch -> 5 ----  Validation Loss -> 1.4079\n----- Time -> 21:37:12 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.3997143\nloss: 1.3595941066741943, time: 21:37:13, step: 1541\nloss: 1.4256998300552368, time: 21:37:19, step: 1542\nloss: 1.4470562934875488, time: 21:37:24, step: 1543\nloss: 1.4468469619750977, time: 21:37:30, step: 1544\nloss: 1.3014427423477173, time: 21:37:35, step: 1545\nloss: 1.3207868337631226, time: 21:37:41, step: 1546\nloss: 1.2368203401565552, time: 21:37:46, step: 1547\nloss: 1.2726185321807861, time: 21:37:52, step: 1548\nloss: 1.4734183549880981, time: 21:37:57, step: 1549\nloss: 1.3939385414123535, time: 21:38:03, step: 1550\nloss: 1.3806438446044922, time: 21:38:08, step: 1551\nloss: 1.3446406126022339, time: 21:38:14, step: 1552\nloss: 1.4796909093856812, time: 21:38:19, step: 1553\nloss: 1.5160253047943115, time: 21:38:25, step: 1554\nloss: 1.3463085889816284, time: 21:38:30, step: 1555\nloss: 1.64120614528656, time: 21:38:36, step: 1556\nloss: 1.2836970090866089, time: 21:38:41, step: 1557\nloss: 1.2306982278823853, time: 21:38:47, step: 1558\nloss: 1.462730884552002, time: 21:38:52, step: 1559\nloss: 1.4868680238723755, time: 21:38:58, step: 1560\n----- Time -> 21:39:02 ----- Validation Batch -> 1 ----  Validation Loss -> 1.5638\n----- Time -> 21:39:03 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4239\n----- Time -> 21:39:03 ----- Validation Batch -> 3 ----  Validation Loss -> 1.2362\n----- Time -> 21:39:04 ----- Validation Batch -> 4 ----  Validation Loss -> 1.2957\n----- Time -> 21:39:04 ----- Validation Batch -> 5 ----  Validation Loss -> 1.3870\n----- Time -> 21:39:04 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.3812988\nloss: 1.486738920211792, time: 21:39:06, step: 1561\nloss: 1.4245113134384155, time: 21:39:11, step: 1562\nloss: 1.2916898727416992, time: 21:39:17, step: 1563\nloss: 1.411015510559082, time: 21:39:22, step: 1564\nloss: 1.2865896224975586, time: 21:39:28, step: 1565\nloss: 1.1887578964233398, time: 21:39:33, step: 1566\nloss: 1.3102596998214722, time: 21:39:39, step: 1567\nloss: 1.272693395614624, time: 21:39:44, step: 1568\nloss: 1.3990302085876465, time: 21:39:50, step: 1569\nloss: 1.3672828674316406, time: 21:39:55, step: 1570\nloss: 1.3848341703414917, time: 21:40:01, step: 1571\nloss: 1.4186662435531616, time: 21:40:06, step: 1572\nloss: 1.3392709493637085, time: 21:40:12, step: 1573\nloss: 1.3029483556747437, time: 21:40:17, step: 1574\nloss: 1.160988688468933, time: 21:40:23, step: 1575\nloss: 1.1992624998092651, time: 21:40:28, step: 1576\nloss: 1.282428503036499, time: 21:40:34, step: 1577\nloss: 1.3409762382507324, time: 21:40:39, step: 1578\nloss: 1.3706837892532349, time: 21:40:45, step: 1579\nloss: 1.2743393182754517, time: 21:40:50, step: 1580\n----- Time -> 21:40:55 ----- Validation Batch -> 1 ----  Validation Loss -> 1.5513\n----- Time -> 21:40:55 ----- Validation Batch -> 2 ----  Validation Loss -> 1.4085\n----- Time -> 21:40:56 ----- Validation Batch -> 3 ----  Validation Loss -> 1.2250\n----- Time -> 21:40:56 ----- Validation Batch -> 4 ----  Validation Loss -> 1.2805\n----- Time -> 21:40:57 ----- Validation Batch -> 5 ----  Validation Loss -> 1.3688\n----- Time -> 21:40:57 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.3668221\nloss: 1.3318910598754883, time: 21:40:58, step: 1581\nloss: 1.3881244659423828, time: 21:41:04, step: 1582\nloss: 1.352041244506836, time: 21:41:09, step: 1583\nloss: 1.296085238456726, time: 21:41:15, step: 1584\nloss: 1.3099220991134644, time: 21:41:20, step: 1585\nloss: 1.2886478900909424, time: 21:41:26, step: 1586\nloss: 1.318811297416687, time: 21:41:31, step: 1587\nloss: 1.3756499290466309, time: 21:41:37, step: 1588\nloss: 1.6025892496109009, time: 21:41:42, step: 1589\nloss: 1.212050199508667, time: 21:41:48, step: 1590\nloss: 1.2664294242858887, time: 21:41:53, step: 1591\nloss: 1.2772141695022583, time: 21:41:59, step: 1592\nloss: 1.3426214456558228, time: 21:42:04, step: 1593\nloss: 1.284762978553772, time: 21:42:10, step: 1594\nloss: 1.4610515832901, time: 21:42:15, step: 1595\nloss: 1.2723596096038818, time: 21:42:21, step: 1596\nloss: 1.3048088550567627, time: 21:42:26, step: 1597\nloss: 1.359124779701233, time: 21:42:32, step: 1598\nloss: 1.3217933177947998, time: 21:42:37, step: 1599\nloss: 1.280978798866272, time: 21:42:43, step: 1600\n----- Time -> 21:42:47 ----- Validation Batch -> 1 ----  Validation Loss -> 1.5380\n----- Time -> 21:42:48 ----- Validation Batch -> 2 ----  Validation Loss -> 1.3947\n----- Time -> 21:42:48 ----- Validation Batch -> 3 ----  Validation Loss -> 1.2142\n----- Time -> 21:42:49 ----- Validation Batch -> 4 ----  Validation Loss -> 1.2674\n----- Time -> 21:42:49 ----- Validation Batch -> 5 ----  Validation Loss -> 1.3522\n----- Time -> 21:42:49 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.3533100\nloss: 1.43180251121521, time: 21:42:51, step: 1601\nloss: 1.270211935043335, time: 21:42:56, step: 1602\nloss: 1.415998101234436, time: 21:43:02, step: 1603\nloss: 1.4467744827270508, time: 21:43:07, step: 1604\nloss: 1.2861193418502808, time: 21:43:13, step: 1605\nloss: 1.2587567567825317, time: 21:43:18, step: 1606\nloss: 1.2609848976135254, time: 21:43:24, step: 1607\nloss: 1.4762094020843506, time: 21:43:29, step: 1608\nloss: 1.3061017990112305, time: 21:43:35, step: 1609\nloss: 1.3466488122940063, time: 21:43:40, step: 1610\nloss: 1.410990595817566, time: 21:43:46, step: 1611\nloss: 1.261061668395996, time: 21:43:51, step: 1612\nloss: 1.4433568716049194, time: 21:43:57, step: 1613\nloss: 1.2899819612503052, time: 21:44:02, step: 1614\nloss: 1.2731682062149048, time: 21:44:08, step: 1615\nloss: 1.434130072593689, time: 21:44:13, step: 1616\nloss: 1.2868249416351318, time: 21:44:19, step: 1617\nloss: 1.2736605405807495, time: 21:44:24, step: 1618\nloss: 1.3380799293518066, time: 21:44:30, step: 1619\nloss: 1.4200719594955444, time: 21:44:35, step: 1620\n----- Time -> 21:44:40 ----- Validation Batch -> 1 ----  Validation Loss -> 1.5244\n----- Time -> 21:44:40 ----- Validation Batch -> 2 ----  Validation Loss -> 1.3820\n----- Time -> 21:44:41 ----- Validation Batch -> 3 ----  Validation Loss -> 1.2022\n----- Time -> 21:44:41 ----- Validation Batch -> 4 ----  Validation Loss -> 1.2525\n----- Time -> 21:44:42 ----- Validation Batch -> 5 ----  Validation Loss -> 1.3395\n----- Time -> 21:44:42 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.3401233\nloss: 1.2595652341842651, time: 21:44:44, step: 1621\nloss: 1.3539786338806152, time: 21:44:49, step: 1622\nloss: 1.3587738275527954, time: 21:44:55, step: 1623\nloss: 1.387756586074829, time: 21:45:00, step: 1624\nloss: 1.2434369325637817, time: 21:45:05, step: 1625\nloss: 1.2698620557785034, time: 21:45:11, step: 1626\nloss: 1.3842155933380127, time: 21:45:16, step: 1627\nloss: 1.2015236616134644, time: 21:45:22, step: 1628\nloss: 1.4687223434448242, time: 21:45:27, step: 1629\nloss: 1.1696826219558716, time: 21:45:33, step: 1630\nloss: 1.3317028284072876, time: 21:45:38, step: 1631\nloss: 1.1720736026763916, time: 21:45:44, step: 1632\nloss: 1.250651240348816, time: 21:45:49, step: 1633\nloss: 1.20119309425354, time: 21:45:55, step: 1634\nloss: 1.439306616783142, time: 21:46:00, step: 1635\nloss: 1.474871039390564, time: 21:46:06, step: 1636\nloss: 1.3388798236846924, time: 21:46:11, step: 1637\nloss: 1.3541263341903687, time: 21:46:17, step: 1638\nloss: 1.3596789836883545, time: 21:46:22, step: 1639\nloss: 1.197543978691101, time: 21:46:28, step: 1640\n----- Time -> 21:46:32 ----- Validation Batch -> 1 ----  Validation Loss -> 1.5094\n----- Time -> 21:46:33 ----- Validation Batch -> 2 ----  Validation Loss -> 1.3671\n----- Time -> 21:46:33 ----- Validation Batch -> 3 ----  Validation Loss -> 1.1923\n----- Time -> 21:46:34 ----- Validation Batch -> 4 ----  Validation Loss -> 1.2398\n----- Time -> 21:46:35 ----- Validation Batch -> 5 ----  Validation Loss -> 1.3226\n----- Time -> 21:46:35 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.3262440\nloss: 1.3897700309753418, time: 21:46:36, step: 1641\nloss: 1.183935523033142, time: 21:46:42, step: 1642\nloss: 1.399023175239563, time: 21:46:47, step: 1643\nloss: 1.3156933784484863, time: 21:46:53, step: 1644\nloss: 1.217434048652649, time: 21:46:58, step: 1645\nloss: 1.313602089881897, time: 21:47:04, step: 1646\nloss: 1.4701173305511475, time: 21:47:09, step: 1647\nloss: 1.3784488439559937, time: 21:47:15, step: 1648\nloss: 1.591982126235962, time: 21:47:20, step: 1649\nloss: 1.2008922100067139, time: 21:47:26, step: 1650\nloss: 1.3365895748138428, time: 21:47:31, step: 1651\nloss: 1.203925609588623, time: 21:47:37, step: 1652\nloss: 1.459694743156433, time: 21:47:42, step: 1653\nloss: 1.442538857460022, time: 21:47:48, step: 1654\nloss: 1.373138427734375, time: 21:47:53, step: 1655\nloss: 1.1953082084655762, time: 21:47:59, step: 1656\nloss: 1.3063201904296875, time: 21:48:04, step: 1657\nloss: 1.3086223602294922, time: 21:48:10, step: 1658\nloss: 1.1973583698272705, time: 21:48:15, step: 1659\nloss: 1.3526611328125, time: 21:48:21, step: 1660\n----- Time -> 21:48:25 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4957\n----- Time -> 21:48:26 ----- Validation Batch -> 2 ----  Validation Loss -> 1.3538\n----- Time -> 21:48:26 ----- Validation Batch -> 3 ----  Validation Loss -> 1.1811\n----- Time -> 21:48:27 ----- Validation Batch -> 4 ----  Validation Loss -> 1.2255\n----- Time -> 21:48:27 ----- Validation Batch -> 5 ----  Validation Loss -> 1.3123\n----- Time -> 21:48:27 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.3136853\nloss: 1.357847809791565, time: 21:48:29, step: 1661\nloss: 1.2859160900115967, time: 21:48:34, step: 1662\nloss: 1.1933151483535767, time: 21:48:40, step: 1663\nloss: 1.4920308589935303, time: 21:48:45, step: 1664\nloss: 1.3053083419799805, time: 21:48:51, step: 1665\nloss: 1.4643195867538452, time: 21:48:56, step: 1666\nloss: 1.1431903839111328, time: 21:49:02, step: 1667\nloss: 1.3011374473571777, time: 21:49:07, step: 1668\nloss: 1.2713027000427246, time: 21:49:13, step: 1669\nloss: 1.2020822763442993, time: 21:49:18, step: 1670\nloss: 1.313042402267456, time: 21:49:24, step: 1671\nloss: 1.227418303489685, time: 21:49:29, step: 1672\nloss: 1.2049365043640137, time: 21:49:35, step: 1673\nloss: 1.244638442993164, time: 21:49:40, step: 1674\nloss: 1.210721731185913, time: 21:49:46, step: 1675\nloss: 1.1956541538238525, time: 21:49:51, step: 1676\nloss: 1.2617803812026978, time: 21:49:57, step: 1677\nloss: 1.2576038837432861, time: 21:50:02, step: 1678\nloss: 1.1360353231430054, time: 21:50:08, step: 1679\nloss: 1.2867096662521362, time: 21:50:13, step: 1680\n----- Time -> 21:50:18 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4825\n----- Time -> 21:50:18 ----- Validation Batch -> 2 ----  Validation Loss -> 1.3382\n----- Time -> 21:50:19 ----- Validation Batch -> 3 ----  Validation Loss -> 1.1713\n----- Time -> 21:50:19 ----- Validation Batch -> 4 ----  Validation Loss -> 1.2110\n----- Time -> 21:50:20 ----- Validation Batch -> 5 ----  Validation Loss -> 1.2948\n----- Time -> 21:50:20 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.2995550\nloss: 1.2389260530471802, time: 21:50:21, step: 1681\nloss: 1.181878924369812, time: 21:50:27, step: 1682\nloss: 1.2447946071624756, time: 21:50:32, step: 1683\nloss: 1.3509643077850342, time: 21:50:38, step: 1684\nloss: 1.1666687726974487, time: 21:50:43, step: 1685\nloss: 1.3321444988250732, time: 21:50:49, step: 1686\nloss: 1.3983477354049683, time: 21:50:54, step: 1687\nloss: 1.2081444263458252, time: 21:51:00, step: 1688\nloss: 1.1862338781356812, time: 21:51:05, step: 1689\nloss: 1.1465736627578735, time: 21:51:11, step: 1690\nloss: 1.5058928728103638, time: 21:51:16, step: 1691\nloss: 1.2530136108398438, time: 21:51:22, step: 1692\nloss: 1.1756154298782349, time: 21:51:27, step: 1693\nloss: 1.205457329750061, time: 21:51:33, step: 1694\nloss: 1.2552460432052612, time: 21:51:38, step: 1695\nloss: 1.2156317234039307, time: 21:51:44, step: 1696\nloss: 1.281021237373352, time: 21:51:49, step: 1697\nloss: 1.3202179670333862, time: 21:51:55, step: 1698\nloss: 1.2867344617843628, time: 21:52:00, step: 1699\nloss: 1.2695400714874268, time: 21:52:06, step: 1700\n----- Time -> 21:52:10 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4691\n----- Time -> 21:52:11 ----- Validation Batch -> 2 ----  Validation Loss -> 1.3237\n----- Time -> 21:52:12 ----- Validation Batch -> 3 ----  Validation Loss -> 1.1595\n----- Time -> 21:52:12 ----- Validation Batch -> 4 ----  Validation Loss -> 1.1966\n----- Time -> 21:52:13 ----- Validation Batch -> 5 ----  Validation Loss -> 1.2789\n----- Time -> 21:52:13 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.2855708\nloss: 1.1271699666976929, time: 21:52:14, step: 1701\nloss: 1.2214627265930176, time: 21:52:20, step: 1702\nloss: 1.2180482149124146, time: 21:52:25, step: 1703\nloss: 1.1615115404129028, time: 21:52:31, step: 1704\nloss: 1.3458075523376465, time: 21:52:36, step: 1705\nloss: 1.1620503664016724, time: 21:52:42, step: 1706\nloss: 1.190943956375122, time: 21:52:47, step: 1707\nloss: 1.3473949432373047, time: 21:52:53, step: 1708\nloss: 1.19312584400177, time: 21:52:58, step: 1709\nloss: 1.2482694387435913, time: 21:53:04, step: 1710\nloss: 1.2026870250701904, time: 21:53:09, step: 1711\nloss: 1.279167652130127, time: 21:53:15, step: 1712\nloss: 1.6640238761901855, time: 21:53:20, step: 1713\nloss: 1.2496559619903564, time: 21:53:26, step: 1714\nloss: 1.0334117412567139, time: 21:53:31, step: 1715\nloss: 1.2165859937667847, time: 21:53:37, step: 1716\nloss: 1.4859986305236816, time: 21:53:42, step: 1717\nloss: 1.1814639568328857, time: 21:53:48, step: 1718\nloss: 1.1575298309326172, time: 21:53:53, step: 1719\nloss: 1.1937055587768555, time: 21:53:59, step: 1720\n----- Time -> 21:54:03 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4577\n----- Time -> 21:54:04 ----- Validation Batch -> 2 ----  Validation Loss -> 1.3092\n----- Time -> 21:54:04 ----- Validation Batch -> 3 ----  Validation Loss -> 1.1483\n----- Time -> 21:54:05 ----- Validation Batch -> 4 ----  Validation Loss -> 1.1829\n----- Time -> 21:54:05 ----- Validation Batch -> 5 ----  Validation Loss -> 1.2642\n----- Time -> 21:54:05 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.2724858\nloss: 1.1997615098953247, time: 21:54:07, step: 1721\nloss: 1.4339479207992554, time: 21:54:12, step: 1722\nloss: 1.10963773727417, time: 21:54:18, step: 1723\nloss: 1.1745582818984985, time: 21:54:23, step: 1724\nloss: 1.2776331901550293, time: 21:54:29, step: 1725\nloss: 1.3124853372573853, time: 21:54:34, step: 1726\nloss: 1.1476260423660278, time: 21:54:40, step: 1727\nloss: 1.1771060228347778, time: 21:54:45, step: 1728\nloss: 1.2580565214157104, time: 21:54:51, step: 1729\nloss: 1.2605204582214355, time: 21:54:56, step: 1730\nloss: 1.2152119874954224, time: 21:55:02, step: 1731\nloss: 1.29313325881958, time: 21:55:07, step: 1732\nloss: 1.2279682159423828, time: 21:55:13, step: 1733\nloss: 1.1693428754806519, time: 21:55:18, step: 1734\nloss: 1.315306544303894, time: 21:55:24, step: 1735\nloss: 1.1717197895050049, time: 21:55:30, step: 1736\nloss: 1.1691961288452148, time: 21:55:35, step: 1737\nloss: 1.200268030166626, time: 21:55:41, step: 1738\nloss: 1.2657859325408936, time: 21:55:46, step: 1739\nloss: 1.1945629119873047, time: 21:55:52, step: 1740\n----- Time -> 21:55:56 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4419\n----- Time -> 21:55:57 ----- Validation Batch -> 2 ----  Validation Loss -> 1.2928\n----- Time -> 21:55:57 ----- Validation Batch -> 3 ----  Validation Loss -> 1.1365\n----- Time -> 21:55:58 ----- Validation Batch -> 4 ----  Validation Loss -> 1.1668\n----- Time -> 21:55:58 ----- Validation Batch -> 5 ----  Validation Loss -> 1.2475\n----- Time -> 21:55:58 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.2570853\nloss: 1.117271900177002, time: 21:56:00, step: 1741\nloss: 1.1686289310455322, time: 21:56:05, step: 1742\nloss: 1.1936064958572388, time: 21:56:11, step: 1743\nloss: 1.2612859010696411, time: 21:56:16, step: 1744\nloss: 1.2344791889190674, time: 21:56:22, step: 1745\nloss: 1.2211709022521973, time: 21:56:27, step: 1746\nloss: 1.2017074823379517, time: 21:56:33, step: 1747\nloss: 1.2979282140731812, time: 21:56:38, step: 1748\nloss: 1.2484946250915527, time: 21:56:44, step: 1749\nloss: 1.3056318759918213, time: 21:56:49, step: 1750\nloss: 1.2383123636245728, time: 21:56:55, step: 1751\nloss: 1.4766796827316284, time: 21:57:00, step: 1752\nloss: 1.104326605796814, time: 21:57:06, step: 1753\nloss: 1.142335057258606, time: 21:57:11, step: 1754\nloss: 1.2880455255508423, time: 21:57:17, step: 1755\nloss: 1.1401889324188232, time: 21:57:22, step: 1756\nloss: 1.2199195623397827, time: 21:57:28, step: 1757\nloss: 1.3691489696502686, time: 21:57:33, step: 1758\nloss: 1.2225635051727295, time: 21:57:39, step: 1759\nloss: 1.2089686393737793, time: 21:57:44, step: 1760\n----- Time -> 21:57:49 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4276\n----- Time -> 21:57:49 ----- Validation Batch -> 2 ----  Validation Loss -> 1.2765\n----- Time -> 21:57:50 ----- Validation Batch -> 3 ----  Validation Loss -> 1.1256\n----- Time -> 21:57:51 ----- Validation Batch -> 4 ----  Validation Loss -> 1.1493\n----- Time -> 21:57:51 ----- Validation Batch -> 5 ----  Validation Loss -> 1.2308\n----- Time -> 21:57:51 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.2419596\nloss: 1.1367193460464478, time: 21:57:53, step: 1761\nloss: 1.1448488235473633, time: 21:57:58, step: 1762\nloss: 1.2575865983963013, time: 21:58:04, step: 1763\nloss: 1.1619454622268677, time: 21:58:09, step: 1764\nloss: 1.1855823993682861, time: 21:58:15, step: 1765\nloss: 1.3810802698135376, time: 21:58:20, step: 1766\nloss: 1.1872339248657227, time: 21:58:26, step: 1767\nloss: 1.1024876832962036, time: 21:58:31, step: 1768\nloss: 1.1139874458312988, time: 21:58:37, step: 1769\nloss: 1.3090531826019287, time: 21:58:42, step: 1770\nloss: 1.2734148502349854, time: 21:58:48, step: 1771\nloss: 1.1803165674209595, time: 21:58:53, step: 1772\nloss: 1.2228589057922363, time: 21:58:59, step: 1773\nloss: 1.1206204891204834, time: 21:59:04, step: 1774\nloss: 1.3120957612991333, time: 21:59:10, step: 1775\nloss: 1.080716609954834, time: 21:59:15, step: 1776\nloss: 1.191750407218933, time: 21:59:21, step: 1777\nloss: 1.1856637001037598, time: 21:59:26, step: 1778\nloss: 1.0646806955337524, time: 21:59:32, step: 1779\nloss: 1.1670154333114624, time: 21:59:37, step: 1780\n----- Time -> 21:59:42 ----- Validation Batch -> 1 ----  Validation Loss -> 1.4158\n----- Time -> 21:59:42 ----- Validation Batch -> 2 ----  Validation Loss -> 1.2611\n----- Time -> 21:59:43 ----- Validation Batch -> 3 ----  Validation Loss -> 1.1113\n----- Time -> 21:59:43 ----- Validation Batch -> 4 ----  Validation Loss -> 1.1348\n----- Time -> 21:59:44 ----- Validation Batch -> 5 ----  Validation Loss -> 1.2145\n----- Time -> 21:59:44 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.2274876\nloss: 1.1383689641952515, time: 21:59:45, step: 1781\nloss: 1.2020100355148315, time: 21:59:51, step: 1782\nloss: 1.277219533920288, time: 21:59:56, step: 1783\nloss: 1.1118890047073364, time: 22:00:02, step: 1784\nloss: 1.2344528436660767, time: 22:00:07, step: 1785\nloss: 1.1621758937835693, time: 22:00:13, step: 1786\nloss: 1.2028995752334595, time: 22:00:18, step: 1787\nloss: 1.1886374950408936, time: 22:00:24, step: 1788\nloss: 1.2163015604019165, time: 22:00:29, step: 1789\nloss: 1.3548057079315186, time: 22:00:35, step: 1790\nloss: 1.2271126508712769, time: 22:00:40, step: 1791\nloss: 1.0705233812332153, time: 22:00:46, step: 1792\nloss: 1.3667423725128174, time: 22:00:51, step: 1793\nloss: 1.1370888948440552, time: 22:00:57, step: 1794\nloss: 1.0437966585159302, time: 22:01:02, step: 1795\nloss: 1.283198595046997, time: 22:01:08, step: 1796\nloss: 1.2804893255233765, time: 22:01:13, step: 1797\nloss: 1.2431014776229858, time: 22:01:19, step: 1798\nloss: 1.219480276107788, time: 22:01:24, step: 1799\nloss: 1.2671287059783936, time: 22:01:30, step: 1800\n----- Time -> 22:01:34 ----- Validation Batch -> 1 ----  Validation Loss -> 1.3987\n----- Time -> 22:01:35 ----- Validation Batch -> 2 ----  Validation Loss -> 1.2452\n----- Time -> 22:01:35 ----- Validation Batch -> 3 ----  Validation Loss -> 1.0964\n----- Time -> 22:01:36 ----- Validation Batch -> 4 ----  Validation Loss -> 1.1179\n----- Time -> 22:01:36 ----- Validation Batch -> 5 ----  Validation Loss -> 1.1961\n----- Time -> 22:01:36 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.2108577\nloss: 1.1116045713424683, time: 22:01:38, step: 1801\nloss: 1.1167914867401123, time: 22:01:44, step: 1802\nloss: 1.1113028526306152, time: 22:01:49, step: 1803\nloss: 1.156501293182373, time: 22:01:54, step: 1804\nloss: 1.3658173084259033, time: 22:02:00, step: 1805\nloss: 1.2404497861862183, time: 22:02:05, step: 1806\nloss: 1.1319375038146973, time: 22:02:11, step: 1807\nloss: 1.230934977531433, time: 22:02:16, step: 1808\nloss: 1.1202694177627563, time: 22:02:22, step: 1809\nloss: 1.3623192310333252, time: 22:02:27, step: 1810\nloss: 1.2615537643432617, time: 22:02:33, step: 1811\nloss: 1.1718015670776367, time: 22:02:38, step: 1812\nloss: 1.223612666130066, time: 22:02:44, step: 1813\nloss: 1.0941390991210938, time: 22:02:49, step: 1814\nloss: 1.1975250244140625, time: 22:02:55, step: 1815\nloss: 1.110241413116455, time: 22:03:00, step: 1816\nloss: 1.2752405405044556, time: 22:03:06, step: 1817\nloss: 1.304713249206543, time: 22:03:11, step: 1818\nloss: 1.0661265850067139, time: 22:03:17, step: 1819\nloss: 1.1770662069320679, time: 22:03:22, step: 1820\n----- Time -> 22:03:27 ----- Validation Batch -> 1 ----  Validation Loss -> 1.3810\n----- Time -> 22:03:27 ----- Validation Batch -> 2 ----  Validation Loss -> 1.2242\n----- Time -> 22:03:28 ----- Validation Batch -> 3 ----  Validation Loss -> 1.0844\n----- Time -> 22:03:28 ----- Validation Batch -> 4 ----  Validation Loss -> 1.0997\n----- Time -> 22:03:29 ----- Validation Batch -> 5 ----  Validation Loss -> 1.1745\n----- Time -> 22:03:29 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.1927319\nloss: 1.1316611766815186, time: 22:03:30, step: 1821\nloss: 1.1362950801849365, time: 22:03:36, step: 1822\nloss: 1.2672488689422607, time: 22:03:41, step: 1823\nloss: 1.165695071220398, time: 22:03:47, step: 1824\nloss: 1.2466124296188354, time: 22:03:52, step: 1825\nloss: 1.0414540767669678, time: 22:03:58, step: 1826\nloss: 1.1638948917388916, time: 22:04:03, step: 1827\nloss: 1.5167824029922485, time: 22:04:09, step: 1828\nloss: 1.1267811059951782, time: 22:04:14, step: 1829\nloss: 1.111385464668274, time: 22:04:20, step: 1830\nloss: 1.2220290899276733, time: 22:04:25, step: 1831\nloss: 1.0835357904434204, time: 22:04:31, step: 1832\nloss: 1.2919062376022339, time: 22:04:36, step: 1833\nloss: 1.1907694339752197, time: 22:04:42, step: 1834\nloss: 1.2118602991104126, time: 22:04:47, step: 1835\nloss: 1.0654504299163818, time: 22:04:53, step: 1836\nloss: 1.5373542308807373, time: 22:04:58, step: 1837\nloss: 1.3641688823699951, time: 22:05:04, step: 1838\nloss: 1.1147915124893188, time: 22:05:09, step: 1839\nloss: 1.2803090810775757, time: 22:05:15, step: 1840\n----- Time -> 22:05:19 ----- Validation Batch -> 1 ----  Validation Loss -> 1.3626\n----- Time -> 22:05:20 ----- Validation Batch -> 2 ----  Validation Loss -> 1.2027\n----- Time -> 22:05:20 ----- Validation Batch -> 3 ----  Validation Loss -> 1.0683\n----- Time -> 22:05:21 ----- Validation Batch -> 4 ----  Validation Loss -> 1.0802\n----- Time -> 22:05:21 ----- Validation Batch -> 5 ----  Validation Loss -> 1.1528\n----- Time -> 22:05:21 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.1733089\nloss: 1.1057064533233643, time: 22:05:23, step: 1841\nloss: 1.083791971206665, time: 22:05:29, step: 1842\nloss: 1.3095225095748901, time: 22:05:34, step: 1843\nloss: 1.1435414552688599, time: 22:05:40, step: 1844\nloss: 1.1708459854125977, time: 22:05:45, step: 1845\nloss: 1.180046796798706, time: 22:05:51, step: 1846\nloss: 1.154489517211914, time: 22:05:56, step: 1847\nloss: 1.0984505414962769, time: 22:06:01, step: 1848\nloss: 1.1691392660140991, time: 22:06:07, step: 1849\nloss: 1.2559150457382202, time: 22:06:13, step: 1850\nloss: 1.3066109418869019, time: 22:06:18, step: 1851\nloss: 1.0761481523513794, time: 22:06:23, step: 1852\nloss: 1.0452251434326172, time: 22:06:29, step: 1853\nloss: 1.3675135374069214, time: 22:06:35, step: 1854\nloss: 1.1769243478775024, time: 22:06:40, step: 1855\nloss: 1.0901508331298828, time: 22:06:45, step: 1856\nloss: 1.1033527851104736, time: 22:06:51, step: 1857\nloss: 1.0276490449905396, time: 22:06:56, step: 1858\nloss: 0.9991410374641418, time: 22:07:02, step: 1859\nloss: 1.1239458322525024, time: 22:07:07, step: 1860\n----- Time -> 22:07:12 ----- Validation Batch -> 1 ----  Validation Loss -> 1.3451\n----- Time -> 22:07:12 ----- Validation Batch -> 2 ----  Validation Loss -> 1.1801\n----- Time -> 22:07:13 ----- Validation Batch -> 3 ----  Validation Loss -> 1.0490\n----- Time -> 22:07:13 ----- Validation Batch -> 4 ----  Validation Loss -> 1.0546\n----- Time -> 22:07:14 ----- Validation Batch -> 5 ----  Validation Loss -> 1.1291\n----- Time -> 22:07:14 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.1515682\nloss: 1.0256056785583496, time: 22:07:16, step: 1861\nloss: 1.1121015548706055, time: 22:07:21, step: 1862\nloss: 1.1537463665008545, time: 22:07:27, step: 1863\nloss: 1.0760128498077393, time: 22:07:32, step: 1864\nloss: 1.1291062831878662, time: 22:07:37, step: 1865\nloss: 1.1471922397613525, time: 22:07:43, step: 1866\nloss: 1.1059285402297974, time: 22:07:48, step: 1867\nloss: 1.3192250728607178, time: 22:07:54, step: 1868\nloss: 1.1350197792053223, time: 22:07:59, step: 1869\nloss: 1.252223014831543, time: 22:08:05, step: 1870\nloss: 1.1577835083007812, time: 22:08:10, step: 1871\nloss: 1.021791934967041, time: 22:08:16, step: 1872\nloss: 0.9659370183944702, time: 22:08:21, step: 1873\nloss: 1.2763376235961914, time: 22:08:27, step: 1874\nloss: 1.2067811489105225, time: 22:08:32, step: 1875\nloss: 1.030844807624817, time: 22:08:38, step: 1876\nloss: 1.0956037044525146, time: 22:08:43, step: 1877\nloss: 1.1998326778411865, time: 22:08:49, step: 1878\nloss: 1.100546956062317, time: 22:08:54, step: 1879\nloss: 1.009774923324585, time: 22:09:00, step: 1880\n----- Time -> 22:09:04 ----- Validation Batch -> 1 ----  Validation Loss -> 1.3257\n----- Time -> 22:09:05 ----- Validation Batch -> 2 ----  Validation Loss -> 1.1583\n----- Time -> 22:09:05 ----- Validation Batch -> 3 ----  Validation Loss -> 1.0332\n----- Time -> 22:09:06 ----- Validation Batch -> 4 ----  Validation Loss -> 1.0328\n----- Time -> 22:09:06 ----- Validation Batch -> 5 ----  Validation Loss -> 1.1059\n----- Time -> 22:09:06 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.1311607\nloss: 1.2067642211914062, time: 22:09:08, step: 1881\nloss: 1.0589224100112915, time: 22:09:13, step: 1882\nloss: 1.036643624305725, time: 22:09:19, step: 1883\nloss: 1.1101585626602173, time: 22:09:24, step: 1884\nloss: 1.018566608428955, time: 22:09:30, step: 1885\nloss: 1.1000324487686157, time: 22:09:35, step: 1886\nloss: 1.020272970199585, time: 22:09:41, step: 1887\nloss: 1.088814616203308, time: 22:09:46, step: 1888\nloss: 1.054440975189209, time: 22:09:52, step: 1889\nloss: 1.1153510808944702, time: 22:09:57, step: 1890\nloss: 1.1471836566925049, time: 22:10:03, step: 1891\nloss: 0.977392852306366, time: 22:10:08, step: 1892\nloss: 1.1199359893798828, time: 22:10:14, step: 1893\nloss: 0.9887316823005676, time: 22:10:19, step: 1894\nloss: 1.2448961734771729, time: 22:10:25, step: 1895\nloss: 1.0446100234985352, time: 22:10:30, step: 1896\nloss: 0.9765933156013489, time: 22:10:36, step: 1897\nloss: 1.0373131036758423, time: 22:10:41, step: 1898\nloss: 1.2245597839355469, time: 22:10:47, step: 1899\nloss: 0.969849705696106, time: 22:10:52, step: 1900\n----- Time -> 22:10:57 ----- Validation Batch -> 1 ----  Validation Loss -> 1.3080\n----- Time -> 22:10:57 ----- Validation Batch -> 2 ----  Validation Loss -> 1.1373\n----- Time -> 22:10:58 ----- Validation Batch -> 3 ----  Validation Loss -> 1.0184\n----- Time -> 22:10:58 ----- Validation Batch -> 4 ----  Validation Loss -> 1.0141\n----- Time -> 22:10:59 ----- Validation Batch -> 5 ----  Validation Loss -> 1.0795\n----- Time -> 22:10:59 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.1114718\nloss: 1.298107624053955, time: 22:11:00, step: 1901\nloss: 1.0535266399383545, time: 22:11:06, step: 1902\nloss: 1.0849764347076416, time: 22:11:11, step: 1903\nloss: 0.9958564043045044, time: 22:11:17, step: 1904\nloss: 1.0320039987564087, time: 22:11:22, step: 1905\nloss: 0.9949814081192017, time: 22:11:28, step: 1906\nloss: 1.095742106437683, time: 22:11:33, step: 1907\nloss: 1.0547149181365967, time: 22:11:39, step: 1908\nloss: 0.9594578146934509, time: 22:11:44, step: 1909\nloss: 1.0368413925170898, time: 22:11:50, step: 1910\nloss: 1.2080352306365967, time: 22:11:55, step: 1911\nloss: 1.1095675230026245, time: 22:12:01, step: 1912\nloss: 0.983876645565033, time: 22:12:06, step: 1913\nloss: 1.004242181777954, time: 22:12:12, step: 1914\nloss: 1.0822519063949585, time: 22:12:17, step: 1915\nloss: 1.1485528945922852, time: 22:12:23, step: 1916\nloss: 1.1693592071533203, time: 22:12:28, step: 1917\nloss: 1.0795594453811646, time: 22:12:34, step: 1918\nloss: 0.9579864144325256, time: 22:12:39, step: 1919\nloss: 0.9142612814903259, time: 22:12:45, step: 1920\n----- Time -> 22:12:49 ----- Validation Batch -> 1 ----  Validation Loss -> 1.2882\n----- Time -> 22:12:50 ----- Validation Batch -> 2 ----  Validation Loss -> 1.1185\n----- Time -> 22:12:50 ----- Validation Batch -> 3 ----  Validation Loss -> 1.0030\n----- Time -> 22:12:51 ----- Validation Batch -> 4 ----  Validation Loss -> 0.9926\n----- Time -> 22:12:51 ----- Validation Batch -> 5 ----  Validation Loss -> 1.0582\n----- Time -> 22:12:51 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.0921120\nloss: 1.0068389177322388, time: 22:12:53, step: 1921\nloss: 1.0420256853103638, time: 22:12:58, step: 1922\nloss: 1.0507373809814453, time: 22:13:04, step: 1923\nloss: 1.0790281295776367, time: 22:13:09, step: 1924\nloss: 1.0649234056472778, time: 22:13:15, step: 1925\nloss: 1.1191571950912476, time: 22:13:20, step: 1926\nloss: 1.0182905197143555, time: 22:13:26, step: 1927\nloss: 1.0505064725875854, time: 22:13:31, step: 1928\nloss: 1.0226681232452393, time: 22:13:37, step: 1929\nloss: 1.0309441089630127, time: 22:13:42, step: 1930\nloss: 1.0659899711608887, time: 22:13:48, step: 1931\nloss: 0.989233672618866, time: 22:13:53, step: 1932\nloss: 1.0285924673080444, time: 22:13:59, step: 1933\nloss: 1.177311897277832, time: 22:14:04, step: 1934\nloss: 0.9759666919708252, time: 22:14:10, step: 1935\nloss: 1.1012272834777832, time: 22:14:15, step: 1936\nloss: 1.0714279413223267, time: 22:14:21, step: 1937\nloss: 0.8742222785949707, time: 22:14:26, step: 1938\nloss: 1.0416207313537598, time: 22:14:32, step: 1939\nloss: 1.0492111444473267, time: 22:14:37, step: 1940\n----- Time -> 22:14:42 ----- Validation Batch -> 1 ----  Validation Loss -> 1.2712\n----- Time -> 22:14:42 ----- Validation Batch -> 2 ----  Validation Loss -> 1.0991\n----- Time -> 22:14:43 ----- Validation Batch -> 3 ----  Validation Loss -> 0.9898\n----- Time -> 22:14:43 ----- Validation Batch -> 4 ----  Validation Loss -> 0.9740\n----- Time -> 22:14:44 ----- Validation Batch -> 5 ----  Validation Loss -> 1.0384\n----- Time -> 22:14:44 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.0745264\nloss: 0.9797543883323669, time: 22:14:46, step: 1941\nloss: 1.02634859085083, time: 22:14:51, step: 1942\nloss: 1.1131504774093628, time: 22:14:57, step: 1943\nloss: 0.9923632740974426, time: 22:15:02, step: 1944\nloss: 1.3391894102096558, time: 22:15:08, step: 1945\nloss: 1.038111686706543, time: 22:15:13, step: 1946\nloss: 1.1753857135772705, time: 22:15:18, step: 1947\nloss: 1.2487238645553589, time: 22:15:24, step: 1948\nloss: 0.9795547723770142, time: 22:15:30, step: 1949\nloss: 0.9667438864707947, time: 22:15:35, step: 1950\nloss: 1.0596176385879517, time: 22:15:41, step: 1951\nloss: 1.094291090965271, time: 22:15:46, step: 1952\nloss: 0.9714192152023315, time: 22:15:52, step: 1953\nloss: 1.08590829372406, time: 22:15:57, step: 1954\nloss: 1.184939980506897, time: 22:16:03, step: 1955\nloss: 1.100806474685669, time: 22:16:08, step: 1956\nloss: 0.9486595392227173, time: 22:16:14, step: 1957\nloss: 1.1883795261383057, time: 22:16:19, step: 1958\nloss: 0.9210690259933472, time: 22:16:25, step: 1959\nloss: 0.9048207402229309, time: 22:16:30, step: 1960\n----- Time -> 22:16:35 ----- Validation Batch -> 1 ----  Validation Loss -> 1.2489\n----- Time -> 22:16:35 ----- Validation Batch -> 2 ----  Validation Loss -> 1.0771\n----- Time -> 22:16:36 ----- Validation Batch -> 3 ----  Validation Loss -> 0.9748\n----- Time -> 22:16:36 ----- Validation Batch -> 4 ----  Validation Loss -> 0.9566\n----- Time -> 22:16:37 ----- Validation Batch -> 5 ----  Validation Loss -> 1.0162\n----- Time -> 22:16:37 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.0547255\nloss: 1.07803475856781, time: 22:16:38, step: 1961\nloss: 1.3265271186828613, time: 22:16:44, step: 1962\nloss: 0.9624696969985962, time: 22:16:49, step: 1963\nloss: 1.1373167037963867, time: 22:16:55, step: 1964\nloss: 0.886397123336792, time: 22:17:00, step: 1965\nloss: 1.0398999452590942, time: 22:17:06, step: 1966\nloss: 0.9380748867988586, time: 22:17:11, step: 1967\nloss: 1.0269198417663574, time: 22:17:17, step: 1968\nloss: 1.073662281036377, time: 22:17:22, step: 1969\nloss: 0.957176685333252, time: 22:17:28, step: 1970\nloss: 1.1322853565216064, time: 22:17:33, step: 1971\nloss: 0.9401450157165527, time: 22:17:39, step: 1972\nloss: 1.1596736907958984, time: 22:17:44, step: 1973\nloss: 0.9071016311645508, time: 22:17:50, step: 1974\nloss: 1.1856728792190552, time: 22:17:55, step: 1975\nloss: 0.8582965731620789, time: 22:18:01, step: 1976\nloss: 1.0770915746688843, time: 22:18:06, step: 1977\nloss: 1.0281519889831543, time: 22:18:12, step: 1978\nloss: 0.9638549089431763, time: 22:18:17, step: 1979\nloss: 1.1658207178115845, time: 22:18:23, step: 1980\n----- Time -> 22:18:27 ----- Validation Batch -> 1 ----  Validation Loss -> 1.2324\n----- Time -> 22:18:28 ----- Validation Batch -> 2 ----  Validation Loss -> 1.0562\n----- Time -> 22:18:28 ----- Validation Batch -> 3 ----  Validation Loss -> 0.9615\n----- Time -> 22:18:29 ----- Validation Batch -> 4 ----  Validation Loss -> 0.9390\n----- Time -> 22:18:29 ----- Validation Batch -> 5 ----  Validation Loss -> 0.9943\n----- Time -> 22:18:29 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.0366734\nloss: 1.0650248527526855, time: 22:18:31, step: 1981\nloss: 1.0665359497070312, time: 22:18:37, step: 1982\nloss: 1.142688274383545, time: 22:18:42, step: 1983\nloss: 1.0287683010101318, time: 22:18:48, step: 1984\nloss: 1.033368706703186, time: 22:18:53, step: 1985\nloss: 1.3314441442489624, time: 22:18:59, step: 1986\nloss: 1.019352674484253, time: 22:19:04, step: 1987\nloss: 0.931799054145813, time: 22:19:10, step: 1988\nloss: 0.9468010067939758, time: 22:19:15, step: 1989\nloss: 0.9343112111091614, time: 22:19:20, step: 1990\nloss: 1.046020269393921, time: 22:19:26, step: 1991\nloss: 1.2421334981918335, time: 22:19:31, step: 1992\nloss: 1.1921961307525635, time: 22:19:37, step: 1993\nloss: 0.9457299709320068, time: 22:19:42, step: 1994\nloss: 1.0238442420959473, time: 22:19:48, step: 1995\nloss: 0.929391086101532, time: 22:19:53, step: 1996\nloss: 1.176889419555664, time: 22:19:59, step: 1997\nloss: 1.2284318208694458, time: 22:20:04, step: 1998\nloss: 1.172090768814087, time: 22:20:10, step: 1999\nloss: 0.9937566518783569, time: 22:20:15, step: 2000\n----- Time -> 22:20:20 ----- Validation Batch -> 1 ----  Validation Loss -> 1.2113\n----- Time -> 22:20:20 ----- Validation Batch -> 2 ----  Validation Loss -> 1.0363\n----- Time -> 22:20:21 ----- Validation Batch -> 3 ----  Validation Loss -> 0.9459\n----- Time -> 22:20:21 ----- Validation Batch -> 4 ----  Validation Loss -> 0.9238\n----- Time -> 22:20:22 ----- Validation Batch -> 5 ----  Validation Loss -> 0.9742\n----- Time -> 22:20:22 ----- Validation Batch Size -> 5 ----  Validation Loss -> 1.0183003\n",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "want to continue training after 2000 steps no\n"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the Model Trained for 2000 Steps on HuggingFace"
      ],
      "metadata": {
        "id": "ge0isrcIu0-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "print('Loading the model on CPU')\n",
        "START=time.time()\n",
        "model = model.cpu()\n",
        "print(f\"Loaded model on cpu in {time.time()-START} seconds \")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-15T22:21:29.846422Z",
          "iopub.execute_input": "2024-11-15T22:21:29.846686Z",
          "iopub.status.idle": "2024-11-15T22:23:23.004593Z",
          "shell.execute_reply.started": "2024-11-15T22:21:29.846658Z",
          "shell.execute_reply": "2024-11-15T22:23:23.002687Z"
        },
        "id": "tC3Cqowbu0-g",
        "outputId": "ab0a378e-e26a-49e9-ee1a-05c1541d018d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Loading the model on CPU\nLoaded model on cpu in 113.14982438087463 seconds \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(hf_token) ##\n",
        "model.push_to_hub(\n",
        "    SAVED_MODEL,\n",
        "    tokenizer=tokenizer,\n",
        "    safe_serialization=True,\n",
        "    create_pr=True,\n",
        "    max_shard_size=\"3GB\",\n",
        ")\n",
        "\n",
        "tokenizer.push_to_hub(\n",
        "    SAVED_MODEL,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-15T22:23:23.005628Z",
          "iopub.execute_input": "2024-11-15T22:23:23.005873Z",
          "iopub.status.idle": "2024-11-15T22:23:45.596064Z",
          "shell.execute_reply.started": "2024-11-15T22:23:23.005848Z",
          "shell.execute_reply": "2024-11-15T22:23:45.594335Z"
        },
        "id": "Mi3T2GIMu0-g",
        "outputId": "239ae6b0-4251-4b5c-8384-1e6821792113"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "adapter_model.safetensors: 100%|██████████| 503M/503M [00:16<00:00, 30.4MB/s] \nNo files have been modified since last commit. Skipping to prevent empty commit.\nWARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
          "output_type": "stream"
        },
        {
          "execution_count": 17,
          "output_type": "execute_result",
          "data": {
            "text/plain": "CommitInfo(commit_url='https://huggingface.co/Alexis-Az/Story-Generation-LlaMA-8B/commit/96fe00d4c6b770c0fef83f9921e652ce34a0144a', commit_message='Upload tokenizer', commit_description='', oid='96fe00d4c6b770c0fef83f9921e652ce34a0144a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Alexis-Az/Story-Generation-LlaMA-8B', endpoint='https://huggingface.co', repo_type='model', repo_id='Alexis-Az/Story-Generation-LlaMA-8B'), pr_revision=None, pr_num=None)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "XvVoQjTVu0-g"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}