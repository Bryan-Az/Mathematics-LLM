{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Mathematics LLM with LoRA Fine-tuning\n",
        "\n",
        "This notebook fine-tunes SmolLM2-1.7B on mathematical problems using LoRA adapters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip install --upgrade --force-reinstall --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install datasets transformers accelerate bitsandbytes torch sympy antlr4-python3-runtime wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import os\n",
        "import re\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import sympy\n",
        "import wandb\n",
        "from sympy.parsing.sympy_parser import parse_expr, standard_transformations, implicit_multiplication_application\n",
        "from datasets import load_dataset, Dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configure sympy parsing\n",
        "transformations = standard_transformations + (implicit_multiplication_application,)\n",
        "x = sympy.Symbol('x')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Login to HuggingFace\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Model and training settings\n",
        "MODEL = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
        "SAVED_MODEL = \"Joash2024/Math-SmolLM2-1.7B\"\n",
        "max_seq_length = 2048\n",
        "device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
        "logger.info(f\"Using device: {device}\")\n",
        "\n",
        "# Training hyperparameters\n",
        "HYPERPARAMS = {\n",
        "    'num_train_epochs': 1,\n",
        "    'per_device_train_batch_size': 4,\n",
        "    'per_device_eval_batch_size': 4,\n",
        "    'gradient_accumulation_steps': 4,\n",
        "    'learning_rate': 2e-4,\n",
        "    'weight_decay': 0.01,\n",
        "    'warmup_ratio': 0.03,\n",
        "    'max_grad_norm': 0.3,\n",
        "    'lora_rank': 12,\n",
        "    'lora_alpha': 16,\n",
        "    'lora_dropout': 0.05\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def find_matching_brace(expr: str, start: int) -> int:\n",
        "    \"\"\"Find the matching closing brace for an opening brace\"\"\"\n",
        "    count = 1\n",
        "    for i in range(start + 1, len(expr)):\n",
        "        if expr[i] == '{':\n",
        "            count += 1\n",
        "        elif expr[i] == '}':\n",
        "            count -= 1\n",
        "            if count == 0:\n",
        "                return i\n",
        "    return -1\n",
        "\n",
        "def replace_powers(expr: str) -> str:\n",
        "    \"\"\"Replace ^ with ** for Python power notation\"\"\"\n",
        "    # Handle simple powers first\n",
        "    expr = re.sub(r'\\^([0-9]+)', r'**\\1', expr)\n",
        "    \n",
        "    # Handle more complex powers with braces\n",
        "    while '^{' in expr:\n",
        "        pos = expr.find('^{')\n",
        "        if pos == -1:\n",
        "            break\n",
        "            \n",
        "        # Find the matching closing brace\n",
        "        close_pos = find_matching_brace(expr, pos + 1)\n",
        "        if close_pos == -1:\n",
        "            break\n",
        "            \n",
        "        # Extract the exponent\n",
        "        exponent = expr[pos+2:close_pos]\n",
        "        \n",
        "        # Replace the power notation\n",
        "        expr = expr[:pos] + f'**({exponent})' + expr[close_pos+1:]\n",
        "    \n",
        "    return expr\n",
        "\n",
        "def parse_latex_fraction(expr: str) -> str:\n",
        "    \"\"\"Convert LaTeX fractions to Python division\"\"\"\n",
        "    while '\\\\frac' in expr:\n",
        "        frac_pos = expr.find('\\\\frac')\n",
        "        if frac_pos == -1:\n",
        "            break\n",
        "            \n",
        "        # Find numerator\n",
        "        num_start = expr.find('{', frac_pos)\n",
        "        if num_start == -1:\n",
        "            break\n",
        "        num_end = find_matching_brace(expr, num_start)\n",
        "        if num_end == -1:\n",
        "            break\n",
        "        numerator = expr[num_start+1:num_end]\n",
        "        \n",
        "        # Find denominator\n",
        "        den_start = expr.find('{', num_end)\n",
        "        if den_start == -1:\n",
        "            break\n",
        "        den_end = find_matching_brace(expr, den_start)\n",
        "        if den_end == -1:\n",
        "            break\n",
        "        denominator = expr[den_start+1:den_end]\n",
        "        \n",
        "        # Replace fraction\n",
        "        replacement = f\"(({numerator})/({denominator}))\"\n",
        "        expr = expr[:frac_pos] + replacement + expr[den_end+1:]\n",
        "    \n",
        "    return expr\n",
        "\n",
        "def parse_function_power(expr: str, func_name: str, power: str, argument: str) -> str:\n",
        "    \"\"\"Handle function with power (e.g., sin^2(x))\"\"\"\n",
        "    # Special case for e^x\n",
        "    if func_name == 'e':\n",
        "        return f\"exp({argument})\"\n",
        "    \n",
        "    # For other functions\n",
        "    return f\"({func_name}({argument}))**({power})\"\n",
        "\n",
        "def parse_latex_functions(expr: str) -> str:\n",
        "    \"\"\"Convert LaTeX math functions to Python/SymPy functions\"\"\"\n",
        "    # Function mapping\n",
        "    func_map = {\n",
        "        '\\\\sin': 'sin',\n",
        "        '\\\\cos': 'cos',\n",
        "        '\\\\tan': 'tan',\n",
        "        '\\\\exp': 'exp',\n",
        "        '\\\\log': 'log',\n",
        "        '\\\\sqrt': 'sqrt',\n",
        "        '\\\\pi': 'pi'\n",
        "    }\n",
        "    \n",
        "    # Replace functions\n",
        "    for latex_func, sympy_func in func_map.items():\n",
        "        expr = expr.replace(latex_func, sympy_func)\n",
        "    \n",
        "    # Handle function powers\n",
        "    power_func_pattern = r'([a-z]+)(\\^{[^{}]+})?\\{([^{}]+)\\}'\n",
        "    while True:\n",
        "        match = re.search(power_func_pattern, expr)\n",
        "        if not match:\n",
        "            break\n",
        "            \n",
        "        func_name = match.group(1)\n",
        "        power = match.group(2)\n",
        "        argument = match.group(3)\n",
        "        \n",
        "        if power:\n",
        "            # Extract power value from ^{...}\n",
        "            power = power[2:-1]\n",
        "            replacement = parse_function_power(expr, func_name, power, argument)\n",
        "        else:\n",
        "            replacement = f\"{func_name}({argument})\"\n",
        "        \n",
        "        expr = expr[:match.start()] + replacement + expr[match.end():]\n",
        "    \n",
        "    return expr\n",
        "\n",
        "def clean_latex(expr: str, debug: bool = True) -> Tuple[str, str, List[str]]:\n",
        "    \"\"\"Clean LaTeX expression with detailed transformation steps\"\"\"\n",
        "    steps = [f\"Original: {expr}\"]\n",
        "    \n",
        "    # Store original\n",
        "    original = expr\n",
        "    cleaned = expr\n",
        "    \n",
        "    # Replace powers first\n",
        "    cleaned = replace_powers(cleaned)\n",
        "    steps.append(f\"After power replacement: {cleaned}\")\n",
        "    \n",
        "    # Handle fractions\n",
        "    cleaned = parse_latex_fraction(cleaned)\n",
        "    steps.append(f\"After fraction parsing: {cleaned}\")\n",
        "    \n",
        "    # Handle functions\n",
        "    cleaned = parse_latex_functions(cleaned)\n",
        "    steps.append(f\"After function parsing: {cleaned}\")\n",
        "    \n",
        "    # Clean up LaTeX artifacts\n",
        "    cleaned = cleaned.replace('\\\\left', '').replace('\\\\right', '')\n",
        "    cleaned = cleaned.replace('\\\\{', '(').replace('\\\\}', ')')\n",
        "    cleaned = cleaned.replace('{', '(').replace('}', ')')\n",
        "    steps.append(f\"After cleanup: {cleaned}\")\n",
        "    \n",
        "    # Add multiplication symbols where needed\n",
        "    cleaned = re.sub(r'([0-9])([a-zA-Z])', r'\\1*\\2', cleaned)\n",
        "    cleaned = re.sub(r'\\)\\(', r')*(', cleaned)\n",
        "    cleaned = re.sub(r'([0-9a-zA-Z])\\s+([a-zA-Z]\\()', r'\\1*\\2', cleaned)\n",
        "    steps.append(f\"Final: {cleaned}\")\n",
        "    \n",
        "    return original, cleaned, steps\n",
        "\n",
        "def validate_math_expression(expr: str, debug: bool = True) -> bool:\n",
        "    \"\"\"Validate if a string is a valid mathematical expression\"\"\"\n",
        "    try:\n",
        "        if not expr or not isinstance(expr, str):\n",
        "            if debug:\n",
        "                print(\"Invalid input: empty or not a string\")\n",
        "            return False\n",
        "        \n",
        "        original, cleaned, steps = clean_latex(expr, debug)\n",
        "        \n",
        "        if debug:\n",
        "            print(\"\\nTransformation steps:\")\n",
        "            for step in steps:\n",
        "                print(step)\n",
        "        \n",
        "        # Parse with sympy using transformations\n",
        "        parsed = parse_expr(cleaned, transformations=transformations, local_dict={'x': x})\n",
        "        \n",
        "        if debug:\n",
        "            print(f\"Successfully parsed as: {parsed}\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        if debug:\n",
        "            print(f\"Failed to parse expression:\")\n",
        "            print(f\"Error type: {type(e).__name__}\")\n",
        "            print(f\"Error message: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def is_valid_derivative_problem(example: Dict, debug: bool = False) -> bool:\n",
        "    \"\"\"Check if an example is a valid derivative problem\"\"\"\n",
        "    if not isinstance(example.get('Function'), str) or not isinstance(example.get('Derivative'), str):\n",
        "        if debug:\n",
        "            print(f\"Invalid types in example: {example}\")\n",
        "        return False\n",
        "    \n",
        "    func = example['Function'].strip()\n",
        "    deriv = example['Derivative'].strip()\n",
        "    \n",
        "    if not func or not deriv:\n",
        "        if debug:\n",
        "            print(f\"Empty function or derivative in example: {example}\")\n",
        "        return False\n",
        "    \n",
        "    if debug:\n",
        "        print(f\"\\nChecking derivative problem:\")\n",
        "        print(f\"Function: {func}\")\n",
        "        print(f\"Derivative: {deriv}\")\n",
        "    \n",
        "    func_valid = validate_math_expression(func, debug)\n",
        "    deriv_valid = validate_math_expression(deriv, debug)\n",
        "    \n",
        "    return func_valid and deriv_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Test expressions\n",
        "test_expressions = [\n",
        "    \"x^2 + 3x\",  # Simple polynomial\n",
        "    \"\\\\frac{1}{2}x^2\",  # Fraction with power\n",
        "    \"\\\\sin{\\\\left(x^2\\\\right)}\",  # Trigonometric function\n",
        "    \"\\\\frac{5 x e^{- 4 x}}{8}\",  # Complex fraction with e\n",
        "    \"- \\\\sin{\\\\left(7 x^{3} \\\\right)}\",  # Negative trig function\n",
        "    \"\\\\cos^{2}{\\\\left(x\\\\right)}\",  # Function with power\n",
        "    \"e^{x} \\\\sin{\\\\left(x^2\\\\right)}\",  # Multiple functions\n",
        "    \"\\\\frac{\\\\sin{\\\\left(x\\\\right)}}{\\\\cos{\\\\left(x\\\\right)}}\",  # Fraction of functions\n",
        "    \"2x + \\\\frac{1}{x}\",  # Mixed terms\n",
        "    \"x^{2} e^{x} \\\\sin{\\\\left(x\\\\right)}\"  # Multiple terms\n",
        "]\n",
        "\n",
        "print(\"Testing expression validation:\")\n",
        "for expr in test_expressions:\n",
        "    print(f\"\\nTesting: {expr}\")\n",
        "    validate_math_expression(expr, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Configure quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_quant_type=\"nf8\",\n",
        "    bnb_8bit_compute_dtype=torch.float16,\n",
        "    bnb_8bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load tokenizer and model\n",
        "logger.info(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "logger.info(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Load datasets\n",
        "logger.info(\"Loading datasets...\")\n",
        "train_data = load_dataset(\"Alexis-Az/math_datasets\", name='derivatives', split='train[:-2000]').shuffle()\n",
        "eval_data = load_dataset(\"Alexis-Az/math_datasets\", 'derivatives', split=\"train[-2000:]\").shuffle()\n",
        "\n",
        "logger.info(f\"Training examples: {len(train_data)}\")\n",
        "logger.info(f\"Validation examples: {len(eval_data)}\")\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\nSample training data:\")\n",
        "print(train_data[0])\n",
        "print(\"\\nDataset columns:\")\n",
        "print(train_data.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def format_math_problem(function: str, derivative: str) -> str:\n",
        "    \"\"\"Format a math problem for the model\"\"\"\n",
        "    return f\"\"\"Given a mathematical function, find its derivative.\n",
        "\n",
        "Function: {function}\n",
        "The derivative of this function is: {derivative}\n",
        "\n",
        "Let's verify this step by step:\n",
        "1. Starting with f(x) = {function}\n",
        "2. Applying differentiation rules\n",
        "3. We get f'(x) = {derivative}\n",
        "\"\"\".strip()\n",
        "\n",
        "def prepare_training_data(examples: Dict) -> Dict:\n",
        "    \"\"\"Convert examples to model inputs with proper tensor handling\"\"\"\n",
        "    # Format all examples in batch\n",
        "    texts = [\n",
        "        format_math_problem(function=func, derivative=deriv)\n",
        "        for func, deriv in zip(examples['Function'], examples['Derivative'])\n",
        "    ]\n",
        "    \n",
        "    # Tokenize with padding and truncation\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length,\n",
        "        return_tensors='pt'  # Return PyTorch tensors directly\n",
        "    )\n",
        "    \n",
        "    # Set up labels (same as input_ids for causal LM)\n",
        "    tokenized['labels'] = tokenized['input_ids'].clone()\n",
        "    \n",
        "    return tokenized\n",
        "\n",
        "# Process a small batch first to verify format\n",
        "logger.info(\"Testing data processing on a small batch...\")\n",
        "test_batch = train_data.select(range(4))\n",
        "processed_batch = prepare_training_data(test_batch)\n",
        "\n",
        "print(\"\\nVerifying test batch:\")\n",
        "for key, value in processed_batch.items():\n",
        "    print(f\"{key}: {type(value)}, shape: {value.shape}, dtype: {value.dtype}\")\n",
        "\n",
        "# Process full datasets\n",
        "logger.info(\"\\nProcessing full datasets...\")\n",
        "train_dataset = train_data.map(\n",
        "    prepare_training_data,\n",
        "    batched=True,\n",
        "    batch_size=100,\n",
        "    remove_columns=train_data.column_names,\n",
        "    desc=\"Processing training data\"\n",
        ")\n",
        "\n",
        "eval_dataset = eval_data.map(\n",
        "    prepare_training_data,\n",
        "    batched=True,\n",
        "    batch_size=100,\n",
        "    remove_columns=eval_data.column_names,\n",
        "    desc=\"Processing validation data\"\n",
        ")\n",
        "\n",
        "# Verify final dataset\n",
        "print(\"\\nVerifying final dataset:\")\n",
        "print(f\"Training examples: {len(train_dataset)}\")\n",
        "print(f\"Validation examples: {len(eval_dataset)}\")\n",
        "\n",
        "sample = train_dataset[0]\n",
        "print(\"\\nSample features:\")\n",
        "for key, value in sample.items():\n",
        "    if isinstance(value, torch.Tensor):\n",
        "        print(f\"{key}: tensor shape {value.shape}, dtype {value.dtype}\")\n",
        "    else:\n",
        "        print(f\"{key}: type {type(value)}\")\n",
        "\n",
        "# Decode a sample to verify content\n",
        "print(\"\\nSample decoded text:\")\n",
        "decoded = tokenizer.decode(sample['input_ids'])\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=HYPERPARAMS['lora_rank'],\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=HYPERPARAMS['lora_alpha'],\n",
        "    lora_dropout=HYPERPARAMS['lora_dropout'],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "logger.info(\"Preparing model for training...\")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Initialize wandb for tracking\n",
        "wandb.init(\n",
        "    project=\"math-llm\",\n",
        "    config=HYPERPARAMS,\n",
        "    name=f\"lora-r{HYPERPARAMS['lora_rank']}-lr{HYPERPARAMS['learning_rate']}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Training configuration\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=HYPERPARAMS['num_train_epochs'],\n",
        "    per_device_train_batch_size=HYPERPARAMS['per_device_train_batch_size'],\n",
        "    per_device_eval_batch_size=HYPERPARAMS['per_device_eval_batch_size'],\n",
        "    gradient_accumulation_steps=HYPERPARAMS['gradient_accumulation_steps'],\n",
        "    eval_steps=100,\n",
        "    logging_steps=50,\n",
        "    learning_rate=HYPERPARAMS['learning_rate'],\n",
        "    weight_decay=HYPERPARAMS['weight_decay'],\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    max_grad_norm=HYPERPARAMS['max_grad_norm'],\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=HYPERPARAMS['warmup_ratio'],\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=SAVED_MODEL,\n",
        "    hub_strategy=\"every_save\",\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=[\"wandb\"],\n",
        "    remove_unused_columns=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "logger.info(\"Starting training...\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "# Log training results\n",
        "logger.info(f\"\\nTraining results:\")\n",
        "for key, value in train_result.metrics.items():\n",
        "    logger.info(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Save the model\n",
        "logger.info(\"\\nSaving model to HuggingFace Hub...\")\n",
        "trainer.push_to_hub()\n",
        "\n",
        "logger.info(f\"\\nTraining complete! Model saved to: {SAVED_MODEL}\")\n",
        "logger.info(f\"You can access your model at: https://huggingface.co/{SAVED_MODEL}\")\n",
        "\n",
        "# Close wandb run\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
